---
layout: single
title: "HW4. Visualization, Correlation, and Linear Models"
permalink: /coursework/SI618/hw4/
comments: false
author_profile: true
read_time: true
toc: true
toc_label: "Table of Contents"
toc_sticky: true
categories: ["SI618"]
---

**Topics: Data visualization, Correlation, Linear models**

--- 

```python
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
from scipy import stats
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.stats.multicomp import pairwise_tukeyhsd
import matplotlib.patches as mpatches
```


```python
warnings.filterwarnings('ignore')
```


```python
MY_UNIQNAME = 'yjwoo'
```

**We will be using two different datasets for the two different parts of this homework. Download the data from:**  

* [https://www.kaggle.com/datasnaek/youtube-new](https://www.kaggle.com/datasnaek/youtube-new)

> YouTube provides a list of trending videos on it's site, determined by user interaction metrics such as likes, comments, and views. This dataset includes months of daily trending video across five different regions: the United States ("US"), Canada ("CA"), Great Britain ("GB"), Germany ("DE"), and France ("FR").

* https://www.kaggle.com/abcsds/pokemon

> This data set includes 721 Pokemon, including their number, name, first and second type, and basic stats: HP, Attack, Defense, Special Attack, Special Defense, and Speed.

# Part 1: Answer the questions below based on the *[YouTube dataset](https://www.kaggle.com/datasnaek/youtube-new)*
- Write Python code that can answer the following questions, and
- <b>Explain your answers in plain English. </b>

## <span style="color:magenta">  Q1. For 15 Points: Compare the distributions of comments, views, likes, and dislikes for </span>

* Plot histograms for these metrics *for Canada*. What can you say about them?
* Try to apply a log transformation, and plot the histograms again. How do they look now?
* Create a pairplot *for Canada*, as we did in this week's class. Do you see anything interesting?
* Create additional pairplots for the other four regions. Do they look similar?


```python
df_youtube_canada = pd.read_csv("./data/CAvideos.csv")
```


```python
df_youtube_canada.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>video_id</th>
      <th>trending_date</th>
      <th>title</th>
      <th>channel_title</th>
      <th>category_id</th>
      <th>publish_time</th>
      <th>tags</th>
      <th>views</th>
      <th>likes</th>
      <th>dislikes</th>
      <th>comment_count</th>
      <th>thumbnail_link</th>
      <th>comments_disabled</th>
      <th>ratings_disabled</th>
      <th>video_error_or_removed</th>
      <th>description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>n1WpP7iowLc</td>
      <td>17.14.11</td>
      <td>Eminem - Walk On Water (Audio) ft. BeyoncÃ©</td>
      <td>EminemVEVO</td>
      <td>10</td>
      <td>2017-11-10T17:00:03.000Z</td>
      <td>Eminem|"Walk"|"On"|"Water"|"Aftermath/Shady/In...</td>
      <td>17158579</td>
      <td>787425</td>
      <td>43420</td>
      <td>125882</td>
      <td>https://i.ytimg.com/vi/n1WpP7iowLc/default.jpg</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>Eminem's new track Walk on Water ft. BeyoncÃ© i...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0dBIkQ4Mz1M</td>
      <td>17.14.11</td>
      <td>PLUSH - Bad Unboxing Fan Mail</td>
      <td>iDubbbzTV</td>
      <td>23</td>
      <td>2017-11-13T17:00:00.000Z</td>
      <td>plush|"bad unboxing"|"unboxing"|"fan mail"|"id...</td>
      <td>1014651</td>
      <td>127794</td>
      <td>1688</td>
      <td>13030</td>
      <td>https://i.ytimg.com/vi/0dBIkQ4Mz1M/default.jpg</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>STill got a lot of packages. Probably will las...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5qpjK5DgCt4</td>
      <td>17.14.11</td>
      <td>Racist Superman | Rudy Mancuso, King Bach &amp; Le...</td>
      <td>Rudy Mancuso</td>
      <td>23</td>
      <td>2017-11-12T19:05:24.000Z</td>
      <td>racist superman|"rudy"|"mancuso"|"king"|"bach"...</td>
      <td>3191434</td>
      <td>146035</td>
      <td>5339</td>
      <td>8181</td>
      <td>https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>WATCH MY PREVIOUS VIDEO â–¶ \n\nSUBSCRIBE â–º http...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>d380meD0W0M</td>
      <td>17.14.11</td>
      <td>I Dare You: GOING BALD!?</td>
      <td>nigahiga</td>
      <td>24</td>
      <td>2017-11-12T18:01:41.000Z</td>
      <td>ryan|"higa"|"higatv"|"nigahiga"|"i dare you"|"...</td>
      <td>2095828</td>
      <td>132239</td>
      <td>1989</td>
      <td>17518</td>
      <td>https://i.ytimg.com/vi/d380meD0W0M/default.jpg</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>I know it's been a while since we did this sho...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2Vv-BfVoq4g</td>
      <td>17.14.11</td>
      <td>Ed Sheeran - Perfect (Official Music Video)</td>
      <td>Ed Sheeran</td>
      <td>10</td>
      <td>2017-11-09T11:04:14.000Z</td>
      <td>edsheeran|"ed sheeran"|"acoustic"|"live"|"cove...</td>
      <td>33523622</td>
      <td>1634130</td>
      <td>21082</td>
      <td>85067</td>
      <td>https://i.ytimg.com/vi/2Vv-BfVoq4g/default.jpg</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>ðŸŽ§: https://ad.gt/yt-perfect\nðŸ’°: https://atlant...</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_youtube_canada.shape
```




    (40881, 16)



The youtube_canada data consists of a total of 40881 rows and 16 columns.


```python
df_youtube_canada.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 40881 entries, 0 to 40880
    Data columns (total 16 columns):
     #   Column                  Non-Null Count  Dtype 
    ---  ------                  --------------  ----- 
     0   video_id                40881 non-null  object
     1   trending_date           40881 non-null  object
     2   title                   40881 non-null  object
     3   channel_title           40881 non-null  object
     4   category_id             40881 non-null  int64 
     5   publish_time            40881 non-null  object
     6   tags                    40881 non-null  object
     7   views                   40881 non-null  int64 
     8   likes                   40881 non-null  int64 
     9   dislikes                40881 non-null  int64 
     10  comment_count           40881 non-null  int64 
     11  thumbnail_link          40881 non-null  object
     12  comments_disabled       40881 non-null  bool  
     13  ratings_disabled        40881 non-null  bool  
     14  video_error_or_removed  40881 non-null  bool  
     15  description             39585 non-null  object
    dtypes: bool(3), int64(5), object(8)
    memory usage: 4.2+ MB


You can check that there are no missing values â€‹â€‹except for the description column.

**Views**


```python
df_youtube_canada["views"].describe()
```




    count    4.088100e+04
    mean     1.147036e+06
    std      3.390913e+06
    min      7.330000e+02
    25%      1.439020e+05
    50%      3.712040e+05
    75%      9.633020e+05
    max      1.378431e+08
    Name: views, dtype: float64




```python
sns.set(style = "darkgrid")
plt.figure(figsize = (8, 7))

sns.histplot(x = df_youtube_canada["views"])
plt.xlabel("Views", fontsize = 16)
plt.ylabel("Count", fontsize = 16)

plt.show()
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_15_0.png)
    


The view column has a very large deviation with a minimum value of 733 and a maximum value of 130 million. Therefore, the standard deviation is also very large of 3.3 million. For this reason, even if you draw a histogram, you cannot check the distribution properly as above. To see the distribution in more detail, let's check only the data within the 95% percentile


```python
percentile_95_views = np.percentile(df_youtube_canada["views"], 95)
percentile_95_views
```




    4090835.0



95% percentile of views column is about 4 million


```python
sns.set(style="darkgrid")

fig, (ax_box, ax_hist) = plt.subplots(2, sharex = True, gridspec_kw = {"height_ratios": (.2, .8)}, figsize = (10, 7))

sns.boxplot(x = df_youtube_canada.views, ax = ax_box, showfliers = False)
sns.histplot(x = df_youtube_canada[df_youtube_canada["views"] <= percentile_95_views].views, ax = ax_hist)

plt.xlabel("Views", fontsize = 16)
plt.ylabel("Count", fontsize = 16)
ax_box.set_xlabel("")

plt.show()
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_19_0.png)
    


If you only look at data below 95% percentile, it is a right-skewed graph with a median of about 370,000.


```python
np.log(df_youtube_canada["views"]).describe()
```




    count    40881.000000
    mean        12.810707
    std          1.508807
    min          6.597146
    25%         11.876888
    50%         12.824507
    75%         13.778122
    max         18.741627
    Name: views, dtype: float64




```python
sns.set(style="darkgrid")
plt.figure(figsize = (8, 7))

sns.histplot(x = np.log(df_youtube_canada["views"]))
plt.xlabel("Views", fontsize = 16)
plt.ylabel("Count", fontsize = 16)

plt.show()
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_22_0.png)
    


After log transformation, it seems to follow a normal distribution with some bell shape. However, since it is not possible to judge only by the shape of histogram, let's check the lag plot, QQplot, and run sequence plot together.


```python
def checkPlots(data, column, isLogTransform):
    
    if isLogTransform:
        series = np.log(data[column])
    
    else:
        series = data[column]

    fig, axs = plt.subplots(2, 2, figsize = (15, 10))
    plt.tight_layout(pad = 0.4, w_pad = 4, h_pad = 1.0)

    # Histogram
    ax = sns.histplot(series, ax = axs[0, 0])
    if isLogTransform:
        ax.set_xlabel(f"log({column})", fontsize = 12)
        ax.set_title("Histogram", fontsize = 16)
    else:
        ax.set_xlabel(column, fontsize = 12)
    ax.set_ylabel("Count", fontsize = 12)
    ax.set_title("Histogram", fontsize = 16)

    # Lag plot
    lag = series.copy()
    lag = np.array(lag[:-1])
    current = series[1:]
    ax = sns.regplot(x = current, y = lag, fit_reg = False, ax = axs[0,1])
    ax.set_ylabel("y_i-1", fontsize = 12)
    ax.set_xlabel("y_i", fontsize = 12)
    ax.set_title("Lag plot", fontsize = 16)

    # QQ plot
    qntls, xr = stats.probplot(series, fit=False)
    ax = sns.regplot(x = xr, y = qntls, ax = axs[1,0])
    ax.set_title("QQ plot", fontsize = 16)

    # Run sequence
    ax = sns.regplot(x = np.arange(len(series)),y = series, ax = axs[1,1])
    ax.set_ylabel("val", fontsize = 12)
    ax.set_xlabel("i", fontsize = 12)
    ax.set_title("Run sequence plot", fontsize = 16)
    
    plt.tight_layout()
    plt.show()
```


```python
checkPlots(data = df_youtube_canada, column = "views", isLogTransform = True)
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_25_0.png)
    


The lag plot and run sequence plot should not show a certain pattern, and the qq plot means that the more points on the diagonal, the closer to the bell shape. Since the log transformation of views satisfies all of these conditions, it can be seen that it is close to a normal distribution.
 

**Likes**


```python
df_youtube_canada["likes"].describe()
```




    count    4.088100e+04
    mean     3.958269e+04
    std      1.326895e+05
    min      0.000000e+00
    25%      2.191000e+03
    50%      8.780000e+03
    75%      2.871700e+04
    max      5.053338e+06
    Name: likes, dtype: float64




```python
sns.set(style="darkgrid")
plt.figure(figsize = (8, 7))

sns.histplot(x = df_youtube_canada["likes"])
plt.xlabel("Likes", fontsize = 16)
plt.ylabel("Count", fontsize = 16)

plt.show()
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_29_0.png)
    


The likes column has a very large deviation with a minimum value of 0 and a maximum value of 5 million. Therefore, the standard deviation is also very large of one hundred thirty-five thousand. For this reason, even if you draw a histogram, you cannot check the distribution properly as above. To see the distribution in more detail, let's check only the data within the 95% percentile


```python
percentile_95_likes = np.percentile(df_youtube_canada["likes"], 95)
percentile_95_likes
```




    165252.0



95% percentile of likes column is about one hundred sixty-five thousand.


```python
sns.set(style="darkgrid")

fig, (ax_box, ax_hist) = plt.subplots(2, sharex = True, gridspec_kw = {"height_ratios": (.2, .8)}, figsize = (10, 7))

sns.boxplot(x = df_youtube_canada.likes, ax = ax_box, showfliers = False)
sns.histplot(x = df_youtube_canada[df_youtube_canada["likes"] <= percentile_95_likes].likes, ax = ax_hist)

plt.xlabel("Likes", fontsize = 16)
plt.ylabel("Count", fontsize = 16)
ax_box.set_xlabel("")

plt.show()
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_33_0.png)
    


If you only look at data below 95% percentile, it is a right-skewed graph with a median of about 8,780.


```python
np.log(df_youtube_canada[df_youtube_canada["likes"] > 0].likes).describe()
```




    count    40597.000000
    mean         8.951208
    std          1.969017
    min          0.000000
    25%          7.727535
    50%          9.095154
    75%         10.275051
    max         15.435560
    Name: likes, dtype: float64




```python
checkPlots(data = df_youtube_canada, column = "likes", isLogTransform = True)
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_36_0.png)
    


Looking at the log-transformed graph, the lag plot and run sequence plot do not show any pattern. Although the qq plot deviated a little from the diagonal, it can be seen that it is almost like a bell shape. Therefore, the log-transformed likes approximates the normal distribution.


**Dislikes**


```python
df_youtube_canada["dislikes"].describe()
```




    count    4.088100e+04
    mean     2.009195e+03
    std      1.900837e+04
    min      0.000000e+00
    25%      9.900000e+01
    50%      3.030000e+02
    75%      9.500000e+02
    max      1.602383e+06
    Name: dislikes, dtype: float64




```python
sns.set(style="darkgrid")
plt.figure(figsize = (8, 7))

sns.histplot(x = df_youtube_canada["dislikes"])
plt.xlabel("Dislikes", fontsize = 16)
plt.ylabel("Count", fontsize = 16)

plt.show()
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_40_0.png)
    


The dislikes column has a very large deviation with a minimum value of 0 and a maximum value of about 1,600,000. Therefore, the standard deviation is also very large of 19,000. For this reason, even if you draw a histogram, you cannot check the distribution properly as above. To see the distribution in more detail, let's check only the data within the 95% percentile.


```python
percentile_95_dislikes = np.percentile(df_youtube_canada["dislikes"], 95)
percentile_95_dislikes
```




    6479.0



95% percentile of likes column is 6,479.


```python
sns.set(style="darkgrid")

fig, (ax_box, ax_hist) = plt.subplots(2, sharex = True, gridspec_kw = {"height_ratios": (.2, .8)}, figsize = (10, 7))

sns.boxplot(x = df_youtube_canada.dislikes, ax = ax_box, showfliers = False)
sns.histplot(x = df_youtube_canada[df_youtube_canada["dislikes"] <= percentile_95_dislikes].dislikes, ax = ax_hist)

plt.xlabel("Dislikes", fontsize = 16)
plt.ylabel("Count", fontsize = 16)
ax_box.set_xlabel("")

plt.show()
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_44_0.png)
    


If you only look at data below 95% percentile, it is a right-skewed graph with a median of about 303.


```python
np.log(df_youtube_canada[df_youtube_canada["dislikes"] > 0].dislikes).describe()
```




    count    40488.000000
    mean         5.761168
    std          1.796411
    min          0.000000
    25%          4.634729
    50%          5.733341
    75%          6.870313
    max         14.287002
    Name: dislikes, dtype: float64




```python
checkPlots(data = df_youtube_canada, column = "dislikes", isLogTransform = True)
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_47_0.png)
    


Looking at the log-transformed graph, the lag plot and run sequence plot do not show any pattern. Also, qqplot is almost diagonal. Therefore, the log-transformed dislikes approximates the normal distribution.

**Comment counts**


```python
df_youtube_canada["comment_count"].describe()
```




    count    4.088100e+04
    mean     5.042975e+03
    std      2.157902e+04
    min      0.000000e+00
    25%      4.170000e+02
    50%      1.301000e+03
    75%      3.713000e+03
    max      1.114800e+06
    Name: comment_count, dtype: float64




```python
sns.set(style="darkgrid")
plt.figure(figsize = (8, 7))

sns.histplot(x = df_youtube_canada["comment_count"])
plt.xlabel("Comment counts", fontsize = 16)
plt.ylabel("Count", fontsize = 16)

plt.show()
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_51_0.png)
    


The comment counts column has a very large deviation with a minimum value of 0 and a maximum value of 1,114,800. Therefore, the standard deviation is also very large of 21,579. For this reason, even if you draw a histogram, you cannot check the distribution properly as above. To see the distribution in more detail, let's check only the data within the 95% percentile


```python
percentile_95_comments = np.percentile(df_youtube_canada["comment_count"], 95)
percentile_95_comments
```




    19210.0



95% percentile of comment counts column is 19210.


```python
sns.set(style="darkgrid")

fig, (ax_box, ax_hist) = plt.subplots(2, sharex = True, gridspec_kw = {"height_ratios": (.2, .8)}, figsize = (10, 7))

sns.boxplot(x = df_youtube_canada.comment_count, ax = ax_box, showfliers = False)
sns.histplot(x = df_youtube_canada[df_youtube_canada["comment_count"] <= percentile_95_comments].comment_count, ax = ax_hist)

plt.xlabel("Comment counts", fontsize = 16)
plt.ylabel("Count", fontsize = 16)
ax_box.set_xlabel("")

plt.show()
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_55_0.png)
    


If you only look at data below 95% percentile, it is a right-skewed graph with a median of about 1,301.


```python
np.log(df_youtube_canada[df_youtube_canada["comment_count"] > 0].comment_count).describe()
```




    count    40235.000000
    mean         7.116182
    std          1.752948
    min          0.000000
    25%          6.102559
    50%          7.202661
    75%          8.238801
    max         13.924186
    Name: comment_count, dtype: float64




```python
checkPlots(data = df_youtube_canada, column = "comment_count", isLogTransform = True)
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_58_0.png)
    


Looking at the log-transformed graph, the lag plot and run sequence plot do not show any pattern. Although the qq plot deviated a little from the diagonal, it can be seen that it is almost like a bell shape. Therefore, the log-transformed comment counts approximates the normal distribution.

**Pair plot**


```python
plt.figure(figsize = (20,20))
sns.pairplot(df_youtube_canada, vars = ['views', 'likes', 'dislikes', 'comment_count'])
plt.show()
```


    <Figure size 1440x1440 with 0 Axes>



    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_61_1.png)
    


Above is a pair plot of views, likes, dislikes, and comment_counts. It is difficult to check the overall distribution because of some extreme values, so let's draw only the data below the 95% percentile for each column. Also, since there are too many points, it is difficult to check the distribution with a simple scatter plot, so let's check it with a kde plot.


```python
df_youtube_canada["country"] = "Canada"

df_youtube_canada_under_percentile = df_youtube_canada[(df_youtube_canada.views <= np.percentile(df_youtube_canada.views, 95)) & \
                                                       (df_youtube_canada.likes <= np.percentile(df_youtube_canada.likes, 95)) & \
                                                       (df_youtube_canada.dislikes <= np.percentile(df_youtube_canada.dislikes, 95)) & \
                                                       (df_youtube_canada.comment_count <= np.percentile(df_youtube_canada.comment_count, 95))]
```


```python
df_youtube_canada.shape, df_youtube_canada_under_percentile.shape
```




    ((40881, 17), (37131, 17))



When only data that falls below the 95% percentile for each column are selected, only 3000 data out of a total of 40,000 are excluded, so it can be seen that the distribution is not affected significantly.


```python
plt.figure(figsize = (20,20))
sns.pairplot(df_youtube_canada_under_percentile, vars = ['views', 'likes', 'dislikes', 'comment_count'], kind = "kde")
plt.show()
```


    <Figure size 1440x1440 with 0 Axes>



    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_66_1.png)
    


The above shows a kde pair plot of columns. But for kde plot, the code takes too long to run, so let's sample only 3000 and draw it.


```python
np.random.seed(0)
plt.figure(figsize = (20,20))
sns.pairplot(df_youtube_canada_under_percentile.sample(3000), vars = ['views', 'likes', 'dislikes', 'comment_count'], kind = "kde")
plt.show()
```


    <Figure size 1440x1440 with 0 Axes>



    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_68_1.png)
    


Comparing the kde plot of the total 37000 data and the kde plot of the sampled 3000 data shows almost similarities. Of course, it would be good to compare the distribution through the entire data, but since there is a problem that the code does not run, let's sample only 3000 data for each country and compare the kde plot.


```python
# read France data
df_youtube_france = pd.read_csv("./data/FRvideos.csv")
df_youtube_france["country"] = "France"

df_youtube_france_under_percentile = df_youtube_france[(df_youtube_france.views <= np.percentile(df_youtube_france.views, 95)) & \
                                                       (df_youtube_france.likes <= np.percentile(df_youtube_france.likes, 95)) & \
                                                       (df_youtube_france.dislikes <= np.percentile(df_youtube_france.dislikes, 95)) & \
                                                       (df_youtube_france.comment_count <= np.percentile(df_youtube_france.comment_count, 95))]

# read US data
df_youtube_us = pd.read_csv("./data/USvideos.csv")
df_youtube_us["country"] = "United States"

df_youtube_us_under_percentile = df_youtube_us[(df_youtube_us.views <= np.percentile(df_youtube_us.views, 95)) & \
                                               (df_youtube_us.likes <= np.percentile(df_youtube_us.likes, 95)) & \
                                               (df_youtube_us.dislikes <= np.percentile(df_youtube_us.dislikes, 95)) & \
                                               (df_youtube_us.comment_count <= np.percentile(df_youtube_us.comment_count, 95))]

# read Germany data
df_youtube_germany = pd.read_csv("./data/DEvideos.csv")
df_youtube_germany["country"] = "Germany"

df_youtube_germany_under_percentile = df_youtube_germany[(df_youtube_germany.views <= np.percentile(df_youtube_germany.views, 95)) & \
                                                         (df_youtube_germany.likes <= np.percentile(df_youtube_germany.likes, 95)) & \
                                                         (df_youtube_germany.dislikes <= np.percentile(df_youtube_germany.dislikes, 95)) & \
                                                         (df_youtube_germany.comment_count <= np.percentile(df_youtube_germany.comment_count, 95))]

# read Great Britain data
df_youtube_gb = pd.read_csv("./data/GBvideos.csv")
df_youtube_gb["country"] = "Great Britain"

df_youtube_gb_under_percentile = df_youtube_gb[(df_youtube_gb.views <= np.percentile(df_youtube_gb.views, 95)) & \
                                               (df_youtube_gb.likes <= np.percentile(df_youtube_gb.likes, 95)) & \
                                               (df_youtube_gb.dislikes <= np.percentile(df_youtube_gb.dislikes, 95)) & \
                                               (df_youtube_gb.comment_count <= np.percentile(df_youtube_gb.comment_count, 95))]

```


```python
plt.figure(figsize = (20,20))
sns.color_palette()
g = sns.pairplot(df_youtube_france_under_percentile.sample(3000), \
                 vars = ['views', 'likes', 'dislikes', 'comment_count'], \
                 plot_kws = {'alpha':0.5}, corner = True)
g.map_lower(sns.kdeplot, color = "darkBlue")
plt.show()
```


    <Figure size 1440x1440 with 0 Axes>



    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_71_1.png)
    



```python
plt.figure(figsize = (20,20))
sns.color_palette()
g = sns.pairplot(df_youtube_us_under_percentile.sample(3000), \
                 vars = ['views', 'likes', 'dislikes', 'comment_count'], \
                 plot_kws = {'alpha':0.5}, corner = True)
g.map_lower(sns.kdeplot, color = "darkBlue")
plt.show()
```


    <Figure size 1440x1440 with 0 Axes>



    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_72_1.png)
    



```python
plt.figure(figsize = (20,20))
sns.color_palette()
g = sns.pairplot(df_youtube_germany_under_percentile.sample(3000), \
                 vars = ['views', 'likes', 'dislikes', 'comment_count'], \
                 plot_kws = {'alpha':0.5}, corner = True)
g.map_lower(sns.kdeplot, color = "darkBlue")
plt.show()
```


    <Figure size 1440x1440 with 0 Axes>



    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_73_1.png)
    



```python
plt.figure(figsize = (20,20))
sns.color_palette()
g = sns.pairplot(df_youtube_gb_under_percentile.sample(3000), \
                 vars = ['views', 'likes', 'dislikes', 'comment_count'], \
                 plot_kws = {'alpha':0.5}, corner = True)
g.map_lower(sns.kdeplot, color = "darkBlue")
plt.show()
```


    <Figure size 1440x1440 with 0 Axes>



    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_74_1.png)
    


If you look at the pairplots of the other regions, you can see that they all have a similar distribution, starting from (0,0), and gradually spreading as the x-axis and y-axis values â€‹â€‹increase.

## <span style="color:magenta"> Q2. For 10 Points: Create a heatmap of correlations between the variables for a region of your choice </span>

>A [heat map (or heatmap)](https://en.wikipedia.org/wiki/Heat_map) is a graphical representation of data where the individual values contained in a matrix are represented as colors.

Seaborn makes it easy to create a heatmap with [`seaborn.heatmap()`](https://seaborn.pydata.org/generated/seaborn.heatmap.html)

* Create a correlation matrix for your numeric variables using Pandas with [`DataFrame.corr()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html). That is, if your dataframe is called `df`, use `df.corr()`.
* Pass in your correlation matrix to `seaborn.heatmap()`, and annotate it with the parameter `annot=True`.
* Experiment with colormaps that are different from the default one and choose one that you think is best.  Comment on why you think so.
* Are there any interesting correlations? What are they?


```python
us_corr = df_youtube_us.corr()
us_corr
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>category_id</th>
      <th>views</th>
      <th>likes</th>
      <th>dislikes</th>
      <th>comment_count</th>
      <th>comments_disabled</th>
      <th>ratings_disabled</th>
      <th>video_error_or_removed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>category_id</th>
      <td>1.000000</td>
      <td>-0.168231</td>
      <td>-0.173921</td>
      <td>-0.033547</td>
      <td>-0.076307</td>
      <td>0.048949</td>
      <td>-0.013506</td>
      <td>-0.030011</td>
    </tr>
    <tr>
      <th>views</th>
      <td>-0.168231</td>
      <td>1.000000</td>
      <td>0.849177</td>
      <td>0.472213</td>
      <td>0.617621</td>
      <td>0.002677</td>
      <td>0.015355</td>
      <td>-0.002256</td>
    </tr>
    <tr>
      <th>likes</th>
      <td>-0.173921</td>
      <td>0.849177</td>
      <td>1.000000</td>
      <td>0.447186</td>
      <td>0.803057</td>
      <td>-0.028918</td>
      <td>-0.020888</td>
      <td>-0.002641</td>
    </tr>
    <tr>
      <th>dislikes</th>
      <td>-0.033547</td>
      <td>0.472213</td>
      <td>0.447186</td>
      <td>1.000000</td>
      <td>0.700184</td>
      <td>-0.004431</td>
      <td>-0.008230</td>
      <td>-0.001853</td>
    </tr>
    <tr>
      <th>comment_count</th>
      <td>-0.076307</td>
      <td>0.617621</td>
      <td>0.803057</td>
      <td>0.700184</td>
      <td>1.000000</td>
      <td>-0.028277</td>
      <td>-0.013819</td>
      <td>-0.003725</td>
    </tr>
    <tr>
      <th>comments_disabled</th>
      <td>0.048949</td>
      <td>0.002677</td>
      <td>-0.028918</td>
      <td>-0.004431</td>
      <td>-0.028277</td>
      <td>1.000000</td>
      <td>0.319230</td>
      <td>-0.002970</td>
    </tr>
    <tr>
      <th>ratings_disabled</th>
      <td>-0.013506</td>
      <td>0.015355</td>
      <td>-0.020888</td>
      <td>-0.008230</td>
      <td>-0.013819</td>
      <td>0.319230</td>
      <td>1.000000</td>
      <td>-0.001526</td>
    </tr>
    <tr>
      <th>video_error_or_removed</th>
      <td>-0.030011</td>
      <td>-0.002256</td>
      <td>-0.002641</td>
      <td>-0.001853</td>
      <td>-0.003725</td>
      <td>-0.002970</td>
      <td>-0.001526</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>



The above table shows the correlation matrix of numerical variables


```python
df_corr = us_corr.unstack().reset_index().rename(columns = {"level_0" : "var1", "level_1" : "var2", 0 : "correlation"})
mask_dups = (df_corr[['var1', 'var2']].apply(frozenset, axis=1).duplicated()) | (df_corr['var1'] == df_corr['var2']) 
df_corr = df_corr[~mask_dups]
df_corr.sort_values("correlation", key = abs, ascending = False).head(5)

```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>var1</th>
      <th>var2</th>
      <th>correlation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>10</th>
      <td>views</td>
      <td>likes</td>
      <td>0.849177</td>
    </tr>
    <tr>
      <th>20</th>
      <td>likes</td>
      <td>comment_count</td>
      <td>0.803057</td>
    </tr>
    <tr>
      <th>28</th>
      <td>dislikes</td>
      <td>comment_count</td>
      <td>0.700184</td>
    </tr>
    <tr>
      <th>12</th>
      <td>views</td>
      <td>comment_count</td>
      <td>0.617621</td>
    </tr>
    <tr>
      <th>11</th>
      <td>views</td>
      <td>dislikes</td>
      <td>0.472213</td>
    </tr>
  </tbody>
</table>
</div>



If 5 pairs with high correlation are selected, it is as above. Since views and likes, likes and comment counts have very high values â€‹â€‹of 0.8 or more, it can be seen that there is a strong linear relationship between the two variables.


```python
sns.heatmap(us_corr, annot = True, linewidths = .5, center = 0)
```




    <AxesSubplot:>




    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_81_1.png)
    


The above graph is a heatmap of the correlation matrix. Let's try with colormaps that are different from the default one.


```python
sns.heatmap(us_corr, annot = True, linewidths = .5, cmap="YlGnBu", center = 0)
```




    <AxesSubplot:>




    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_83_1.png)
    



```python
sns.heatmap(us_corr, annot = True, linewidths = .5, cmap="Blues", center = 0)
```




    <AxesSubplot:>




    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_84_1.png)
    



```python
sns.heatmap(us_corr, annot = True, linewidths = .5, cmap="BuPu", center = 0)
```




    <AxesSubplot:>




    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_85_1.png)
    



```python
sns.heatmap(us_corr, annot = True, linewidths = .5, cmap="Greens", center = 0)
```




    <AxesSubplot:>




    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_86_1.png)
    



```python
sns.heatmap(us_corr, annot = True, linewidths = .5, cmap="PiYG", center = 0)
```




    <AxesSubplot:>




    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_87_1.png)
    



```python
sns.heatmap(us_corr, annot = True, linewidths = .5, cmap="BrBG", center = 0)
```




    <AxesSubplot:>




    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_88_1.png)
    


I think BrBG is the best because the distinction between high and low values â€‹â€‹is clear.

Although correlation was obtained with all numerical columns, category_id, comments_disabled, ratings_disabled, and video_error_or_removed columns do not have numerical meaning. Therefore, it can be seen that only the views, likes, dislikes, and comment_counts columns that have real numerical meaning have high correlation values. Among them, the linear relationship between view and like, like and comment_count, and dislike and comment_count was strong.

## <span style="color:magenta"> Q3. For 15 points: Create and compare OLS models using variables of your choice, for a region of your choice </span>
* Use statsmodels to perform an ANOVA (categorical regression) of a variable of your choice as the dependent variable (for example, views) and the video category as the independent variable. Note that you need to use a categorical variable as your independent variable.
* Provide your interpretation of the results. 
* Create two different regression models where the dependent variable is the same, and the independent variables are different.  Note that your independent variable needs to be a continuous numerical variables. What does your interpretation say about the two models?


```python
sns.boxplot(x = "category_id", y = "dislikes", data = df_youtube_us, showfliers = False)
plt.show()
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_92_0.png)
    


The boxplot between category_id and dislikes in us region is as above. Among category_ids, 1, 10, and 20 seem to have a higher number of dislikes, especially compared to other categories. Let's check this numerically through anova analysis.



```python
lm0 = smf.ols("dislikes ~ category_id", data = df_youtube_us)
res0 = lm0.fit()
print(res0.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:               dislikes   R-squared:                       0.001
    Model:                            OLS   Adj. R-squared:                  0.001
    Method:                 Least Squares   F-statistic:                     46.13
    Date:                Tue, 15 Feb 2022   Prob (F-statistic):           1.12e-11
    Time:                        15:46:37   Log-Likelihood:            -4.7888e+05
    No. Observations:               40949   AIC:                         9.578e+05
    Df Residuals:                   40947   BIC:                         9.578e+05
    Df Model:                           1                                         
    Covariance Type:            nonrobust                                         
    ===============================================================================
                      coef    std err          t      P>|t|      [0.025      0.975]
    -------------------------------------------------------------------------------
    Intercept    6281.3987    404.626     15.524      0.000    5488.323    7074.474
    category_id  -128.6773     18.945     -6.792      0.000    -165.809     -91.545
    ==============================================================================
    Omnibus:                   118799.008   Durbin-Watson:                   1.996
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):       6797851742.784
    Skew:                          40.289   Prob(JB):                         0.00
    Kurtosis:                    1997.416   Cond. No.                         60.4
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.


When anova analysis is performed, the F-statistic probability is almost 0. This means that the null hypothesis that there is no significant difference between Generations can be rejected, so it can be seen that there is statistical difference in dislikes between category_id.


```python
tukeyhsd_res0 = pairwise_tukeyhsd(df_youtube_us["dislikes"], df_youtube_us["category_id"])
tukeyhsd_res0.summary()
```




<table class="simpletable">
<caption>Multiple Comparison of Means - Tukey HSD, FWER=0.05</caption>
<tr>
  <th>group1</th> <th>group2</th>  <th>meandiff</th>    <th>p-adj</th>    <th>lower</th>       <th>upper</th>    <th>reject</th>
</tr>
<tr>
     <td>1</td>      <td>2</td>   <td>-1957.8429</td>    <td>0.9</td>  <td>-7401.0862</td>   <td>3485.4004</td>   <td>False</td>
</tr>
<tr>
     <td>1</td>     <td>10</td>    <td>5317.0763</td>   <td>0.001</td>  <td>2933.8638</td>   <td>7700.2887</td>   <td>True</td> 
</tr>
<tr>
     <td>1</td>     <td>15</td>   <td>-2017.4434</td>    <td>0.9</td>  <td>-5863.9747</td>   <td>1829.0879</td>   <td>False</td>
</tr>
<tr>
     <td>1</td>     <td>17</td>    <td>-229.3424</td>    <td>0.9</td>  <td>-3173.1743</td>   <td>2714.4894</td>   <td>False</td>
</tr>
<tr>
     <td>1</td>     <td>19</td>   <td>-1743.8481</td>    <td>0.9</td>  <td>-7081.3482</td>   <td>3593.652</td>    <td>False</td>
</tr>
<tr>
     <td>1</td>     <td>20</td>    <td>8651.015</td>    <td>0.001</td>  <td>4634.1106</td>  <td>12667.9194</td>   <td>True</td> 
</tr>
<tr>
     <td>1</td>     <td>22</td>    <td>583.1195</td>     <td>0.9</td>  <td>-2102.9121</td>   <td>3269.151</td>    <td>False</td>
</tr>
<tr>
     <td>1</td>     <td>23</td>    <td>-499.1596</td>    <td>0.9</td>  <td>-3144.3733</td>   <td>2146.0541</td>   <td>False</td>
</tr>
<tr>
     <td>1</td>     <td>24</td>    <td>1723.6163</td>  <td>0.4009</td>  <td>-545.8102</td>   <td>3993.0428</td>   <td>False</td>
</tr>
<tr>
     <td>1</td>     <td>25</td>    <td>-909.9219</td>    <td>0.9</td>  <td>-3756.0023</td>   <td>1936.1585</td>   <td>False</td>
</tr>
<tr>
     <td>1</td>     <td>26</td>   <td>-1270.3971</td>    <td>0.9</td>  <td>-3825.2314</td>   <td>1284.4373</td>   <td>False</td>
</tr>
<tr>
     <td>1</td>     <td>27</td>   <td>-1774.2732</td>  <td>0.8543</td> <td>-4948.0451</td>   <td>1399.4986</td>   <td>False</td>
</tr>
<tr>
     <td>1</td>     <td>28</td>    <td>-696.3033</td>    <td>0.9</td>  <td>-3567.0137</td>   <td>2174.4071</td>   <td>False</td>
</tr>
<tr>
     <td>1</td>     <td>29</td>   <td>55486.1782</td>   <td>0.001</td> <td>42231.4548</td>  <td>68740.9016</td>   <td>True</td> 
</tr>
<tr>
     <td>1</td>     <td>43</td>   <td>-2160.7165</td>    <td>0.9</td>  <td>-15415.4399</td> <td>11094.0068</td>   <td>False</td>
</tr>
<tr>
     <td>2</td>     <td>10</td>    <td>7274.9192</td>   <td>0.001</td>  <td>2081.6175</td>  <td>12468.2209</td>   <td>True</td> 
</tr>
<tr>
     <td>2</td>     <td>15</td>    <td>-59.6005</td>     <td>0.9</td>  <td>-6066.8032</td>   <td>5947.6022</td>   <td>False</td>
</tr>
<tr>
     <td>2</td>     <td>17</td>    <td>1728.5005</td>    <td>0.9</td>  <td>-3744.7826</td>   <td>7201.7835</td>   <td>False</td>
</tr>
<tr>
     <td>2</td>     <td>19</td>    <td>213.9948</td>     <td>0.9</td>  <td>-6841.4704</td>    <td>7269.46</td>    <td>False</td>
</tr>
<tr>
     <td>2</td>     <td>20</td>   <td>10608.8579</td>   <td>0.001</td>  <td>4491.1621</td>  <td>16726.5537</td>   <td>True</td> 
</tr>
<tr>
     <td>2</td>     <td>22</td>    <td>2540.9624</td>    <td>0.9</td>  <td>-2798.0868</td>   <td>7880.0116</td>   <td>False</td>
</tr>
<tr>
     <td>2</td>     <td>23</td>    <td>1458.6833</td>    <td>0.9</td>  <td>-3859.9478</td>   <td>6777.3144</td>   <td>False</td>
</tr>
<tr>
     <td>2</td>     <td>24</td>    <td>3681.4592</td>  <td>0.5062</td> <td>-1460.6199</td>   <td>8823.5384</td>   <td>False</td>
</tr>
<tr>
     <td>2</td>     <td>25</td>    <td>1047.921</td>     <td>0.9</td>  <td>-4373.4123</td>   <td>6469.2543</td>   <td>False</td>
</tr>
<tr>
     <td>2</td>     <td>26</td>    <td>687.4458</td>     <td>0.9</td>  <td>-4586.8181</td>   <td>5961.7097</td>   <td>False</td>
</tr>
<tr>
     <td>2</td>     <td>27</td>    <td>183.5697</td>     <td>0.9</td>  <td>-5416.7436</td>   <td>5783.883</td>    <td>False</td>
</tr>
<tr>
     <td>2</td>     <td>28</td>    <td>1261.5396</td>    <td>0.9</td>  <td>-4172.7643</td>   <td>6695.8436</td>   <td>False</td>
</tr>
<tr>
     <td>2</td>     <td>29</td>   <td>57444.0211</td>   <td>0.001</td> <td>43409.1227</td>  <td>71478.9195</td>   <td>True</td> 
</tr>
<tr>
     <td>2</td>     <td>43</td>    <td>-202.8736</td>    <td>0.9</td>  <td>-14237.772</td>  <td>13832.0247</td>   <td>False</td>
</tr>
<tr>
    <td>10</td>     <td>15</td>   <td>-7334.5197</td>   <td>0.001</td> <td>-10818.3808</td> <td>-3850.6586</td>   <td>True</td> 
</tr>
<tr>
    <td>10</td>     <td>17</td>   <td>-5546.4187</td>   <td>0.001</td> <td>-7997.4655</td>  <td>-3095.3719</td>   <td>True</td> 
</tr>
<tr>
    <td>10</td>     <td>19</td>   <td>-7060.9244</td>   <td>0.001</td> <td>-12143.2853</td> <td>-1978.5635</td>   <td>True</td> 
</tr>
<tr>
    <td>10</td>     <td>20</td>    <td>3333.9387</td>  <td>0.1255</td>  <td>-337.1655</td>   <td>7005.0429</td>   <td>False</td>
</tr>
<tr>
    <td>10</td>     <td>22</td>   <td>-4733.9568</td>   <td>0.001</td> <td>-6868.4943</td>  <td>-2599.4193</td>   <td>True</td> 
</tr>
<tr>
    <td>10</td>     <td>23</td>   <td>-5816.2359</td>   <td>0.001</td> <td>-7899.1762</td>  <td>-3733.2956</td>   <td>True</td> 
</tr>
<tr>
    <td>10</td>     <td>24</td>    <td>-3593.46</td>    <td>0.001</td> <td>-5171.9977</td>  <td>-2014.9222</td>   <td>True</td> 
</tr>
<tr>
    <td>10</td>     <td>25</td>   <td>-6226.9982</td>   <td>0.001</td> <td>-8559.7344</td>  <td>-3894.2619</td>   <td>True</td> 
</tr>
<tr>
    <td>10</td>     <td>26</td>   <td>-6587.4734</td>   <td>0.001</td> <td>-8554.3652</td>  <td>-4620.5815</td>   <td>True</td> 
</tr>
<tr>
    <td>10</td>     <td>27</td>   <td>-7091.3495</td>   <td>0.001</td>  <td>-9814.273</td>   <td>-4368.426</td>   <td>True</td> 
</tr>
<tr>
    <td>10</td>     <td>28</td>   <td>-6013.3795</td>   <td>0.001</td> <td>-8376.1032</td>  <td>-3650.6559</td>   <td>True</td> 
</tr>
<tr>
    <td>10</td>     <td>29</td>   <td>50169.1019</td>   <td>0.001</td> <td>37015.0464</td>  <td>63323.1574</td>   <td>True</td> 
</tr>
<tr>
    <td>10</td>     <td>43</td>   <td>-7477.7928</td>  <td>0.8334</td> <td>-20631.8483</td>  <td>5676.2627</td>   <td>False</td>
</tr>
<tr>
    <td>15</td>     <td>17</td>    <td>1788.101</td>     <td>0.9</td>  <td>-2100.8233</td>   <td>5677.0253</td>   <td>False</td>
</tr>
<tr>
    <td>15</td>     <td>19</td>    <td>273.5953</td>     <td>0.9</td>  <td>-5637.9607</td>   <td>6185.1512</td>   <td>False</td>
</tr>
<tr>
    <td>15</td>     <td>20</td>   <td>10668.4584</td>   <td>0.001</td>  <td>5915.2377</td>  <td>15421.6792</td>   <td>True</td> 
</tr>
<tr>
    <td>15</td>     <td>22</td>    <td>2600.5629</td>  <td>0.5342</td> <td>-1097.0515</td>   <td>6298.1772</td>   <td>False</td>
</tr>
<tr>
    <td>15</td>     <td>23</td>    <td>1518.2838</td>    <td>0.9</td>  <td>-2149.7868</td>   <td>5186.3544</td>   <td>False</td>
</tr>
<tr>
    <td>15</td>     <td>24</td>    <td>3741.0597</td>  <td>0.0158</td>  <td>334.0254</td>    <td>7148.0941</td>   <td>True</td> 
</tr>
<tr>
    <td>15</td>     <td>25</td>    <td>1107.5215</td>    <td>0.9</td>  <td>-2707.9418</td>   <td>4922.9848</td>   <td>False</td>
</tr>
<tr>
    <td>15</td>     <td>26</td>    <td>747.0463</td>     <td>0.9</td>  <td>-2856.3916</td>   <td>4350.4843</td>   <td>False</td>
</tr>
<tr>
    <td>15</td>     <td>27</td>    <td>243.1702</td>     <td>0.9</td>   <td>-3822.591</td>   <td>4308.9314</td>   <td>False</td>
</tr>
<tr>
    <td>15</td>     <td>28</td>    <td>1321.1401</td>    <td>0.9</td>  <td>-2512.7306</td>   <td>5155.0108</td>   <td>False</td>
</tr>
<tr>
    <td>15</td>     <td>29</td>   <td>57503.6216</td>   <td>0.001</td> <td>44007.5008</td>  <td>70999.7424</td>   <td>True</td> 
</tr>
<tr>
    <td>15</td>     <td>43</td>    <td>-143.2731</td>    <td>0.9</td>  <td>-13639.394</td>  <td>13352.8477</td>   <td>False</td>
</tr>
<tr>
    <td>17</td>     <td>19</td>   <td>-1514.5057</td>    <td>0.9</td>  <td>-6882.6372</td>   <td>3853.6259</td>   <td>False</td>
</tr>
<tr>
    <td>17</td>     <td>20</td>    <td>8880.3574</td>   <td>0.001</td>  <td>4822.8397</td>  <td>12937.8752</td>   <td>True</td> 
</tr>
<tr>
    <td>17</td>     <td>22</td>    <td>812.4619</td>     <td>0.9</td>  <td>-1933.9347</td>   <td>3558.8586</td>   <td>False</td>
</tr>
<tr>
    <td>17</td>     <td>23</td>    <td>-269.8172</td>    <td>0.9</td>  <td>-2976.3065</td>   <td>2436.6722</td>   <td>False</td>
</tr>
<tr>
    <td>17</td>     <td>24</td>    <td>1952.9588</td>  <td>0.2363</td>  <td>-387.6022</td>   <td>4293.5197</td>   <td>False</td>
</tr>
<tr>
    <td>17</td>     <td>25</td>    <td>-680.5795</td>    <td>0.9</td>  <td>-3583.6989</td>    <td>2222.54</td>    <td>False</td>
</tr>
<tr>
    <td>17</td>     <td>26</td>   <td>-1041.0546</td>    <td>0.9</td>  <td>-3659.2807</td>   <td>1577.1714</td>   <td>False</td>
</tr>
<tr>
    <td>17</td>     <td>27</td>   <td>-1544.9308</td>    <td>0.9</td>  <td>-4769.9512</td>   <td>1680.0896</td>   <td>False</td>
</tr>
<tr>
    <td>17</td>     <td>28</td>    <td>-466.9608</td>    <td>0.9</td>  <td>-3394.2304</td>   <td>2460.3087</td>   <td>False</td>
</tr>
<tr>
    <td>17</td>     <td>29</td>   <td>55715.5206</td>   <td>0.001</td> <td>42448.4328</td>  <td>68982.6085</td>   <td>True</td> 
</tr>
<tr>
    <td>17</td>     <td>43</td>   <td>-1931.3741</td>    <td>0.9</td>  <td>-15198.4619</td> <td>11335.7138</td>   <td>False</td>
</tr>
<tr>
    <td>19</td>     <td>20</td>   <td>10394.8631</td>   <td>0.001</td>  <td>4371.0594</td>  <td>16418.6669</td>   <td>True</td> 
</tr>
<tr>
    <td>19</td>     <td>22</td>    <td>2326.9676</td>    <td>0.9</td>  <td>-2904.2327</td>   <td>7558.1679</td>   <td>False</td>
</tr>
<tr>
    <td>19</td>     <td>23</td>    <td>1244.6885</td>    <td>0.9</td>   <td>-3965.671</td>   <td>6455.048</td>    <td>False</td>
</tr>
<tr>
    <td>19</td>     <td>24</td>    <td>3467.4644</td>  <td>0.5652</td> <td>-1562.5442</td>   <td>8497.4731</td>   <td>False</td>
</tr>
<tr>
    <td>19</td>     <td>25</td>    <td>833.9262</td>     <td>0.9</td>   <td>-4481.228</td>   <td>6149.0804</td>   <td>False</td>
</tr>
<tr>
    <td>19</td>     <td>26</td>     <td>473.451</td>     <td>0.9</td>  <td>-4691.6113</td>   <td>5638.5134</td>   <td>False</td>
</tr>
<tr>
    <td>19</td>     <td>27</td>    <td>-30.4251</td>     <td>0.9</td>  <td>-5528.0172</td>   <td>5467.1669</td>   <td>False</td>
</tr>
<tr>
    <td>19</td>     <td>28</td>    <td>1047.5448</td>    <td>0.9</td>  <td>-4280.8385</td>   <td>6375.9282</td>   <td>False</td>
</tr>
<tr>
    <td>19</td>     <td>29</td>   <td>57230.0263</td>   <td>0.001</td> <td>43235.7996</td>   <td>71224.253</td>   <td>True</td> 
</tr>
<tr>
    <td>19</td>     <td>43</td>    <td>-416.8684</td>    <td>0.9</td>  <td>-14411.0952</td> <td>13577.3583</td>   <td>False</td>
</tr>
<tr>
    <td>20</td>     <td>22</td>   <td>-8067.8955</td>   <td>0.001</td> <td>-11942.4368</td> <td>-4193.3543</td>   <td>True</td> 
</tr>
<tr>
    <td>20</td>     <td>23</td>   <td>-9150.1746</td>   <td>0.001</td> <td>-12996.5313</td> <td>-5303.8179</td>   <td>True</td> 
</tr>
<tr>
    <td>20</td>     <td>24</td>   <td>-6927.3987</td>   <td>0.001</td> <td>-10525.6762</td> <td>-3329.1212</td>   <td>True</td> 
</tr>
<tr>
    <td>20</td>     <td>25</td>   <td>-9560.9369</td>   <td>0.001</td> <td>-13548.1011</td> <td>-5573.7727</td>   <td>True</td> 
</tr>
<tr>
    <td>20</td>     <td>26</td>   <td>-9921.4121</td>   <td>0.001</td> <td>-13706.182</td>  <td>-6136.6422</td>   <td>True</td> 
</tr>
<tr>
    <td>20</td>     <td>27</td>   <td>-10425.2882</td>  <td>0.001</td> <td>-14652.5961</td> <td>-6197.9803</td>   <td>True</td> 
</tr>
<tr>
    <td>20</td>     <td>28</td>   <td>-9347.3183</td>   <td>0.001</td> <td>-13352.1007</td> <td>-5342.5358</td>   <td>True</td> 
</tr>
<tr>
    <td>20</td>     <td>29</td>   <td>46835.1632</td>   <td>0.001</td> <td>33289.4998</td>  <td>60380.8265</td>   <td>True</td> 
</tr>
<tr>
    <td>20</td>     <td>43</td>   <td>-10811.7315</td> <td>0.3086</td> <td>-24357.3949</td>  <td>2733.9318</td>   <td>False</td>
</tr>
<tr>
    <td>22</td>     <td>23</td>   <td>-1082.2791</td>    <td>0.9</td>  <td>-3505.8517</td>   <td>1341.2935</td>   <td>False</td>
</tr>
<tr>
    <td>22</td>     <td>24</td>    <td>1140.4968</td>  <td>0.8337</td>  <td>-866.2033</td>   <td>3147.1969</td>   <td>False</td>
</tr>
<tr>
    <td>22</td>     <td>25</td>   <td>-1493.0414</td>  <td>0.8405</td>  <td>-4134.39</td>    <td>1148.3072</td>   <td>False</td>
</tr>
<tr>
    <td>22</td>     <td>26</td>   <td>-1853.5166</td>  <td>0.3103</td> <td>-4178.1084</td>   <td>471.0753</td>    <td>False</td>
</tr>
<tr>
    <td>22</td>     <td>27</td>   <td>-2357.3927</td>  <td>0.3306</td> <td>-5348.9436</td>   <td>634.1581</td>    <td>False</td>
</tr>
<tr>
    <td>22</td>     <td>28</td>   <td>-1279.4228</td>    <td>0.9</td>  <td>-3947.2921</td>   <td>1388.4466</td>   <td>False</td>
</tr>
<tr>
    <td>22</td>     <td>29</td>   <td>54903.0587</td>   <td>0.001</td> <td>41690.7826</td>  <td>68115.3348</td>   <td>True</td> 
</tr>
<tr>
    <td>22</td>     <td>43</td>    <td>-2743.836</td>    <td>0.9</td>  <td>-15956.1121</td> <td>10468.4401</td>   <td>False</td>
</tr>
<tr>
    <td>23</td>     <td>24</td>    <td>2222.7759</td>  <td>0.0094</td>  <td>271.0497</td>    <td>4174.5022</td>   <td>True</td> 
</tr>
<tr>
    <td>23</td>     <td>25</td>    <td>-410.7623</td>    <td>0.9</td>  <td>-3010.5916</td>   <td>2189.067</td>    <td>False</td>
</tr>
<tr>
    <td>23</td>     <td>26</td>    <td>-771.2375</td>    <td>0.9</td>  <td>-3048.5423</td>   <td>1506.0674</td>   <td>False</td>
</tr>
<tr>
    <td>23</td>     <td>27</td>   <td>-1275.1136</td>    <td>0.9</td>  <td>-4230.0699</td>   <td>1679.8426</td>   <td>False</td>
</tr>
<tr>
    <td>23</td>     <td>28</td>    <td>-197.1437</td>    <td>0.9</td>   <td>-2823.913</td>   <td>2429.6256</td>   <td>False</td>
</tr>
<tr>
    <td>23</td>     <td>29</td>   <td>55985.3378</td>   <td>0.001</td> <td>42781.2994</td>  <td>69189.3762</td>   <td>True</td> 
</tr>
<tr>
    <td>23</td>     <td>43</td>   <td>-1661.5569</td>    <td>0.9</td>  <td>-14865.5953</td> <td>11542.4815</td>   <td>False</td>
</tr>
<tr>
    <td>24</td>     <td>25</td>   <td>-2633.5382</td>  <td>0.0048</td> <td>-4849.8986</td>   <td>-417.1778</td>   <td>True</td> 
</tr>
<tr>
    <td>24</td>     <td>26</td>   <td>-2994.0134</td>   <td>0.001</td> <td>-4821.3772</td>  <td>-1166.6496</td>   <td>True</td> 
</tr>
<tr>
    <td>24</td>     <td>27</td>   <td>-3497.8896</td>   <td>0.001</td> <td>-6121.8003</td>   <td>-873.9788</td>   <td>True</td> 
</tr>
<tr>
    <td>24</td>     <td>28</td>   <td>-2419.9196</td>  <td>0.0207</td> <td>-4667.8204</td>   <td>-172.0188</td>   <td>True</td> 
</tr>
<tr>
    <td>24</td>     <td>29</td>   <td>53762.5619</td>   <td>0.001</td> <td>40628.6451</td>  <td>66896.4787</td>   <td>True</td> 
</tr>
<tr>
    <td>24</td>     <td>43</td>   <td>-3884.3329</td>    <td>0.9</td>  <td>-17018.2497</td>  <td>9249.584</td>    <td>False</td>
</tr>
<tr>
    <td>25</td>     <td>26</td>    <td>-360.4752</td>    <td>0.9</td>  <td>-2868.2901</td>   <td>2147.3397</td>   <td>False</td>
</tr>
<tr>
    <td>25</td>     <td>27</td>    <td>-864.3513</td>    <td>0.9</td>  <td>-4000.3973</td>   <td>2271.6947</td>   <td>False</td>
</tr>
<tr>
    <td>25</td>     <td>28</td>    <td>213.6186</td>     <td>0.9</td>  <td>-2615.3273</td>   <td>3042.5645</td>   <td>False</td>
</tr>
<tr>
    <td>25</td>     <td>29</td>   <td>56396.1001</td>   <td>0.001</td> <td>43150.3593</td>  <td>69641.8409</td>   <td>True</td> 
</tr>
<tr>
    <td>25</td>     <td>43</td>   <td>-1250.7946</td>    <td>0.9</td>  <td>-14496.5354</td> <td>11994.9461</td>   <td>False</td>
</tr>
<tr>
    <td>26</td>     <td>27</td>    <td>-503.8762</td>    <td>0.9</td>  <td>-3378.2091</td>   <td>2370.4567</td>   <td>False</td>
</tr>
<tr>
    <td>26</td>     <td>28</td>    <td>574.0938</td>     <td>0.9</td>  <td>-1961.6388</td>   <td>3109.8264</td>   <td>False</td>
</tr>
<tr>
    <td>26</td>     <td>29</td>   <td>56756.5753</td>   <td>0.001</td> <td>43570.3456</td>   <td>69942.805</td>   <td>True</td> 
</tr>
<tr>
    <td>26</td>     <td>43</td>    <td>-890.3195</td>    <td>0.9</td>  <td>-14076.5491</td> <td>12295.9102</td>   <td>False</td>
</tr>
<tr>
    <td>27</td>     <td>28</td>     <td>1077.97</td>     <td>0.9</td>  <td>-2080.4456</td>   <td>4236.3856</td>   <td>False</td>
</tr>
<tr>
    <td>27</td>     <td>29</td>   <td>57260.4514</td>   <td>0.001</td> <td>43940.4551</td>  <td>70580.4478</td>   <td>True</td> 
</tr>
<tr>
    <td>27</td>     <td>43</td>    <td>-386.4433</td>    <td>0.9</td>  <td>-13706.4396</td>  <td>12933.553</td>   <td>False</td>
</tr>
<tr>
    <td>28</td>     <td>29</td>   <td>56182.4815</td>   <td>0.001</td> <td>42931.4267</td>  <td>69433.5362</td>   <td>True</td> 
</tr>
<tr>
    <td>28</td>     <td>43</td>   <td>-1464.4133</td>    <td>0.9</td>  <td>-14715.468</td>  <td>11786.6415</td>   <td>False</td>
</tr>
<tr>
    <td>29</td>     <td>43</td>   <td>-57646.8947</td>  <td>0.001</td> <td>-76168.1573</td> <td>-39125.6322</td>  <td>True</td> 
</tr>
</table>



The above shows the turkeyhsd table for dislikes between category_id. When the reject column is true, it can be interpreted that there is a statistically significant difference between the two groups. As can be seen from the table above, there are many pairs with a significant difference in dislikes between category_id.

Also, after merging the data from all 5 regions, let's check if there is a significant difference in the dislike value by region.


```python
df_youtube = pd.concat([df_youtube_canada, df_youtube_france, df_youtube_us, df_youtube_germany,
                        df_youtube_gb], axis = 0, ignore_index = True)
```


```python
sns.boxplot(x = "country", y = "dislikes", data=df_youtube, showfliers = False)
```




    <AxesSubplot:xlabel='country', ylabel='dislikes'>




    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_100_1.png)
    


Looking at the boxplot, it seems that United States and Great Britain regions have particularly high dislikes. Let's check this numerically through anova analysis.


```python
lm0 = smf.ols("dislikes ~ country", data = df_youtube)
res0 = lm0.fit()
print(res0.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:               dislikes   R-squared:                       0.007
    Model:                            OLS   Adj. R-squared:                  0.007
    Method:                 Least Squares   F-statistic:                     365.5
    Date:                Tue, 15 Feb 2022   Prob (F-statistic):          3.44e-314
    Time:                        15:52:38   Log-Likelihood:            -2.3623e+06
    No. Observations:              202310   AIC:                         4.725e+06
    Df Residuals:                  202305   BIC:                         4.725e+06
    Df Model:                           4                                         
    Covariance Type:            nonrobust                                         
    ============================================================================================
                                   coef    std err          t      P>|t|      [0.025      0.975]
    --------------------------------------------------------------------------------------------
    Intercept                 2009.1954    140.942     14.256      0.000    1732.953    2285.438
    country[T.France]        -1194.2331    199.514     -5.986      0.000   -1585.275    -803.191
    country[T.Germany]        -612.0595    199.372     -3.070      0.002   -1002.823    -221.296
    country[T.Great Britain]  5603.3645    201.822     27.764      0.000    5207.798    5998.931
    country[T.United States]  1702.2054    199.239      8.544      0.000    1311.702    2092.709
    ==============================================================================
    Omnibus:                   609442.615   Durbin-Watson:                   1.976
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):      50644814857.182
    Skew:                          44.672   Prob(JB):                         0.00
    Kurtosis:                    2452.490   Cond. No.                         5.80
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.


When anova analysis is performed, the F-statistic probability is almost 0. This means that the null hypothesis that there is no significant difference between Generations can be rejected, so it can be seen that there is statistical difference in dislikes between 5 regeions.


```python
tukeyhsd_res0 = pairwise_tukeyhsd(df_youtube["dislikes"], df_youtube["country"])
tukeyhsd_res0.summary()
```




<table class="simpletable">
<caption>Multiple Comparison of Means - Tukey HSD, FWER=0.05</caption>
<tr>
     <th>group1</th>        <th>group2</th>      <th>meandiff</th>   <th>p-adj</th>    <th>lower</th>      <th>upper</th>   <th>reject</th>
</tr>
<tr>
     <td>Canada</td>        <td>France</td>     <td>-1194.2331</td>  <td>0.001</td> <td>-1738.4618</td>  <td>-650.0043</td>  <td>True</td> 
</tr>
<tr>
     <td>Canada</td>        <td>Germany</td>     <td>-612.0595</td> <td>0.0182</td> <td>-1155.9009</td>  <td>-68.2181</td>   <td>True</td> 
</tr>
<tr>
     <td>Canada</td>     <td>Great Britain</td>  <td>5603.3645</td>  <td>0.001</td>  <td>5052.839</td>   <td>6153.8901</td>  <td>True</td> 
</tr>
<tr>
     <td>Canada</td>     <td>United States</td>  <td>1702.2054</td>  <td>0.001</td>  <td>1158.7263</td>  <td>2245.6846</td>  <td>True</td> 
</tr>
<tr>
     <td>France</td>        <td>Germany</td>     <td>582.1735</td>  <td>0.0291</td>   <td>37.8085</td>   <td>1126.5386</td>  <td>True</td> 
</tr>
<tr>
     <td>France</td>     <td>Great Britain</td>  <td>6797.5976</td>  <td>0.001</td>  <td>6246.5548</td>  <td>7348.6404</td>  <td>True</td> 
</tr>
<tr>
     <td>France</td>     <td>United States</td>  <td>2896.4385</td>  <td>0.001</td>  <td>2352.4353</td>  <td>3440.4417</td>  <td>True</td> 
</tr>
<tr>
     <td>Germany</td>    <td>Great Britain</td>  <td>6215.4241</td>  <td>0.001</td>  <td>5664.7638</td>  <td>6766.0843</td>  <td>True</td> 
</tr>
<tr>
     <td>Germany</td>    <td>United States</td>  <td>2314.265</td>   <td>0.001</td>  <td>1770.6493</td>  <td>2857.8806</td>  <td>True</td> 
</tr>
<tr>
  <td>Great Britain</td> <td>United States</td> <td>-3901.1591</td>  <td>0.001</td> <td>-4451.4617</td> <td>-3350.8565</td>  <td>True</td> 
</tr>
</table>



The above shows the turkeyhsd table for dislikes between regeions. When the reject column is true, it can be interpreted that there is a statistically significant difference between the two groups. As can be seen from the table above, All different regeion pairs showed statistically significant differences in dislike values.

# Part 2: Answer the questions below based on the *[PokÃ©mon dataset](https://www.kaggle.com/abcsds/pokemon)* </span>
- Write Python code that can answer the following questions, and
- Explain your answers in plain English. 


```python
df_pokemon = pd.read_csv("./data/Pokemon.csv")
```


```python
df_pokemon.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>#</th>
      <th>Name</th>
      <th>Type 1</th>
      <th>Type 2</th>
      <th>Total</th>
      <th>HP</th>
      <th>Attack</th>
      <th>Defense</th>
      <th>Sp. Atk</th>
      <th>Sp. Def</th>
      <th>Speed</th>
      <th>Generation</th>
      <th>Legendary</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>Bulbasaur</td>
      <td>Grass</td>
      <td>Poison</td>
      <td>318</td>
      <td>45</td>
      <td>49</td>
      <td>49</td>
      <td>65</td>
      <td>65</td>
      <td>45</td>
      <td>1</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>Ivysaur</td>
      <td>Grass</td>
      <td>Poison</td>
      <td>405</td>
      <td>60</td>
      <td>62</td>
      <td>63</td>
      <td>80</td>
      <td>80</td>
      <td>60</td>
      <td>1</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Venusaur</td>
      <td>Grass</td>
      <td>Poison</td>
      <td>525</td>
      <td>80</td>
      <td>82</td>
      <td>83</td>
      <td>100</td>
      <td>100</td>
      <td>80</td>
      <td>1</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>VenusaurMega Venusaur</td>
      <td>Grass</td>
      <td>Poison</td>
      <td>625</td>
      <td>80</td>
      <td>100</td>
      <td>123</td>
      <td>122</td>
      <td>120</td>
      <td>80</td>
      <td>1</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>Charmander</td>
      <td>Fire</td>
      <td>NaN</td>
      <td>309</td>
      <td>39</td>
      <td>52</td>
      <td>43</td>
      <td>60</td>
      <td>50</td>
      <td>65</td>
      <td>1</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>



Delete the "#" column because it is an index.


```python
df_pokemon.drop("#", axis = 1, inplace = True)
df_pokemon.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Name</th>
      <th>Type 1</th>
      <th>Type 2</th>
      <th>Total</th>
      <th>HP</th>
      <th>Attack</th>
      <th>Defense</th>
      <th>Sp. Atk</th>
      <th>Sp. Def</th>
      <th>Speed</th>
      <th>Generation</th>
      <th>Legendary</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Bulbasaur</td>
      <td>Grass</td>
      <td>Poison</td>
      <td>318</td>
      <td>45</td>
      <td>49</td>
      <td>49</td>
      <td>65</td>
      <td>65</td>
      <td>45</td>
      <td>1</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Ivysaur</td>
      <td>Grass</td>
      <td>Poison</td>
      <td>405</td>
      <td>60</td>
      <td>62</td>
      <td>63</td>
      <td>80</td>
      <td>80</td>
      <td>60</td>
      <td>1</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Venusaur</td>
      <td>Grass</td>
      <td>Poison</td>
      <td>525</td>
      <td>80</td>
      <td>82</td>
      <td>83</td>
      <td>100</td>
      <td>100</td>
      <td>80</td>
      <td>1</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>VenusaurMega Venusaur</td>
      <td>Grass</td>
      <td>Poison</td>
      <td>625</td>
      <td>80</td>
      <td>100</td>
      <td>123</td>
      <td>122</td>
      <td>120</td>
      <td>80</td>
      <td>1</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Charmander</td>
      <td>Fire</td>
      <td>NaN</td>
      <td>309</td>
      <td>39</td>
      <td>52</td>
      <td>43</td>
      <td>60</td>
      <td>50</td>
      <td>65</td>
      <td>1</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_pokemon.shape
```




    (800, 12)



Pokemon data consists of a total of 800 rows and 12 columns.


```python
df_pokemon.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 800 entries, 0 to 799
    Data columns (total 12 columns):
     #   Column      Non-Null Count  Dtype 
    ---  ------      --------------  ----- 
     0   Name        800 non-null    object
     1   Type 1      800 non-null    object
     2   Type 2      414 non-null    object
     3   Total       800 non-null    int64 
     4   HP          800 non-null    int64 
     5   Attack      800 non-null    int64 
     6   Defense     800 non-null    int64 
     7   Sp. Atk     800 non-null    int64 
     8   Sp. Def     800 non-null    int64 
     9   Speed       800 non-null    int64 
     10  Generation  800 non-null    int64 
     11  Legendary   800 non-null    bool  
    dtypes: bool(1), int64(8), object(3)
    memory usage: 69.7+ KB


It can be seen that only type2 has a missing value.

## <span style="color:magenta"> Q4. For 10 Points: Plot the pairs of different ability points (HP, Attack, Sp. Attack, Defense, etc.). </span>

* Which pairs have the most/least correlation coefficients?


```python
sns.pairplot(data = df_pokemon, vars = ["Total", "HP", "Attack", "Defense", "Sp. Atk", "Sp. Def", "Speed"], \
             kind = "reg", plot_kws={'line_kws':{'color':'red'}, 'scatter_kws': {'alpha': 0.5}}, corner = True)
```




    <seaborn.axisgrid.PairGrid at 0x7f77fbf8e9d0>




    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_116_1.png)
    


The graph above is a pair plot of different abilities. Through the red regression line in each scatter plot, it is possible to check how much linear relationship there is between the two variables. When visually confirmed, the linear relationship between HP and Attack, and Total and Sp.Atk was strongest, and the linear relationship between Defense and Speed was the smallest. Let's check this numerically.


```python
pokemon_ability_corr = df_pokemon[["Total", "HP", "Attack", "Defense", "Sp. Atk", "Sp. Def", "Speed"]].corr()
pokemon_ability_corr
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Total</th>
      <th>HP</th>
      <th>Attack</th>
      <th>Defense</th>
      <th>Sp. Atk</th>
      <th>Sp. Def</th>
      <th>Speed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Total</th>
      <td>1.000000</td>
      <td>0.618748</td>
      <td>0.736211</td>
      <td>0.612787</td>
      <td>0.747250</td>
      <td>0.717609</td>
      <td>0.575943</td>
    </tr>
    <tr>
      <th>HP</th>
      <td>0.618748</td>
      <td>1.000000</td>
      <td>0.422386</td>
      <td>0.239622</td>
      <td>0.362380</td>
      <td>0.378718</td>
      <td>0.175952</td>
    </tr>
    <tr>
      <th>Attack</th>
      <td>0.736211</td>
      <td>0.422386</td>
      <td>1.000000</td>
      <td>0.438687</td>
      <td>0.396362</td>
      <td>0.263990</td>
      <td>0.381240</td>
    </tr>
    <tr>
      <th>Defense</th>
      <td>0.612787</td>
      <td>0.239622</td>
      <td>0.438687</td>
      <td>1.000000</td>
      <td>0.223549</td>
      <td>0.510747</td>
      <td>0.015227</td>
    </tr>
    <tr>
      <th>Sp. Atk</th>
      <td>0.747250</td>
      <td>0.362380</td>
      <td>0.396362</td>
      <td>0.223549</td>
      <td>1.000000</td>
      <td>0.506121</td>
      <td>0.473018</td>
    </tr>
    <tr>
      <th>Sp. Def</th>
      <td>0.717609</td>
      <td>0.378718</td>
      <td>0.263990</td>
      <td>0.510747</td>
      <td>0.506121</td>
      <td>1.000000</td>
      <td>0.259133</td>
    </tr>
    <tr>
      <th>Speed</th>
      <td>0.575943</td>
      <td>0.175952</td>
      <td>0.381240</td>
      <td>0.015227</td>
      <td>0.473018</td>
      <td>0.259133</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>




```python
sns.heatmap(pokemon_ability_corr, annot = True, linewidths = .5, cmap="BrBG", center = 0)
```




    <AxesSubplot:>




    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_119_1.png)
    


Looking at the above table and heat map, it can be seen that total has a strong linear relationship with all other abilities. Since the stat name is total, it can be inferred that total is the sum of other abilities.


```python
np.sum(df_pokemon.Total != df_pokemon["HP"] + df_pokemon["Attack"] + df_pokemon["Defense"] + \
                           df_pokemon["Sp. Atk"] + df_pokemon["Sp. Def"] + df_pokemon["Speed"])
```




    0



As expected, it can be seen that total is the simple sum of the remaining 6 ability values. Therefore, let's check the linear relationship between the other abilities except for the total ability.


```python
pokemon_ability_corr = df_pokemon[["HP", "Attack", "Defense", "Sp. Atk", "Sp. Def", "Speed"]].corr()
pokemon_ability_corr
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>HP</th>
      <th>Attack</th>
      <th>Defense</th>
      <th>Sp. Atk</th>
      <th>Sp. Def</th>
      <th>Speed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>HP</th>
      <td>1.000000</td>
      <td>0.422386</td>
      <td>0.239622</td>
      <td>0.362380</td>
      <td>0.378718</td>
      <td>0.175952</td>
    </tr>
    <tr>
      <th>Attack</th>
      <td>0.422386</td>
      <td>1.000000</td>
      <td>0.438687</td>
      <td>0.396362</td>
      <td>0.263990</td>
      <td>0.381240</td>
    </tr>
    <tr>
      <th>Defense</th>
      <td>0.239622</td>
      <td>0.438687</td>
      <td>1.000000</td>
      <td>0.223549</td>
      <td>0.510747</td>
      <td>0.015227</td>
    </tr>
    <tr>
      <th>Sp. Atk</th>
      <td>0.362380</td>
      <td>0.396362</td>
      <td>0.223549</td>
      <td>1.000000</td>
      <td>0.506121</td>
      <td>0.473018</td>
    </tr>
    <tr>
      <th>Sp. Def</th>
      <td>0.378718</td>
      <td>0.263990</td>
      <td>0.510747</td>
      <td>0.506121</td>
      <td>1.000000</td>
      <td>0.259133</td>
    </tr>
    <tr>
      <th>Speed</th>
      <td>0.175952</td>
      <td>0.381240</td>
      <td>0.015227</td>
      <td>0.473018</td>
      <td>0.259133</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>




```python
sns.heatmap(pokemon_ability_corr, annot = True, linewidths = .5, cmap="BrBG", center = 0)
```




    <AxesSubplot:>




    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_124_1.png)
    


When Total was excluded, Defense and Sp.Def, and Sp.Atk and Sp.Def showed the highest correlation at 0.51. Also, since the correlation between Speed and Defense is 0.015, as confirmed in the pair plot earlier, it can be seen that there is little linear relationship between the two abilities.

## <span style="color:magenta"> Q5. For 15 Points: Plot the distribution of ability points per PokÃ©mon type </span>

* How would you describe each PokÃ©mon type with different ability points?

There are two types of Pokemon: Type1 and Type2.


```python
df_pokemon.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 800 entries, 0 to 799
    Data columns (total 12 columns):
     #   Column      Non-Null Count  Dtype 
    ---  ------      --------------  ----- 
     0   Name        800 non-null    object
     1   Type 1      800 non-null    object
     2   Type 2      414 non-null    object
     3   Total       800 non-null    int64 
     4   HP          800 non-null    int64 
     5   Attack      800 non-null    int64 
     6   Defense     800 non-null    int64 
     7   Sp. Atk     800 non-null    int64 
     8   Sp. Def     800 non-null    int64 
     9   Speed       800 non-null    int64 
     10  Generation  800 non-null    int64 
     11  Legendary   800 non-null    bool  
    dtypes: bool(1), int64(8), object(3)
    memory usage: 69.7+ KB


Looking at the table above, all Pokemon have Type1, but about half do not have Type2.


```python
print(sorted(df_pokemon["Type 1"].unique()))
```

    ['Bug', 'Dark', 'Dragon', 'Electric', 'Fairy', 'Fighting', 'Fire', 'Flying', 'Ghost', 'Grass', 'Ground', 'Ice', 'Normal', 'Poison', 'Psychic', 'Rock', 'Steel', 'Water']


There are a total of 18 types in Type1 as shown in the list above.


```python
print(sorted(df_pokemon[df_pokemon["Type 2"].isnull() == False]["Type 2"].unique()))
```

    ['Bug', 'Dark', 'Dragon', 'Electric', 'Fairy', 'Fighting', 'Fire', 'Flying', 'Ghost', 'Grass', 'Ground', 'Ice', 'Normal', 'Poison', 'Psychic', 'Rock', 'Steel', 'Water']


As in the list above, in Type2, there are a total of 18 types. If we compare Type1 and Type2, we can confirm that they are all the same. It is said that the order of Type1 and Type2 is not important in the real Pokemon world. (https://forums.serebii.net/threads/does-type-order-really-matter.316547/) 


```python
# Create data by combining type1 and type2 columns as one Type column. Dual type Pokemon will have 2 rows.
df_pokemon_type_indifference = df_pokemon[['Name', 'Total', 'HP', 'Attack', 'Defense', 'Sp. Atk', \
                                           'Sp. Def', 'Speed', 'Generation', 'Legendary']] \
                                   .merge(pd.concat([df_pokemon[["Name", "Type 1"]].rename(columns = {"Type 1" : "Type"}), \
                                                     df_pokemon[["Name", "Type 2"]].rename(columns = {"Type 2" : "Type"})], \
                                                     axis = 0, ignore_index = True), \
                                          on = "Name", how = "left") \
                                   [["Name", "Type", "Total", "HP", "Attack", 'Defense', 'Sp. Atk', \
                                     'Sp. Def', 'Speed', 'Generation', 'Legendary']]
```


```python
type_count = df_pokemon_type_indifference.Type.value_counts().reset_index().rename(columns = {"index" : "Type", "Type" : "Total Count"}) \
             .merge(df_pokemon[df_pokemon["Type 2"].isnull()]["Type 1"].value_counts().reset_index() \
             .rename(columns = {"index" : "Type", "Type 1" : "Single-type Count"}), on = "Type", how = "left")
type_count["Single-type Ratio"] = np.round(type_count["Single-type Count"] / type_count["Total Count"] * 100, 2)
type_count
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Type</th>
      <th>Total Count</th>
      <th>Single-type Count</th>
      <th>Single-type Ratio</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Water</td>
      <td>126</td>
      <td>59</td>
      <td>46.83</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Normal</td>
      <td>102</td>
      <td>61</td>
      <td>59.80</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Flying</td>
      <td>101</td>
      <td>2</td>
      <td>1.98</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Grass</td>
      <td>95</td>
      <td>33</td>
      <td>34.74</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Psychic</td>
      <td>90</td>
      <td>38</td>
      <td>42.22</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Bug</td>
      <td>72</td>
      <td>17</td>
      <td>23.61</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Ground</td>
      <td>67</td>
      <td>13</td>
      <td>19.40</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Fire</td>
      <td>64</td>
      <td>28</td>
      <td>43.75</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Poison</td>
      <td>62</td>
      <td>15</td>
      <td>24.19</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Rock</td>
      <td>58</td>
      <td>9</td>
      <td>15.52</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Fighting</td>
      <td>53</td>
      <td>20</td>
      <td>37.74</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Dark</td>
      <td>51</td>
      <td>10</td>
      <td>19.61</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Dragon</td>
      <td>50</td>
      <td>11</td>
      <td>22.00</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Electric</td>
      <td>50</td>
      <td>27</td>
      <td>54.00</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Steel</td>
      <td>49</td>
      <td>5</td>
      <td>10.20</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Ghost</td>
      <td>46</td>
      <td>10</td>
      <td>21.74</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Fairy</td>
      <td>40</td>
      <td>15</td>
      <td>37.50</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Ice</td>
      <td>38</td>
      <td>13</td>
      <td>34.21</td>
    </tr>
  </tbody>
</table>
</div>




```python
plt.figure(figsize = (15, 7))

# bar graph for total pokemon count
color = "darkblue"
ax1 = sns.barplot(x = "Type", y = "Total Count", color = color, alpha = 0.8, \
                  data = type_count)
ax1.set_xlabel("Type", fontsize = 16)
top_bar = mpatches.Patch(color = color, label = 'Num of Total Pokemon')

# bar graph for total non-type2 pokemon count
color = "lightblue"
ax2 = sns.barplot(x = "Type", y = "Single-type Count",  color = color, alpha = 0.8, \
                  data = type_count)
ax2.set_ylabel("Num of Pokemon", fontsize = 16)
low_bar = mpatches.Patch(color = color, label = 'Num of single-type Pokemon')

plt.legend(handles=[top_bar, low_bar])
plt.show()
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_136_0.png)
    


The table and graph above show the total number of Pokemon and total number of single-type Pokemon for each type. Looking at the number of total pokemon, the most common type is Water type, followed by Normal, Flying, Grass, and Psychic type. The types with the fewest Pokemon are fairy and ice types. It can be seen that Pokemon belonging to the water type are three times larger than the Pokemon belonging to the ice type. Also, when looking at the ratio of single-type Pokemon, about 20 to 50% of Pokemon belonging to each type were single-type PokÃ©mon. However, the ratio of Flying type is 1.98% and Steel type1 is 10.2%, which is very small compared to other types. In other words, most Flying and Steel type Pokemon are dual type Pokemon with another type. On the other hand, in electric, the ratio of single-type Pokemon was about 54% and normal was about 59.8%, which was significantly higher than other types. In other words, about half of electric and normal type Pokemon are single type Pokemon that do not have another type.


```python
df_pokemon_type_indifference[df_pokemon_type_indifference.Legendary == True] \
                       .groupby(["Type", "Legendary"]).count().Name.sort_values(ascending = False) \
                       .reset_index().rename(columns = {"Name" : "Count"})
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Type</th>
      <th>Legendary</th>
      <th>Count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Psychic</td>
      <td>True</td>
      <td>19</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Dragon</td>
      <td>True</td>
      <td>16</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Flying</td>
      <td>True</td>
      <td>15</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Fire</td>
      <td>True</td>
      <td>8</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Electric</td>
      <td>True</td>
      <td>5</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Ground</td>
      <td>True</td>
      <td>5</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Ice</td>
      <td>True</td>
      <td>5</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Steel</td>
      <td>True</td>
      <td>5</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Water</td>
      <td>True</td>
      <td>5</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Fighting</td>
      <td>True</td>
      <td>4</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Rock</td>
      <td>True</td>
      <td>4</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Dark</td>
      <td>True</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Fairy</td>
      <td>True</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Ghost</td>
      <td>True</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Grass</td>
      <td>True</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Normal</td>
      <td>True</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>




```python
plt.figure(figsize = (15, 7))
sns.barplot(x = "Type", y = "Count", color = "darkblue", alpha = 0.8, \
            data = df_pokemon_type_indifference[df_pokemon_type_indifference.Legendary == True] \
                       .groupby(["Type", "Legendary"]).count().Name.sort_values(ascending = False) \
                       .reset_index().rename(columns = {"Name" : "Count"}))
plt.xlabel("Type", fontsize = 16)
plt.ylabel("Number of Legendary Pokemon", fontsize = 16)
plt.show()
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_139_0.png)
    


The table and graph above show the number of legendary Pokemon for each type. While most types have fewer than 5 legendary Pokemon, Psychic, Dragon, and Flying Pokemon have more than 15 legendary Pokemon each.


```python
sns.pairplot(data = df_pokemon_type_indifference, vars = ["Total", "HP", "Attack", "Defense", "Sp. Atk", "Sp. Def", "Speed"], \
             plot_kws = {'alpha': 0.5}, corner = True, hue = "Legendary")
plt.show()
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_141_0.png)
    


It can be seen that legendary Pokemon have higher overall abilities compared to other normal Pokemon as above. Therefore, when comparing the abilities of each Pokemon type, I think that the more general characteristics of each type can be grasped by excluding the legendary Pokemon. Therefore, in the comparison of the abilities of each type, the analysis proceeds by excluding the legendary Pokemon.



```python
df_pokemon_type_indifference_non_legend = df_pokemon_type_indifference[df_pokemon_type_indifference.Legendary == False]
```

Now let's compare the abilities of each Pokemon type.


```python
plt.figure(figsize = (15, 7))

sns.boxplot(data = df_pokemon_type_indifference_non_legend, x = "Type", y = "Total")
plt.xlabel("Type", fontsize = 16)
plt.ylabel("Total", fontsize = 16)
plt.show()
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_145_0.png)
    


First, comparing the Total, the dragon type seems a bit higher than the other types.

Except for Total, Pokemon's stats include HP, Attack, Defense, Sp.Atc, Sp.Def, and Speed. Sp.Atc means special attack, Sp.Def means special defense. The higher the Speed, the first to attack. Therefore, among the six detailed stats, Hp, Defense, and Sp.Def are defense-related stats, and attack, Sp.atc, and Speed are attack-related stats. First, let's take a look at the difference between attack-related stats and defense-related stats for each type.


```python
df_pokemon_type_indifference_non_legend["Total_attack"] = df_pokemon_type_indifference_non_legend["Attack"] \
                                                          + df_pokemon_type_indifference_non_legend["Sp. Atk"] \
                                                          + df_pokemon_type_indifference_non_legend["Speed"]

df_pokemon_type_indifference_non_legend["Total_defense"] = df_pokemon_type_indifference_non_legend["Defense"] \
                                                           + df_pokemon_type_indifference_non_legend["Sp. Def"] \
                                                           + df_pokemon_type_indifference_non_legend["HP"]
```


```python
sns.set(style="darkgrid")

attack_by_type = df_pokemon_type_indifference_non_legend.groupby("Type").mean()["Total_attack"]
attack_by_type = pd.DataFrame((attack_by_type - np.mean(attack_by_type)) / np.std(attack_by_type)).reset_index()
attack_by_type["color"] = ['red' if x < 0 else 'green' for x in attack_by_type['Total_attack']]
attack_by_type.sort_values('Total_attack', inplace = True)

plt.figure(figsize = (12,12))
plt.hlines(y = attack_by_type.Type, xmin = 0, xmax = attack_by_type.Total_attack)
for x, y, tex in zip(attack_by_type.Total_attack, attack_by_type.Type, attack_by_type.Total_attack):
     t = plt.text(x, y, round(tex, 2),
                  horizontalalignment = 'right' if x < 0 else 'left',
                  verticalalignment = 'center',
                  fontdict = {'color':'red' if x < 0 else 'green', 'size' : 14})

plt.yticks(attack_by_type.Type, attack_by_type.Type, fontsize = 12)
plt.ylabel("Type", fontsize = 16)
plt.xlabel("Total attack ability comparison", fontsize = 16)
plt.xlim(-3, 3)
plt.show()
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_149_0.png)
    


The average of the Total_attack for each type was obtained, and this was standardized using the overall average and variance to compare which type had relatively high or low values. Dragon type showed higher total attack ability compared to other types, and Bug and Fairy showed low total attack ability.



```python
sns.set(style = "darkgrid")

defense_by_type = df_pokemon_type_indifference_non_legend.groupby("Type").mean()["Total_defense"]
defense_by_type = pd.DataFrame((defense_by_type - np.mean(defense_by_type)) / np.std(defense_by_type)).reset_index()
defense_by_type["color"] = ['red' if x < 0 else 'green' for x in defense_by_type['Total_defense']]
defense_by_type.sort_values('Total_defense', inplace = True)

plt.figure(figsize = (12,12))
plt.hlines(y = defense_by_type.Type, xmin = 0, xmax = defense_by_type.Total_defense)
for x, y, tex in zip(defense_by_type.Total_defense, defense_by_type.Type, defense_by_type.Total_defense):
     t = plt.text(x, y, round(tex, 2),
                  horizontalalignment = 'right' if x < 0 else 'left',
                  verticalalignment = 'center',
                  fontdict = {'color':'red' if x < 0 else 'green', 'size' : 14})

plt.yticks(defense_by_type.Type, defense_by_type.Type, fontsize = 12)
plt.ylabel("Type", fontsize = 16)
plt.xlabel("Total defense ability comparison", fontsize = 16)
plt.xlim(-3, 3)
plt.show()
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_151_0.png)
    


The average of the Total_defense for each type was obtained, and this was standardized using the overall average and variance to compare which type had relatively high or low values. Steel, Rock type showed higher total defense ability compared to other types, and Bug and Poison showed low total defense ability. The Dragon had the highest total attack stat, and the total defense stat was also higher than other types. Due to this, the total ability value that was first seen through the boxplot appears higher than other types.


Comparing the overall attack ability and the overall defensive ability together,        
    - Above-average attack & above-average defense : Dragon, Fighting, Ice          
    - Above-average attack & below-average defense : Dark, Fire, Electric, Flying          
    - Below Average Attack & Above Average Defense : Steel, Psychic, Ground, Rock, Fairy          
    - Below Average Attack & Below Average Defense : Ghost, Water, Poison, Grass, Normal, Bug          

## <span style="color:magenta"> Q6. For 15 Points: Explore how the PokÃ©mon in each generation differ from each other? </span>

* Do you think designers of *PokÃ©mon* tried to address different distributions of ability points in each generation?


```python
sns.set(style = "ticks")
plt.figure(figsize = (15, 7))

# bar plot for value count of each generation
color = "tab:blue"
ax1 = sns.barplot(x = "Generation", y = "Count", color = color, alpha = 0.5, \
                  data = df_pokemon.Generation.value_counts().reset_index().rename(columns = {"index" : "Generation", "Generation" : "Count"}))
ax1.set_xlabel("Generation", fontsize = 16)
ax1.set_ylabel("Number of Pokemon", color = color, fontsize = 16)

# line plot for number of Type1 of each generation
ax2 = ax1.twinx()
color = "tab:green"
ax2 = sns.lineplot(data = df_pokemon_type_indifference.groupby("Generation")["Type"].nunique().values, color = color, linewidth = 3, alpha = 0.5)
ax2.set_ylabel("Number of distinct type", color = color, fontsize = 16)
plt.yticks([18])

plt.show()
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_155_0.png)
    


n the graph above, the blue bar graph shows the number of PokÃ©mon per generation, and the green line graph shows the number of distinct types per generation. 1st generation Pokemon has the most, followed by 3rd and 5th generation Pokemon. Generation 6 Pokemon is the smallest with about 80. Also, it can be seen that 18 types of Pokemon exist in all of the 1st to 6th generations.

Next, let's look at the number of Pokemon of each type by each generation.


```python
generation_type = pd.DataFrame(columns = ["generation", "type"])
for i in range(1, 7):
    temp = pd.DataFrame()
    temp = pd.DataFrame(pd.concat([df_pokemon[df_pokemon["Generation"] == i]["Type 1"], df_pokemon[df_pokemon["Generation"] == i]["Type 2"]], 
                        ignore_index = True)).rename(columns = {0 : "type"})
    temp["generation"] = i
    generation_type = pd.concat([generation_type, temp])

generation_type["values"] = 0 # just dummy column for counting in pivot_table function
```


```python
generation_type.pivot_table(index = "generation", columns = "type", values = "values", aggfunc='count')
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>type</th>
      <th>Bug</th>
      <th>Dark</th>
      <th>Dragon</th>
      <th>Electric</th>
      <th>Fairy</th>
      <th>Fighting</th>
      <th>Fire</th>
      <th>Flying</th>
      <th>Ghost</th>
      <th>Grass</th>
      <th>Ground</th>
      <th>Ice</th>
      <th>Normal</th>
      <th>Poison</th>
      <th>Psychic</th>
      <th>Rock</th>
      <th>Steel</th>
      <th>Water</th>
    </tr>
    <tr>
      <th>generation</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>14</td>
      <td>1</td>
      <td>4</td>
      <td>9</td>
      <td>5</td>
      <td>9</td>
      <td>14</td>
      <td>23</td>
      <td>4</td>
      <td>15</td>
      <td>14</td>
      <td>5</td>
      <td>24</td>
      <td>36</td>
      <td>18</td>
      <td>12</td>
      <td>2</td>
      <td>35</td>
    </tr>
    <tr>
      <th>2</th>
      <td>12</td>
      <td>8</td>
      <td>2</td>
      <td>9</td>
      <td>8</td>
      <td>4</td>
      <td>11</td>
      <td>19</td>
      <td>1</td>
      <td>10</td>
      <td>11</td>
      <td>5</td>
      <td>15</td>
      <td>4</td>
      <td>10</td>
      <td>8</td>
      <td>6</td>
      <td>18</td>
    </tr>
    <tr>
      <th>3</th>
      <td>14</td>
      <td>13</td>
      <td>15</td>
      <td>5</td>
      <td>8</td>
      <td>9</td>
      <td>9</td>
      <td>14</td>
      <td>8</td>
      <td>18</td>
      <td>16</td>
      <td>7</td>
      <td>18</td>
      <td>5</td>
      <td>28</td>
      <td>12</td>
      <td>12</td>
      <td>31</td>
    </tr>
    <tr>
      <th>4</th>
      <td>11</td>
      <td>7</td>
      <td>8</td>
      <td>12</td>
      <td>2</td>
      <td>10</td>
      <td>6</td>
      <td>16</td>
      <td>9</td>
      <td>17</td>
      <td>12</td>
      <td>8</td>
      <td>18</td>
      <td>8</td>
      <td>10</td>
      <td>7</td>
      <td>12</td>
      <td>15</td>
    </tr>
    <tr>
      <th>5</th>
      <td>18</td>
      <td>16</td>
      <td>12</td>
      <td>12</td>
      <td>3</td>
      <td>17</td>
      <td>16</td>
      <td>21</td>
      <td>9</td>
      <td>20</td>
      <td>12</td>
      <td>9</td>
      <td>19</td>
      <td>7</td>
      <td>16</td>
      <td>10</td>
      <td>12</td>
      <td>18</td>
    </tr>
    <tr>
      <th>6</th>
      <td>3</td>
      <td>6</td>
      <td>9</td>
      <td>3</td>
      <td>14</td>
      <td>4</td>
      <td>8</td>
      <td>8</td>
      <td>15</td>
      <td>15</td>
      <td>2</td>
      <td>4</td>
      <td>8</td>
      <td>2</td>
      <td>8</td>
      <td>9</td>
      <td>5</td>
      <td>9</td>
    </tr>
  </tbody>
</table>
</div>




```python
plt.figure(figsize = (15, 7))
sns.heatmap(generation_type.pivot_table(index = "generation", columns = "type", values = "values", aggfunc='count'), \
            annot = True, linewidths = .5, cmap="Blues")
plt.xlabel("Type", fontsize = 16)
plt.ylabel("Generation", fontsize = 16)
plt.show()
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_160_0.png)
    


Looking at the number of PokÃ©mon belonging to each type by generation, it can be seen that the Flying, Grass, and Normal types are evenly distributed across generations 1 to 6. On the other hand, most of the Poison type Pokemon are distributed in the first generation.


```python
plt.figure(figsize = (15, 7))
sns.barplot(x = "Generation", y = "Count", color = "darkblue", alpha = 0.8, \
            data = df_pokemon[df_pokemon.Legendary == True] \
                   .groupby(["Generation", "Legendary"]).count().Name.sort_values(ascending = False) \
                   .reset_index().rename(columns = {"Name" : "Count"}))
plt.xlabel("Generation", fontsize = 16)
plt.ylabel("Number of Legendary Pokemon", fontsize = 16)
plt.show()
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_162_0.png)
    


The graph above shows the number of legendary Pokemon per generation. The number of legendary Pokemon was the lowest in Generations 1 and 2, then more than doubled in Generations 3, 4, and 5, and decreased in the 6th generation, conversely.

When comparing abilities by generation, for the same reason as when comparing abilities by type, the analysis proceeds by excluding legendary Pokemon.


```python
df_pokemon_non_legend = df_pokemon[df_pokemon.Legendary == False]
```


```python
df_pokemon_non_legend["Total_attack"] = df_pokemon_non_legend["Attack"] \
                                        + df_pokemon_non_legend["Sp. Atk"] \
                                        + df_pokemon_non_legend["Speed"]

df_pokemon_non_legend["Total_defense"] = df_pokemon_non_legend["Defense"] \
                                         + df_pokemon_non_legend["Sp. Def"] \
                                         + df_pokemon_non_legend["HP"]
```


```python
column_list = ["Total", "Total_attack", "Total_defense"]

fig, axes = plt.subplots(1, 3, figsize = (30,10))

for i, column in enumerate(column_list):
    sns.boxplot(ax = axes[i%3], x = "Generation", y = column, data = df_pokemon_non_legend)
    axes[i%3].set_xlabel("Generation", fontsize = 16)
    axes[i%3].set_ylabel(column, fontsize = 16)
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_167_0.png)
    


As with the analysis by Type, the Tota_attack column was created by combining Attack, Sp.Atk, and Speed, and the Total_defense column was created by combining Defense, Sp.Def, and HP. Afterwards, Total, Total_attack, and Total_defense were compared by Generation. Compared to other generations, the 4th Generation tends to have higher Total_attack and Total_defense abilities, so it seems that the 4th Generation has the highest Total. To check this more precisely, let's proceed with anova analysis.



```python
lm_total = smf.ols("Total ~ Generation", data = df_pokemon_non_legend)
lm_total_res = lm_total.fit()
print(lm_total_res.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                  Total   R-squared:                       0.000
    Model:                            OLS   Adj. R-squared:                 -0.001
    Method:                 Least Squares   F-statistic:                    0.1754
    Date:                Tue, 15 Feb 2022   Prob (F-statistic):              0.675
    Time:                        14:38:35   Log-Likelihood:                -4475.2
    No. Observations:                 735   AIC:                             8954.
    Df Residuals:                     733   BIC:                             8964.
    Df Model:                           1                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    Intercept    413.9727      8.684     47.673      0.000     396.925     431.020
    Generation     0.9868      2.356      0.419      0.675      -3.639       5.612
    ==============================================================================
    Omnibus:                       46.635   Durbin-Watson:                   2.123
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):               17.358
    Skew:                          -0.030   Prob(JB):                     0.000170
    Kurtosis:                       2.250   Cond. No.                         8.60
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.


When anova analysis is performed, the F-statistic probability is very high at 0.67. This means that the null hypothesis that there is no significant difference between Generations cannot be rejected, so it can be seen that there is no statistical difference in Total between generations.


```python
tukeyhsd_total = pairwise_tukeyhsd(df_pokemon_non_legend["Total"], df_pokemon_non_legend["Generation"])
tukeyhsd_total.summary()
```




<table class="simpletable">
<caption>Multiple Comparison of Means - Tukey HSD, FWER=0.05</caption>
<tr>
  <th>group1</th> <th>group2</th> <th>meandiff</th>  <th>p-adj</th>   <th>lower</th>   <th>upper</th>  <th>reject</th>
</tr>
<tr>
     <td>1</td>      <td>2</td>    <td>-9.6467</td>   <td>0.9</td>  <td>-48.3976</td> <td>29.1041</td>  <td>False</td>
</tr>
<tr>
     <td>1</td>      <td>3</td>    <td>-8.6761</td>   <td>0.9</td>  <td>-43.8306</td> <td>26.4784</td>  <td>False</td>
</tr>
<tr>
     <td>1</td>      <td>4</td>    <td>19.9359</td> <td>0.6427</td> <td>-18.0373</td> <td>57.9091</td>  <td>False</td>
</tr>
<tr>
     <td>1</td>      <td>5</td>    <td>-1.3238</td>   <td>0.9</td>   <td>-35.978</td> <td>33.3305</td>  <td>False</td>
</tr>
<tr>
     <td>1</td>      <td>6</td>    <td>-3.8492</td>   <td>0.9</td>  <td>-46.7152</td> <td>39.0169</td>  <td>False</td>
</tr>
<tr>
     <td>2</td>      <td>3</td>    <td>0.9706</td>    <td>0.9</td>  <td>-38.7193</td> <td>40.6605</td>  <td>False</td>
</tr>
<tr>
     <td>2</td>      <td>4</td>    <td>29.5826</td> <td>0.3419</td> <td>-12.6242</td> <td>71.7894</td>  <td>False</td>
</tr>
<tr>
     <td>2</td>      <td>5</td>     <td>8.323</td>    <td>0.9</td>  <td>-30.9245</td> <td>47.5705</td>  <td>False</td>
</tr>
<tr>
     <td>2</td>      <td>6</td>    <td>5.7976</td>    <td>0.9</td>  <td>-40.8602</td> <td>52.4554</td>  <td>False</td>
</tr>
<tr>
     <td>3</td>      <td>4</td>    <td>28.612</td>  <td>0.2885</td>  <td>-10.319</td> <td>67.543</td>   <td>False</td>
</tr>
<tr>
     <td>3</td>      <td>5</td>    <td>7.3524</td>    <td>0.9</td>  <td>-28.3488</td> <td>43.0536</td>  <td>False</td>
</tr>
<tr>
     <td>3</td>      <td>6</td>     <td>4.827</td>    <td>0.9</td>  <td>-38.8898</td> <td>48.5438</td>  <td>False</td>
</tr>
<tr>
     <td>4</td>      <td>5</td>   <td>-21.2596</td> <td>0.5975</td> <td>-59.7395</td> <td>17.2203</td>  <td>False</td>
</tr>
<tr>
     <td>4</td>      <td>6</td>    <td>-23.785</td> <td>0.6561</td>  <td>-69.799</td> <td>22.2289</td>  <td>False</td>
</tr>
<tr>
     <td>5</td>      <td>6</td>    <td>-2.5254</td>   <td>0.9</td>   <td>-45.841</td> <td>40.7902</td>  <td>False</td>
</tr>
</table>



The above shows the turkeyhsd table for Total between generations. When the reject column is true, it can be interpreted that there is a statistically significant difference between the two groups. However, as can be seen from the table above, there is no pair with a significant difference in Total between generations.


```python
lm_total_attack = smf.ols("Total_attack ~ Generation", data = df_pokemon_non_legend)
lm_total_attack_res = lm_total_attack.fit()
print(lm_total_attack_res.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:           Total_attack   R-squared:                       0.000
    Model:                            OLS   Adj. R-squared:                 -0.001
    Method:                 Least Squares   F-statistic:                   0.03249
    Date:                Tue, 15 Feb 2022   Prob (F-statistic):              0.857
    Time:                        14:42:14   Log-Likelihood:                -4105.9
    No. Observations:                 735   AIC:                             8216.
    Df Residuals:                     733   BIC:                             8225.
    Df Model:                           1                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    Intercept    210.4234      5.254     40.053      0.000     200.109     220.737
    Generation    -0.2569      1.425     -0.180      0.857      -3.055       2.542
    ==============================================================================
    Omnibus:                        7.219   Durbin-Watson:                   1.953
    Prob(Omnibus):                  0.027   Jarque-Bera (JB):                7.081
    Skew:                           0.210   Prob(JB):                       0.0290
    Kurtosis:                       2.767   Cond. No.                         8.60
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.



```python
tukeyhsd_total_attack = pairwise_tukeyhsd(df_pokemon_non_legend["Total_attack"], df_pokemon_non_legend["Generation"])
tukeyhsd_total_attack.summary()
```




<table class="simpletable">
<caption>Multiple Comparison of Means - Tukey HSD, FWER=0.05</caption>
<tr>
  <th>group1</th> <th>group2</th> <th>meandiff</th>  <th>p-adj</th>   <th>lower</th>   <th>upper</th>  <th>reject</th>
</tr>
<tr>
     <td>1</td>      <td>2</td>   <td>-20.3667</td> <td>0.1282</td> <td>-43.7456</td> <td>3.0121</td>   <td>False</td>
</tr>
<tr>
     <td>1</td>      <td>3</td>    <td>-6.365</td>    <td>0.9</td>  <td>-27.5741</td> <td>14.8442</td>  <td>False</td>
</tr>
<tr>
     <td>1</td>      <td>4</td>    <td>3.7588</td>    <td>0.9</td>  <td>-19.1509</td> <td>26.6685</td>  <td>False</td>
</tr>
<tr>
     <td>1</td>      <td>5</td>    <td>-4.9142</td>   <td>0.9</td>  <td>-25.8215</td> <td>15.9932</td>  <td>False</td>
</tr>
<tr>
     <td>1</td>      <td>6</td>   <td>-12.2064</td>  <td>0.73</td>   <td>-38.068</td> <td>13.6552</td>  <td>False</td>
</tr>
<tr>
     <td>2</td>      <td>3</td>    <td>14.0017</td> <td>0.5444</td>  <td>-9.9436</td> <td>37.9471</td>  <td>False</td>
</tr>
<tr>
     <td>2</td>      <td>4</td>    <td>24.1255</td> <td>0.0752</td>  <td>-1.3383</td> <td>49.5894</td>  <td>False</td>
</tr>
<tr>
     <td>2</td>      <td>5</td>    <td>15.4525</td>  <td>0.427</td>  <td>-8.226</td>  <td>39.131</td>   <td>False</td>
</tr>
<tr>
     <td>2</td>      <td>6</td>    <td>8.1603</td>    <td>0.9</td>  <td>-19.9889</td> <td>36.3095</td>  <td>False</td>
</tr>
<tr>
     <td>3</td>      <td>4</td>    <td>10.1238</td> <td>0.7975</td> <td>-13.3638</td> <td>33.6113</td>  <td>False</td>
</tr>
<tr>
     <td>3</td>      <td>5</td>    <td>1.4508</td>    <td>0.9</td>  <td>-20.0882</td> <td>22.9898</td>  <td>False</td>
</tr>
<tr>
     <td>3</td>      <td>6</td>    <td>-5.8415</td>   <td>0.9</td>  <td>-32.2163</td> <td>20.5334</td>  <td>False</td>
</tr>
<tr>
     <td>4</td>      <td>5</td>    <td>-8.673</td>  <td>0.8921</td> <td>-31.8883</td> <td>14.5424</td>  <td>False</td>
</tr>
<tr>
     <td>4</td>      <td>6</td>   <td>-15.9652</td> <td>0.5602</td>  <td>-43.726</td> <td>11.7956</td>  <td>False</td>
</tr>
<tr>
     <td>5</td>      <td>6</td>    <td>-7.2923</td>   <td>0.9</td>  <td>-33.4251</td> <td>18.8406</td>  <td>False</td>
</tr>
</table>



The same goes for Total_attack ability. There is no statistically significant difference in Total_attack between generations.


```python
lm_total_defense = smf.ols("Total_defense ~ Generation", data = df_pokemon_non_legend)
lm_total_defense_res = lm_total_defense.fit()
print(lm_total_defense_res.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:          Total_defense   R-squared:                       0.001
    Model:                            OLS   Adj. R-squared:                 -0.000
    Method:                 Least Squares   F-statistic:                    0.8683
    Date:                Tue, 15 Feb 2022   Prob (F-statistic):              0.352
    Time:                        14:43:28   Log-Likelihood:                -4057.5
    No. Observations:                 735   AIC:                             8119.
    Df Residuals:                     733   BIC:                             8128.
    Df Model:                           1                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    Intercept    203.5493      4.919     41.378      0.000     193.892     213.207
    Generation     1.2437      1.335      0.932      0.352      -1.377       3.864
    ==============================================================================
    Omnibus:                       15.901   Durbin-Watson:                   1.958
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):               16.396
    Skew:                           0.363   Prob(JB):                     0.000275
    Kurtosis:                       3.095   Cond. No.                         8.60
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.



```python
tukeyhsd_total_defense = pairwise_tukeyhsd(df_pokemon_non_legend["Total_defense"], df_pokemon_non_legend["Generation"])
tukeyhsd_total_defense.summary()
```




<table class="simpletable">
<caption>Multiple Comparison of Means - Tukey HSD, FWER=0.05</caption>
<tr>
  <th>group1</th> <th>group2</th> <th>meandiff</th>  <th>p-adj</th>   <th>lower</th>   <th>upper</th>  <th>reject</th>
</tr>
<tr>
     <td>1</td>      <td>2</td>     <td>10.72</td>  <td>0.7022</td> <td>-11.2058</td> <td>32.6458</td>  <td>False</td>
</tr>
<tr>
     <td>1</td>      <td>3</td>    <td>-2.3112</td>   <td>0.9</td>  <td>-22.2021</td> <td>17.5798</td>  <td>False</td>
</tr>
<tr>
     <td>1</td>      <td>4</td>    <td>16.1771</td> <td>0.2622</td>  <td>-5.3087</td> <td>37.6629</td>  <td>False</td>
</tr>
<tr>
     <td>1</td>      <td>5</td>    <td>3.5904</td>    <td>0.9</td>  <td>-16.0175</td> <td>23.1983</td>  <td>False</td>
</tr>
<tr>
     <td>1</td>      <td>6</td>    <td>8.3573</td>    <td>0.9</td>   <td>-15.897</td> <td>32.6115</td>  <td>False</td>
</tr>
<tr>
     <td>2</td>      <td>3</td>   <td>-13.0312</td> <td>0.5517</td> <td>-35.4883</td>  <td>9.426</td>   <td>False</td>
</tr>
<tr>
     <td>2</td>      <td>4</td>    <td>5.4571</td>    <td>0.9</td>  <td>-18.4242</td> <td>29.3383</td>  <td>False</td>
</tr>
<tr>
     <td>2</td>      <td>5</td>    <td>-7.1296</td>   <td>0.9</td>  <td>-29.3364</td> <td>15.0773</td>  <td>False</td>
</tr>
<tr>
     <td>2</td>      <td>6</td>    <td>-2.3627</td>   <td>0.9</td>  <td>-28.7624</td> <td>24.037</td>   <td>False</td>
</tr>
<tr>
     <td>3</td>      <td>4</td>    <td>18.4883</td> <td>0.1582</td>  <td>-3.5395</td> <td>40.516</td>   <td>False</td>
</tr>
<tr>
     <td>3</td>      <td>5</td>    <td>5.9016</td>    <td>0.9</td>  <td>-14.2987</td> <td>26.1019</td>  <td>False</td>
</tr>
<tr>
     <td>3</td>      <td>6</td>    <td>10.6684</td>  <td>0.797</td> <td>-14.0672</td> <td>35.4041</td>  <td>False</td>
</tr>
<tr>
     <td>4</td>      <td>5</td>   <td>-12.5867</td> <td>0.5553</td> <td>-34.3592</td> <td>9.1859</td>   <td>False</td>
</tr>
<tr>
     <td>4</td>      <td>6</td>    <td>-7.8198</td>   <td>0.9</td>  <td>-33.8552</td> <td>18.2156</td>  <td>False</td>
</tr>
<tr>
     <td>5</td>      <td>6</td>    <td>4.7668</td>    <td>0.9</td>  <td>-19.7418</td> <td>29.2755</td>  <td>False</td>
</tr>
</table>



The same goes for Total_defense. There is no statistically significant difference in Total_defense between generations.

When comparing the Total, Total_attack, and Total_defense by generation, there was no statistically significant difference by generation. Next, let's compare the detailed 6 abilities.


```python
column_list = ["HP", "Attack", "Defense", "Sp. Atk", "Sp. Def", "Speed"]

fig, axes = plt.subplots(2, 3, figsize = (20,10))

for i, column in enumerate(column_list):
    sns.boxplot(ax = axes[i//3 ,i%3], x = "Generation", y = column, data = df_pokemon_non_legend)
```


    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_180_0.png)
    


When comparing other detailed abilities, there was little difference by generation. Therefore, it can be seen that there is no significant difference in the abilities of Pokemon by generation.

Next, let's check if there is a distributional difference even if there is no numerical difference in ability values.


```python
plt.figure(figsize = (20, 20))

g = sns.pairplot(data = df_pokemon_non_legend, vars = ["Total", "Total_attack", "Total_defense"], \
                 plot_kws = {'alpha': 0.5}, hue = "Generation", palette = 'Dark2')
g.map_lower(sns.kdeplot, levels=4, color=".2")
plt.show()
```


    <Figure size 1440x1440 with 0 Axes>



    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_183_1.png)
    


First, let's compare the distribution of Total, Total_attack, and Total_defense ability by generation. There are 6 generations, so it is difficult to compare, but if you look at the histogram in diagnor, you can see that there is almost no difference in the histogram for each generation.


```python
plt.figure(figsize = (20, 20))

g = sns.pairplot(data = df_pokemon_non_legend, vars = ["Attack", "Sp. Atk", "Speed", "Defense", "Sp. Def", "HP"], \
                 plot_kws = {'alpha': 0.5}, hue = "Generation", palette = 'Dark2')
g.map_lower(sns.kdeplot, levels=4, color=".2")
plt.show()
```


    <Figure size 1440x1440 with 0 Axes>



    
![png](/assets/images/coursework/SI618/hw4/hw4_upload_185_1.png)
    


Next, even if we compare the numerical values â€‹â€‹of more detailed ability values, it is difficult to confirm the difference in the distribution by generation.

In other words, although the number of legend Pokemon and the distribution of Pokemon types are different for each generation, there is no difference in the distribution of detailed stats or numerical differences.
