---
layout: single
title: "3. Train machine learning models to predict sepsis outcome (In progress)"
permalink: /projects/sepsis_prediction/3.modeling/
comments: false
author_profile: true
read_time: true
toc: true
toc_label: "Table of Contents"
toc_sticky: true
categories: ["sepsis_prediction"]
---

```python
import pandas as pd 
import numpy as np 

import matplotlib.pyplot as plt 
import matplotlib.patches as mpatches
import seaborn as sns 
import missingno as msno 
from utils import my_histogram, make_stacked_table, my_stacked_barplot

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import cross_validate
from sklearn.model_selection import train_test_split

from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis 
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn import tree
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
import xgboost as xgb
import lightgbm as lgb

import optuna
from optuna.integration import LightGBMPruningCallback
from optuna.integration import XGBoostPruningCallback
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold

from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import log_loss
from sklearn.metrics import classification_report


import imblearn
from imblearn.over_sampling import SMOTE

```


```python
import warnings
warnings.filterwarnings('ignore')
```


```python
pd.set_option('display.max_columns', None)
```

# 1. Prepare the data

- Load all tables with extracted features.


```python
train_outcome = pd.read_csv("../data/train_outcome.csv")
test_outcome = pd.read_csv("../data/test_outcome.csv")

df_train_demo = pd.read_csv("../data/df_train_demo.csv")
df_test_demo = pd.read_csv("../data/df_test_demo.csv")

df_train_measurment_time_info = pd.read_csv("../data/df_train_measurment_time_info.csv")
df_test_measurment_time_info = pd.read_csv("../data/df_test_measurment_time_info.csv")

df_train_measurement_grouped_stat = pd.read_csv("../data/df_train_measurement_grouped_stat.csv")
df_test_measurement_grouped_stat = pd.read_csv("../data/df_test_measurment_grouped_stat.csv")
```

- Merge all tables.


```python
df_train_all = df_train_demo.merge(df_train_measurment_time_info, how = "left", on = "ID") \
                            .merge(df_train_measurement_grouped_stat, how = "left", on = "ID") \
                            .merge(train_outcome, how = "left", on = "ID")    
```


```python
df_test_all = df_test_demo.merge(df_test_measurment_time_info, how = "left", on = "ID") \
                          .merge(df_test_measurement_grouped_stat, how = "left", on = "ID") \
                          .merge(test_outcome, how = "left", on = "ID")    
```


```python
X_train = df_train_all.drop(["Outcome", "ID"], axis = 1)
y_train = df_train_all.Outcome

X_test = df_test_all.drop(["Outcome", "ID"], axis = 1)
y_test = df_test_all.Outcome
```

- Standardize the train and test data.


```python
col_names = X_train.columns
```


```python
scaler = StandardScaler()
scaler.fit(X_train)
```




<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div></div></div>




```python
X_train_std = pd.DataFrame(scaler.transform(X_train), columns = col_names)
X_test_std = pd.DataFrame(scaler.transform(X_test), columns = col_names)
```

# 2. Baseline error


```python
y_test.value_counts()
```




    0    5623
    1     867
    Name: Outcome, dtype: int64




```python
sns.countplot(x = y_test)
```




    <AxesSubplot:xlabel='Outcome', ylabel='count'>




    
![png](/assets/images/projects/sepsis_prediction/3.modeling_16_1.png)
    


- Note that only 13% of patients have an Outcome = 1.
- That is, if we just predict all patient as Outcome = 0, we can achieve the 13% error, that is our baseline error.

- Let's choose models to tune the hyperparameters by comparing each model's error with the baseline error.


```python
names = ["KNN", 
         "LDA", 
         "QDA", 
         "Naive Bayes",
         "Logistic regression",
         "Decesion tree", 
         "Random Forest",  
         "AdaBoost",
         "XGBoost",
         "Polynomial kernel SVM",
         "Radial kernel SVM",
         "GBM",
         "LightGBM"
        ]

classifiers = [
    KNeighborsClassifier(3),
    LinearDiscriminantAnalysis(),
    QuadraticDiscriminantAnalysis(),
    GaussianNB(), 
    LogisticRegression(),
    DecisionTreeClassifier(random_state = 42),
    RandomForestClassifier(),
    AdaBoostClassifier(),
    xgb.XGBClassifier(),
    SVC(kernel = "poly", probability = True),
    SVC(kernel = "rbf", probability = True),
    GradientBoostingClassifier(),
    lgb.LGBMClassifier()
    ]
```


```python
result_accuracy = pd.DataFrame(names, columns = ["model_name"])
```


```python
y_pred_dict = {}

for name, clf in zip(names, classifiers):
    clf.fit(X_train_std, y_train)
    
    y_pred = clf.predict(X_test_std)
    y_pred_proba = clf.predict_proba(X_test_std)
    y_pred_dict[name] = y_pred
    
    error = np.mean(y_pred != y_test)
    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred_proba[:, 1])
    
    result_accuracy.loc[result_accuracy.model_name == name, "Error"] = error
    result_accuracy.loc[result_accuracy.model_name == name, "BER"] = 1 - balanced_accuracy
    result_accuracy.loc[result_accuracy.model_name == name, "AUC"] = auc
    
```


```python
result_accuracy
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model_name</th>
      <th>Error</th>
      <th>BER</th>
      <th>AUC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>KNN</td>
      <td>0.092296</td>
      <td>0.301056</td>
      <td>0.764946</td>
    </tr>
    <tr>
      <th>1</th>
      <td>LDA</td>
      <td>0.110169</td>
      <td>0.362100</td>
      <td>0.813528</td>
    </tr>
    <tr>
      <th>2</th>
      <td>QDA</td>
      <td>0.155470</td>
      <td>0.301905</td>
      <td>0.799748</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Naive Bayes</td>
      <td>0.117565</td>
      <td>0.295640</td>
      <td>0.841536</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logistic regression</td>
      <td>0.106317</td>
      <td>0.354024</td>
      <td>0.815425</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Decesion tree</td>
      <td>0.115408</td>
      <td>0.236349</td>
      <td>0.763651</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Random Forest</td>
      <td>0.066256</td>
      <td>0.216764</td>
      <td>0.911303</td>
    </tr>
    <tr>
      <th>7</th>
      <td>AdaBoost</td>
      <td>0.072111</td>
      <td>0.223069</td>
      <td>0.888091</td>
    </tr>
    <tr>
      <th>8</th>
      <td>XGBoost</td>
      <td>0.065485</td>
      <td>0.203637</td>
      <td>0.911156</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Polynomial kernel SVM</td>
      <td>0.089522</td>
      <td>0.300431</td>
      <td>0.825197</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Radial kernel SVM</td>
      <td>0.077042</td>
      <td>0.258108</td>
      <td>0.900935</td>
    </tr>
    <tr>
      <th>11</th>
      <td>GBM</td>
      <td>0.065948</td>
      <td>0.211708</td>
      <td>0.908800</td>
    </tr>
    <tr>
      <th>12</th>
      <td>LightGBM</td>
      <td>0.063328</td>
      <td>0.200441</td>
      <td>0.916949</td>
    </tr>
  </tbody>
</table>
</div>



- Almost all models have smaller error than the baseline error 13%.
- Among the models, the error and the balanced error of LightGBM is the lowest at 6.3% and 20%.
- Also, Randomforest and XGBoost have small BER and high AUC values.
- So let's tune the hyperparameters of LightGBM, XGBoost, and RandomForest.

- Before tune the hyperparameters, let's check the confusion matrix.


```python
lgbm_cm = confusion_matrix(y_test, y_pred_dict["LightGBM"])
lgbm_cm_display = ConfusionMatrixDisplay(confusion_matrix = lgbm_cm, display_labels = clf.classes_)

xgboost_cm = confusion_matrix(y_test, y_pred_dict["XGBoost"])
xgboost_cm_display = ConfusionMatrixDisplay(confusion_matrix = xgboost_cm, display_labels = clf.classes_)

rf_cm = confusion_matrix(y_test, y_pred_dict["Random Forest"])
rf_cm_display = ConfusionMatrixDisplay(confusion_matrix = rf_cm, display_labels = clf.classes_)

fig, axes = plt.subplots(1, 3, figsize = (20, 5))

lgbm_cm_display.plot(ax = axes[0])
axes[0].set_title("LightGBM", fontsize = 15)

xgboost_cm_display.plot(ax = axes[1])
axes[1].set_title("XGBoost", fontsize = 15)

rf_cm_display.plot(ax = axes[2])
axes[2].set_title("Random Forest", fontsize = 15)


```




    Text(0.5, 1.0, 'Random Forest')




    
![png](/assets/images/projects/sepsis_prediction/3.modeling_25_1.png)
    


- Models perform poorly at predicting Outcome = 1.

# 3. Hyperparameters tunning

- Now, let's tune the hyperparameter.
- There are two evaluation metrics. One is BER, and the other is AUC.
- First, tune the hyperparemeter of LightGBM, XGBoost, and RandomForest to maximize the AUC. 
- Among trained LightGBM, XGBoost, and RandomForest models with different hyperparameters, select top 15 models.
- From the best 15 models, get average of the scores.
- By using the mean score from 15 best models, calcualte the AUC.
- Then, let's find the best cutoff value for the mean score to achieve the minimum BER.   


## 3.1. LightGBM

- Candidate hyperparameters are as follow:
    - learning_rate: 0.01 ~ 0.3 
    - num_leaves: 20 ~ 3000 with step = 20
    - max_depth: 3 ~ 12 
    - min_data_in_leaf: 200 ~ 10000 with step = 100
    - max_bing: 200 ~ 300
    - lambda_l1: 0 ~ 100 with step = 5
    - lambda_l2: 0 ~ 100 with step = 5
    - min_gain_to_split: 0 ~ 15
    - bagging_fraction: 0.2 ~ 0.95 with step = 0.1
    - feature_fraction: 0.2 ~ 0.95 with step = 0.1

- There are two evaluation metrics. One is BER, and the other is AUC.
- First, tune the hyperparemeter to maximize the AUC. 
- Then, let's find the best cutoff value for score to achieve the minimum BER.  


```python
def lgbm_objective(trial, X, y):
    param_grid = {
        "n_estimators": trial.suggest_categorical("n_estimators", [10000]),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "num_leaves": trial.suggest_int("num_leaves", 20, 3000, step = 20),
        "max_depth": trial.suggest_int("max_depth", 3, 12),
        "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 200, 10000, step = 100),
        "max_bin": trial.suggest_int("max_bin", 200, 300),
        "lambda_l1": trial.suggest_int("lambda_l1", 0, 100, step = 5),
        "lambda_l2": trial.suggest_int("lambda_l2", 0, 100, step = 5),
        "min_gain_to_split": trial.suggest_float("min_gain_to_split", 0, 15),
        "bagging_fraction": trial.suggest_float(
            "bagging_fraction", 0.2, 0.95, step = 0.1
        ),
        "bagging_freq": trial.suggest_categorical("bagging_freq", [1]),
        "feature_fraction": trial.suggest_float(
            "feature_fraction", 0.2, 0.95, step = 0.1
        ),
        "silent": 1,
        "verbose": -1
    }
    
    cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)
    cv_scores = np.empty(5)
    
    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        model = lgb.LGBMClassifier(objective = "binary", **param_grid, n_jobs = -1)
        model.fit(
            X_train,
            y_train,
            eval_set=[(X_test, y_test)],
            eval_metric = ["auc"],
            early_stopping_rounds = 100,
            callbacks=[
                LightGBMPruningCallback(trial, "auc")
            ],  # Add a pruning callback
            verbose = -1
        )
        preds_proba = model.predict_proba(X_test)
        auc = roc_auc_score(y_test, preds_proba[:, 1])
        cv_scores[idx] = auc

    return np.mean(cv_scores)
```


```python
lgbm_study = optuna.create_study(direction = "maximize", study_name = "LGBM Classifier")
func = lambda trial: lgbm_objective(trial, X_train_std, y_train)
lgbm_study.optimize(func, n_trials = 100)
```


```python
lgbm_trials_history = lgbm_study.get_trials()

trial_list = []
hyperparameters_list = []
auc_list = []

for i in range(len(lgbm_trials_history)):
    trial_list.append(lgbm_trials_history[i].number) 
    hyperparameters_list.append(lgbm_trials_history[i].params)
    auc_list.append(lgbm_trials_history[i].values[0])
    
lgbm_auc_history = pd.DataFrame({"trial": trial_list, "auc": auc_list, "params": hyperparameters_list})
```


```python
lgbm_auc_history.sort_values("auc", ascending = False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>trial</th>
      <th>auc</th>
      <th>params</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>85</th>
      <td>85</td>
      <td>0.943124</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.145...</td>
    </tr>
    <tr>
      <th>55</th>
      <td>55</td>
      <td>0.940711</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.120...</td>
    </tr>
    <tr>
      <th>67</th>
      <td>67</td>
      <td>0.940047</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.132...</td>
    </tr>
    <tr>
      <th>80</th>
      <td>80</td>
      <td>0.939630</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.172...</td>
    </tr>
    <tr>
      <th>39</th>
      <td>39</td>
      <td>0.939020</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.136...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>10</th>
      <td>10</td>
      <td>0.500000</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.197...</td>
    </tr>
    <tr>
      <th>8</th>
      <td>8</td>
      <td>0.500000</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.259...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>0.500000</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.253...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>0.500000</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.054...</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0.500000</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.265...</td>
    </tr>
  </tbody>
</table>
<p>100 rows × 3 columns</p>
</div>



## 3.2. XGBoost

- Candidate hyperparameters are as follow:
    - n_estimators: 100, 300, 500, 1000
    - learning_rate: 1e-8 ~ 1 
    - max_depth: 3 ~ 20 
    - lambda: 1e-8 ~ 1 
    - alpha: 1e-8 ~ 1 
    - gamma: 1e-8 ~ 1 
    - subsample: 0.01 ~ 0.3 with step = 0.02
    - colsample_bytree: 0.4 ~ 1.0 with step = 0.1
    - colsample_bylevel: 0.4 ~ 1.0 with step = 0.1


```python
def xgboost_objective(trial, X, y):
    param_grid = {
        "n_estimators": trial.suggest_categorical("n_estimators", [100, 300, 500, 1000]),
        "eta": trial.suggest_loguniform("alpha", 1e-8, 1.0),
        "max_depth": trial.suggest_int("max_depth", 3, 20),
        "lambda": trial.suggest_loguniform("lambda", 1e-8, 1.0),
        "alpha": trial.suggest_loguniform("alpha", 1e-8, 1.0),
        "gamma": trial.suggest_loguniform("alpha", 1e-8, 1.0),
        "subsample": trial.suggest_float("subsample", 0.01, 0.3, step = 0.02),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.4, 1.0, step = 0.1),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.4, 1.0, step = 0.1),
        "verbosity": 0
    }

    cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)
    cv_scores = np.empty(5)

    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        model = xgb.XGBClassifier(objective = "binary:logistic", **param_grid, n_jobs = -1)
        model.fit(
            X_train,
            y_train,
            eval_set=[(X_test, y_test)],
            eval_metric = ["auc"],
            early_stopping_rounds = 100,
            callbacks=[
                XGBoostPruningCallback(trial, "validation_0-auc")
            ],  # Add a pruning callback
        )
        
        preds_proba = model.predict_proba(X_test)
        auc = roc_auc_score(y_test, preds_proba[:, 1])
        cv_scores[idx] = auc

    return np.mean(cv_scores)
```


```python
xgboost_study = optuna.create_study(direction = "maximize", study_name = "XGBoost Classifier")
func = lambda trial: xgboost_objective(trial, X_train_std, y_train)
xgboost_study.optimize(func, n_trials = 100)
```


```python
xgboost_trials_history = xgboost_study.get_trials()

trial_list = []
hyperparameters_list = []
auc_list = []

for i in range(len(xgboost_trials_history)):
    trial_list.append(xgboost_trials_history[i].number) 
    hyperparameters_list.append(xgboost_trials_history[i].params)
    auc_list.append(xgboost_trials_history[i].values[0])
    
xgboost_auc_history = pd.DataFrame({"trial": trial_list, "auc": auc_list, "params": hyperparameters_list})
```


```python
xgboost_auc_history.sort_values("auc", ascending = False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>trial</th>
      <th>auc</th>
      <th>params</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>25</th>
      <td>25</td>
      <td>0.925149</td>
      <td>{'n_estimators': 500, 'alpha': 9.4863506092290...</td>
    </tr>
    <tr>
      <th>41</th>
      <td>41</td>
      <td>0.924905</td>
      <td>{'n_estimators': 500, 'alpha': 3.4232784401779...</td>
    </tr>
    <tr>
      <th>34</th>
      <td>34</td>
      <td>0.919624</td>
      <td>{'n_estimators': 500, 'alpha': 2.2767849840122...</td>
    </tr>
    <tr>
      <th>29</th>
      <td>29</td>
      <td>0.917429</td>
      <td>{'n_estimators': 300, 'alpha': 2.0219184532052...</td>
    </tr>
    <tr>
      <th>88</th>
      <td>88</td>
      <td>0.916726</td>
      <td>{'n_estimators': 300, 'alpha': 2.7510681730152...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>80</th>
      <td>80</td>
      <td>0.500000</td>
      <td>{'n_estimators': 500, 'alpha': 2.4663181451532...</td>
    </tr>
    <tr>
      <th>21</th>
      <td>21</td>
      <td>0.500000</td>
      <td>{'n_estimators': 500, 'alpha': 1.0123144391688...</td>
    </tr>
    <tr>
      <th>9</th>
      <td>9</td>
      <td>0.500000</td>
      <td>{'n_estimators': 100, 'alpha': 4.2479074365499...</td>
    </tr>
    <tr>
      <th>57</th>
      <td>57</td>
      <td>0.500000</td>
      <td>{'n_estimators': 100, 'alpha': 3.2214812056006...</td>
    </tr>
    <tr>
      <th>22</th>
      <td>22</td>
      <td>0.500000</td>
      <td>{'n_estimators': 500, 'alpha': 1.0470925931211...</td>
    </tr>
  </tbody>
</table>
<p>100 rows × 3 columns</p>
</div>



## 3.3. RandomForest

- Candidate hyperparameters are as follow:
    - n_estimators: 100, 300, 500, 1000
    - learning_rate: 1e-8 ~ 1 
    - max_depth: 3 ~ 20 
    - max_features: auto, sqrt, log2
    - min_samples_leaf: 1 ~ 10
    - min_samples_split: 2 ~ 10


```python
def rf_objective(trial, X, y):
    param_grid = {
        "n_estimators": trial.suggest_categorical("n_estimators", [100, 300, 500, 1000]),
        "max_depth": trial.suggest_int("max_depth", 3, 20, step = 2),
        "max_features": trial.suggest_categorical("max_features", ["sqrt", "log2"]),
        "min_samples_leaf": trial.suggest_int("min_samples_leaf", 1, 10),
        "min_samples_split": trial.suggest_int("min_samples_split", 2, 10),
    }
    
    cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)
    
    model = RandomForestClassifier(**param_grid, n_jobs = -1)
    cv_results = cross_validate(model, X, y, cv = cv, scoring = "roc_auc", n_jobs = -1)


    return np.mean(cv_results['test_score'])
```


```python
rf_study = optuna.create_study(direction = "maximize", study_name = "RandomForest Classifier")
func = lambda trial: rf_objective(trial, X_train_std, y_train)
rf_study.optimize(func, n_trials = 20)
```


```python
rf_trials_history = rf_study.get_trials()

trial_list = []
hyperparameters_list = []
auc_list = []

for i in range(len(rf_trials_history)):
    trial_list.append(rf_trials_history[i].number) 
    hyperparameters_list.append(rf_trials_history[i].params)
    auc_list.append(rf_trials_history[i].values[0])
    
rf_auc_history = pd.DataFrame({"trial": trial_list, "auc": auc_list, "params": hyperparameters_list})
```


```python
rf_auc_history.sort_values("auc", ascending = False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>trial</th>
      <th>auc</th>
      <th>params</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>13</th>
      <td>13</td>
      <td>0.921056</td>
      <td>{'n_estimators': 500, 'max_depth': 15, 'max_fe...</td>
    </tr>
    <tr>
      <th>15</th>
      <td>15</td>
      <td>0.920583</td>
      <td>{'n_estimators': 300, 'max_depth': 15, 'max_fe...</td>
    </tr>
    <tr>
      <th>19</th>
      <td>19</td>
      <td>0.920497</td>
      <td>{'n_estimators': 300, 'max_depth': 15, 'max_fe...</td>
    </tr>
    <tr>
      <th>14</th>
      <td>14</td>
      <td>0.920441</td>
      <td>{'n_estimators': 500, 'max_depth': 15, 'max_fe...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>0.920377</td>
      <td>{'n_estimators': 500, 'max_depth': 17, 'max_fe...</td>
    </tr>
    <tr>
      <th>18</th>
      <td>18</td>
      <td>0.920144</td>
      <td>{'n_estimators': 300, 'max_depth': 15, 'max_fe...</td>
    </tr>
    <tr>
      <th>11</th>
      <td>11</td>
      <td>0.920087</td>
      <td>{'n_estimators': 500, 'max_depth': 13, 'max_fe...</td>
    </tr>
    <tr>
      <th>12</th>
      <td>12</td>
      <td>0.919690</td>
      <td>{'n_estimators': 500, 'max_depth': 13, 'max_fe...</td>
    </tr>
    <tr>
      <th>10</th>
      <td>10</td>
      <td>0.919352</td>
      <td>{'n_estimators': 500, 'max_depth': 13, 'max_fe...</td>
    </tr>
    <tr>
      <th>9</th>
      <td>9</td>
      <td>0.918802</td>
      <td>{'n_estimators': 100, 'max_depth': 19, 'max_fe...</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5</td>
      <td>0.917771</td>
      <td>{'n_estimators': 100, 'max_depth': 19, 'max_fe...</td>
    </tr>
    <tr>
      <th>16</th>
      <td>16</td>
      <td>0.917703</td>
      <td>{'n_estimators': 300, 'max_depth': 11, 'max_fe...</td>
    </tr>
    <tr>
      <th>17</th>
      <td>17</td>
      <td>0.917431</td>
      <td>{'n_estimators': 300, 'max_depth': 11, 'max_fe...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>0.916428</td>
      <td>{'n_estimators': 100, 'max_depth': 19, 'max_fe...</td>
    </tr>
    <tr>
      <th>7</th>
      <td>7</td>
      <td>0.905486</td>
      <td>{'n_estimators': 100, 'max_depth': 7, 'max_fea...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>0.905325</td>
      <td>{'n_estimators': 1000, 'max_depth': 7, 'max_fe...</td>
    </tr>
    <tr>
      <th>6</th>
      <td>6</td>
      <td>0.895565</td>
      <td>{'n_estimators': 300, 'max_depth': 5, 'max_fea...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0.893271</td>
      <td>{'n_estimators': 1000, 'max_depth': 5, 'max_fe...</td>
    </tr>
    <tr>
      <th>8</th>
      <td>8</td>
      <td>0.892012</td>
      <td>{'n_estimators': 1000, 'max_depth': 5, 'max_fe...</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0.871615</td>
      <td>{'n_estimators': 500, 'max_depth': 3, 'max_fea...</td>
    </tr>
  </tbody>
</table>
</div>



## 3.4. Choose the best 15 models

- Among all trained models, let's choose the best 15 models based on the AUC value.


```python
lgbm_auc_history["model"] = "lgbm"
xgboost_auc_history["model"] = "xgboost"
rf_auc_history["model"] = "rf"

all_auc_history = pd.concat([lgbm_auc_history, xgboost_auc_history, rf_auc_history])
```


```python
best_15_models = all_auc_history.sort_values("auc", ascending = False).head(15)
best_15_models
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>trial</th>
      <th>auc</th>
      <th>params</th>
      <th>model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>85</th>
      <td>85</td>
      <td>0.943124</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.145...</td>
      <td>lgbm</td>
    </tr>
    <tr>
      <th>55</th>
      <td>55</td>
      <td>0.940711</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.120...</td>
      <td>lgbm</td>
    </tr>
    <tr>
      <th>67</th>
      <td>67</td>
      <td>0.940047</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.132...</td>
      <td>lgbm</td>
    </tr>
    <tr>
      <th>80</th>
      <td>80</td>
      <td>0.939630</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.172...</td>
      <td>lgbm</td>
    </tr>
    <tr>
      <th>39</th>
      <td>39</td>
      <td>0.939020</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.136...</td>
      <td>lgbm</td>
    </tr>
    <tr>
      <th>60</th>
      <td>60</td>
      <td>0.938714</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.127...</td>
      <td>lgbm</td>
    </tr>
    <tr>
      <th>87</th>
      <td>87</td>
      <td>0.930233</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.110...</td>
      <td>lgbm</td>
    </tr>
    <tr>
      <th>31</th>
      <td>31</td>
      <td>0.929903</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.120...</td>
      <td>lgbm</td>
    </tr>
    <tr>
      <th>30</th>
      <td>30</td>
      <td>0.928990</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.129...</td>
      <td>lgbm</td>
    </tr>
    <tr>
      <th>61</th>
      <td>61</td>
      <td>0.928796</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.114...</td>
      <td>lgbm</td>
    </tr>
    <tr>
      <th>95</th>
      <td>95</td>
      <td>0.928361</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.127...</td>
      <td>lgbm</td>
    </tr>
    <tr>
      <th>38</th>
      <td>38</td>
      <td>0.927850</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.124...</td>
      <td>lgbm</td>
    </tr>
    <tr>
      <th>28</th>
      <td>28</td>
      <td>0.927820</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.139...</td>
      <td>lgbm</td>
    </tr>
    <tr>
      <th>96</th>
      <td>96</td>
      <td>0.927811</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.127...</td>
      <td>lgbm</td>
    </tr>
    <tr>
      <th>33</th>
      <td>33</td>
      <td>0.927721</td>
      <td>{'n_estimators': 10000, 'learning_rate': 0.107...</td>
      <td>lgbm</td>
    </tr>
  </tbody>
</table>
</div>



# 4. Select cut-off score

- Let's find the best cut-off score for classifying binary outcome class.
- Split the train set into train and validation set and train the model by using the train set with the parameters found in the hyperparamter tuning step. Then, we can define a set of thresholds and then evaluate predicted probabilities under each in order to find and select the optimal threshold by using the validation set.

- First, split the train set into the train and validation set.
- Since our data is imbalanced, use the stratified sampling with the ratio = 0.2.


```python
X_y_train = pd.concat([X_train_std, pd.DataFrame({"Outcome": y_train})], axis = 1)
```


```python
X_y_train_train = X_y_train.groupby("Outcome", group_keys = False).apply(lambda x: x.sample(frac = 0.8))
X_y_train_train.Outcome.value_counts()
```




    0    10463
    1     1652
    Name: Outcome, dtype: int64




```python
X_y_train_valid = X_y_train[~X_y_train.index.isin(X_y_train_train.index)]
X_y_train_valid.Outcome.value_counts()
```




    0    2616
    1     413
    Name: Outcome, dtype: int64




```python
X_train_train = X_y_train_train.drop("Outcome", axis = 1)
y_train_train = X_y_train_train.Outcome

X_train_valid = X_y_train_valid.drop("Outcome", axis = 1)
y_train_valid = X_y_train_valid.Outcome
```

- We splited the train set into train and validation set
    - train set: 12,115
        - Outcome = 0: 10,463
        - Outcome = 1: 1,652
    - validation set: 3,029
        - Outcome = 0: 2,619
        - Outcome = 1: 413
        

- Now calculate the scores from the best 15 models and get the average scores, which will be used to calcualte the AUC and BER.


```python
best_15_models_proba = []

for i in range(best_15_models.shape[0]):
    params = best_15_models.iloc[i].params 
    model_name = best_15_models.iloc[i].model 
    
    if model_name == "lgbm": model = lgb.LGBMClassifier(**params)
    elif model_name == "xgboost": model = xgb.XGBClassifier(**params)
    elif model_name == "rf": model = RandomForestClassifier(**params)
    
    model.fit(X_train_train, y_train_train)
    y_pred_proba = model.predict_proba(X_train_valid)
    best_15_models_proba.append(y_pred_proba)
    
```


```python
best_15_avg_prob = best_15_models_proba[0]

for i in range(1, 15):
    best_15_avg_prob = best_15_avg_prob + best_15_models_proba[i]

best_15_avg_prob = best_15_avg_prob/15
```


```python
best_15_avg_prob
```




    array([[0.98045849, 0.01954151],
           [0.97563099, 0.02436901],
           [0.95791448, 0.04208552],
           ...,
           [0.04403311, 0.95596689],
           [0.769854  , 0.230146  ],
           [0.98536498, 0.01463502]])




```python
thresholds = np.arange(0.0, 1.0, 0.0001)
ber_list = np.zeros(shape = (len(thresholds)))

```


```python
for index, elem in enumerate(thresholds):
    # Corrected probabilities
    y_pred = (best_15_avg_prob[:, 1] > elem).astype("int")
    
    # Calculate the BER
    ber_list[index] = 1 - balanced_accuracy_score(y_train_valid, y_pred)
```


```python
# Find the optimal threshold
index = np.argmin(ber_list)
threshold_Opt = round(thresholds[index], ndigits = 4)
ber_Opt = round(ber_list[index], ndigits = 4)
print('Best Threshold: {} with BER: {}'.format(threshold_Opt, ber_Opt))
```

    Best Threshold: 0.1867 with BER: 0.126



```python
plt.grid()
sns.lineplot(x = thresholds, y = ber_list)
plt.xlabel("Threshold", fontsize = 12)
plt.ylabel("Balanced Error Rate", fontsize = 12)
plt.title("Threshold Tuning Curve", fontsize = 15)

plt.plot(threshold_Opt, ber_Opt, "ro")
plt.text(x = threshold_Opt - 0.08, y = ber_Opt + 0.05, s = f'Optimal threshold \n for Outcome = 1: {threshold_Opt}')
```




    Text(0.1067, 0.176, 'Optimal threshold \n for Outcome = 1: 0.1867')




    
![png](/assets/images/projects/sepsis_prediction/3.modeling_67_1.png)
    


- When we set 0.1867 as the cut-off score for the Outcome = 1, then we can achieve the best BER 0.126 on the validation set.

# 5. Calculate the BER and AUC

- Now let's calculate the BER and AUC on the test data.


```python
best_15_models_proba_test = []

for i in range(best_15_models.shape[0]):
    params = best_15_models.iloc[i].params 
    model_name = best_15_models.iloc[i].model 
    
    if model_name == "lgbm": model = lgb.LGBMClassifier(**params)
    elif model_name == "xgboost": model = xgb.XGBClassifier(**params)
    elif model_name == "rf": model = RandomForestClassifier(**params)
    
    model.fit(X_train_std, y_train)
    y_pred_proba = model.predict_proba(X_test_std)
    best_15_models_proba_test.append(y_pred_proba)
    
```


```python
best_15_avg_prob_test = best_15_models_proba_test[0]

for i in range(1, 15):
    best_15_avg_prob_test = best_15_avg_prob_test + best_15_models_proba_test[i]

best_15_avg_prob_test = best_15_avg_prob_test / 15
```


```python
best_15_avg_prob_test
```




    array([[0.98290209, 0.01709791],
           [0.89942117, 0.10057883],
           [0.97529847, 0.02470153],
           ...,
           [0.98090421, 0.01909579],
           [0.98463654, 0.01536346],
           [0.96578141, 0.03421859]])




```python
y_pred = (best_15_avg_prob_test[:, 1] > threshold_Opt).astype("int")

test_ber = 1 - balanced_accuracy_score(y_test, y_pred)
test_auc = roc_auc_score(y_test, best_15_avg_prob_test[:, 1])
```


```python
print("Best AUC before tuning the parameters: ", result_accuracy.AUC.max())
print("AUC after tuning the parameters: ", test_auc)
```

    Best AUC before tuning the parameters:  0.9169488636328671
    AUC after tuning the parameters:  0.9158752536593299



```python
print("Best BER before tuning the parameters: ", result_accuracy.BER.min())
print("BER after tuning the parameters: ", test_ber)
```

    Best BER before tuning the parameters:  0.200440664177713
    BER after tuning the parameters:  0.16305466857266282


- AUC is almost the same after tuning, but BER significantly decreased after tuning.


```python
cm = confusion_matrix(y_test, y_pred)
cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = clf.classes_)
cm_display.plot();    
```


    
![png](/assets/images/projects/sepsis_prediction/3.modeling_78_0.png)
    



```python
print(classification_report(y_test, y_pred))
```

                  precision    recall  f1-score   support
    
               0       0.96      0.94      0.95      5623
               1       0.64      0.74      0.69       867
    
        accuracy                           0.91      6490
       macro avg       0.80      0.84      0.82      6490
    weighted avg       0.92      0.91      0.91      6490
    

