---
layout: single
title: "3 - 1) Train machine learning models to predict match results (In progress)"
permalink: /projects/european_soccer_prediction/3.1.modeling/
comments: false
author_profile: true
read_time: true
toc: true
toc_label: "Table of Contents"
toc_sticky: true
categories: ["european_soccer_prediction"]
---

```python
import pandas as pd 
import numpy as np 

import matplotlib.pyplot as plt 
import matplotlib.patches as mpatches
import seaborn as sns 
import missingno as msno 

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import cross_validate
from sklearn.model_selection import train_test_split
from sklearn import preprocessing

from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis 
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn import tree
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
import xgboost as xgb
import lightgbm as lgb

import optuna
from optuna.integration import LightGBMPruningCallback
from optuna.integration import XGBoostPruningCallback
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold

from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import log_loss
from sklearn.metrics import classification_report

from sklearn.inspection import permutation_importance

import imblearn
from imblearn.over_sampling import SMOTE

```


```python
import warnings
warnings.filterwarnings('ignore')
```


```python
pd.set_option('display.max_columns', None)
```

- We have 5 kinds of variable sets 
    - variable set 1: player attributes PC features
    - Variable set 2: betting information features
    - variable set 3: team attributes features
    - variable set 4: goal and win percentage rolling features
    - variable set 5: each team's Elo rating


```python
df_match_basic = pd.read_csv("../data/df_match_basic.csv")
```


```python
df_match_player_attr_pcs = pd.read_csv("../data/df_match_player_attr_pcs.csv")
```


```python
df_match_betting_stat = pd.read_csv("../data/df_match_betting_stat.csv")
```


```python
df_match_team_num_attr = pd.read_csv("../data/df_match_team_num_attr.csv")
```


```python
df_team_win_goal_rolling_features = pd.read_csv("../data/df_team_win_goal_rolling_features.csv")
```


```python
df_match_elo = pd.read_csv("../data/df_match_elo.csv")
```

- First, let's predict the match result and compare the result by using each variable sets.

# 1. Train test split

- Set last season as test set, other seasons as train set.


```python
target_bool = (df_match_basic.match_api_id.isin(df_match_player_attr_pcs.match_api_id)) & \
              (df_match_basic.match_api_id.isin(df_match_betting_stat.match_api_id)) & \
              (df_match_basic.match_api_id.isin(df_match_team_num_attr.match_api_id)) & \
              (df_match_basic.match_api_id.isin(df_team_win_goal_rolling_features.match_api_id)) & \
              (df_match_basic.match_api_id.isin(df_match_elo.match_api_id))
    
```


```python
target_matches = df_match_basic[target_bool]
```


```python
test_match_api_id = target_matches[target_matches.season == "2015/2016"].match_api_id
train_match_api_id = target_matches[target_matches.season != "2015/2016"].match_api_id
```


```python
print(len(train_match_api_id), len(test_match_api_id))
```

    16988 2621


- There are 16,988 train set and 2,621 test set.

# 2. Baseline accuracy


```python
df_match_basic[df_match_basic.match_api_id.isin(train_match_api_id)].match_result.value_counts()
```




    home_win    7840
    away_win    4855
    draw        4293
    Name: match_result, dtype: int64




```python
sns.countplot(x = df_match_basic[df_match_basic.match_api_id.isin(train_match_api_id)].match_result)
```




    <AxesSubplot:xlabel='match_result', ylabel='count'>




    
![png](/assets/images/projects/european_soccer_prediction/3.1.modeling_20_1.png)
    


- About 46% of all 16,988 matches were won by the home team.
- That is, if we predict all matches as home team win, then we can achieve about 46% accuracy, that can be used as our baseline accuracy.

- Let's check the baseline accuracy in the test data set.


```python
df_match_basic[df_match_basic.match_api_id.isin(test_match_api_id)].match_result.value_counts()
```




    home_win    1161
    away_win     801
    draw         659
    Name: match_result, dtype: int64




```python
sns.countplot(x = df_match_basic[df_match_basic.match_api_id.isin(test_match_api_id)].match_result)
```




    <AxesSubplot:xlabel='match_result', ylabel='count'>




    
![png](/assets/images/projects/european_soccer_prediction/3.1.modeling_24_1.png)
    


- Baseline accuracy in the test dataset is about 44% (1,161 / 2,621)

# 3. Modeling with all variable sets

## 3.1. Variable set 1: Player attributes PC features


```python
df_match_player_attr_pcs = df_match_player_attr_pcs.merge(df_match_basic[["match_api_id", "match_result"]], how = "left", on = "match_api_id")
```


```python
df_match_player_attr_pcs = df_match_player_attr_pcs.set_index("match_api_id")
df_match_player_attr_pcs

```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>home_player_1_pc_1</th>
      <th>home_player_1_pc_2</th>
      <th>home_player_1_pc_3</th>
      <th>home_player_1_pc_4</th>
      <th>home_player_1_pc_5</th>
      <th>home_player_2_pc_1</th>
      <th>home_player_2_pc_2</th>
      <th>home_player_2_pc_3</th>
      <th>home_player_2_pc_4</th>
      <th>home_player_2_pc_5</th>
      <th>home_player_3_pc_1</th>
      <th>home_player_3_pc_2</th>
      <th>home_player_3_pc_3</th>
      <th>home_player_3_pc_4</th>
      <th>home_player_3_pc_5</th>
      <th>home_player_4_pc_1</th>
      <th>home_player_4_pc_2</th>
      <th>home_player_4_pc_3</th>
      <th>home_player_4_pc_4</th>
      <th>home_player_4_pc_5</th>
      <th>home_player_5_pc_1</th>
      <th>home_player_5_pc_2</th>
      <th>home_player_5_pc_3</th>
      <th>home_player_5_pc_4</th>
      <th>home_player_5_pc_5</th>
      <th>home_player_6_pc_1</th>
      <th>home_player_6_pc_2</th>
      <th>home_player_6_pc_3</th>
      <th>home_player_6_pc_4</th>
      <th>home_player_6_pc_5</th>
      <th>home_player_7_pc_1</th>
      <th>home_player_7_pc_2</th>
      <th>home_player_7_pc_3</th>
      <th>home_player_7_pc_4</th>
      <th>home_player_7_pc_5</th>
      <th>home_player_8_pc_1</th>
      <th>home_player_8_pc_2</th>
      <th>home_player_8_pc_3</th>
      <th>home_player_8_pc_4</th>
      <th>home_player_8_pc_5</th>
      <th>home_player_9_pc_1</th>
      <th>home_player_9_pc_2</th>
      <th>home_player_9_pc_3</th>
      <th>home_player_9_pc_4</th>
      <th>home_player_9_pc_5</th>
      <th>home_player_10_pc_1</th>
      <th>home_player_10_pc_2</th>
      <th>home_player_10_pc_3</th>
      <th>home_player_10_pc_4</th>
      <th>home_player_10_pc_5</th>
      <th>home_player_11_pc_1</th>
      <th>home_player_11_pc_2</th>
      <th>home_player_11_pc_3</th>
      <th>home_player_11_pc_4</th>
      <th>home_player_11_pc_5</th>
      <th>away_player_1_pc_1</th>
      <th>away_player_1_pc_2</th>
      <th>away_player_1_pc_3</th>
      <th>away_player_1_pc_4</th>
      <th>away_player_1_pc_5</th>
      <th>away_player_2_pc_1</th>
      <th>away_player_2_pc_2</th>
      <th>away_player_2_pc_3</th>
      <th>away_player_2_pc_4</th>
      <th>away_player_2_pc_5</th>
      <th>away_player_3_pc_1</th>
      <th>away_player_3_pc_2</th>
      <th>away_player_3_pc_3</th>
      <th>away_player_3_pc_4</th>
      <th>away_player_3_pc_5</th>
      <th>away_player_4_pc_1</th>
      <th>away_player_4_pc_2</th>
      <th>away_player_4_pc_3</th>
      <th>away_player_4_pc_4</th>
      <th>away_player_4_pc_5</th>
      <th>away_player_5_pc_1</th>
      <th>away_player_5_pc_2</th>
      <th>away_player_5_pc_3</th>
      <th>away_player_5_pc_4</th>
      <th>away_player_5_pc_5</th>
      <th>away_player_6_pc_1</th>
      <th>away_player_6_pc_2</th>
      <th>away_player_6_pc_3</th>
      <th>away_player_6_pc_4</th>
      <th>away_player_6_pc_5</th>
      <th>away_player_7_pc_1</th>
      <th>away_player_7_pc_2</th>
      <th>away_player_7_pc_3</th>
      <th>away_player_7_pc_4</th>
      <th>away_player_7_pc_5</th>
      <th>away_player_8_pc_1</th>
      <th>away_player_8_pc_2</th>
      <th>away_player_8_pc_3</th>
      <th>away_player_8_pc_4</th>
      <th>away_player_8_pc_5</th>
      <th>away_player_9_pc_1</th>
      <th>away_player_9_pc_2</th>
      <th>away_player_9_pc_3</th>
      <th>away_player_9_pc_4</th>
      <th>away_player_9_pc_5</th>
      <th>away_player_10_pc_1</th>
      <th>away_player_10_pc_2</th>
      <th>away_player_10_pc_3</th>
      <th>away_player_10_pc_4</th>
      <th>away_player_10_pc_5</th>
      <th>away_player_11_pc_1</th>
      <th>away_player_11_pc_2</th>
      <th>away_player_11_pc_3</th>
      <th>away_player_11_pc_4</th>
      <th>away_player_11_pc_5</th>
      <th>match_result</th>
    </tr>
    <tr>
      <th>match_api_id</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>493017</th>
      <td>9.172915</td>
      <td>-0.705596</td>
      <td>1.028500</td>
      <td>-0.044401</td>
      <td>1.246272</td>
      <td>3.957784</td>
      <td>1.650964</td>
      <td>-2.348632</td>
      <td>-0.837480</td>
      <td>0.223750</td>
      <td>-0.817702</td>
      <td>-0.589548</td>
      <td>0.195433</td>
      <td>-2.144582</td>
      <td>1.524434</td>
      <td>3.108730</td>
      <td>0.633708</td>
      <td>-1.772338</td>
      <td>0.807727</td>
      <td>1.684577</td>
      <td>0.615229</td>
      <td>-0.611994</td>
      <td>-0.845425</td>
      <td>0.395221</td>
      <td>3.107710</td>
      <td>-0.038702</td>
      <td>-0.551509</td>
      <td>-0.280096</td>
      <td>-0.068082</td>
      <td>2.645878</td>
      <td>1.086047</td>
      <td>-2.583583</td>
      <td>-0.596348</td>
      <td>-1.915882</td>
      <td>-0.987251</td>
      <td>-0.845848</td>
      <td>-0.053597</td>
      <td>0.746034</td>
      <td>-0.515174</td>
      <td>1.763346</td>
      <td>4.244623</td>
      <td>1.088030</td>
      <td>-2.054898</td>
      <td>-0.414998</td>
      <td>0.270225</td>
      <td>-0.559441</td>
      <td>1.233335</td>
      <td>0.426201</td>
      <td>0.609910</td>
      <td>1.364324</td>
      <td>1.472274</td>
      <td>-0.298277</td>
      <td>-1.530989</td>
      <td>0.743621</td>
      <td>-0.875547</td>
      <td>9.794795</td>
      <td>-0.549117</td>
      <td>1.941560</td>
      <td>0.281992</td>
      <td>0.521328</td>
      <td>-1.886782</td>
      <td>1.005850</td>
      <td>1.291888</td>
      <td>0.172012</td>
      <td>1.212622</td>
      <td>1.320201</td>
      <td>1.065244</td>
      <td>-0.406996</td>
      <td>-1.803778</td>
      <td>0.573483</td>
      <td>2.628391</td>
      <td>0.940615</td>
      <td>-0.609534</td>
      <td>-1.586848</td>
      <td>0.652795</td>
      <td>3.207876</td>
      <td>0.685128</td>
      <td>-1.480756</td>
      <td>-1.499414</td>
      <td>0.134771</td>
      <td>-1.360311</td>
      <td>-2.979972</td>
      <td>1.181522</td>
      <td>-0.726996</td>
      <td>1.161218</td>
      <td>-1.735000</td>
      <td>-0.323721</td>
      <td>0.977427</td>
      <td>-1.047350</td>
      <td>1.307837</td>
      <td>-0.004187</td>
      <td>1.646099</td>
      <td>0.543917</td>
      <td>0.101722</td>
      <td>0.374692</td>
      <td>-1.836650</td>
      <td>-2.551881</td>
      <td>1.212316</td>
      <td>0.139711</td>
      <td>1.155972</td>
      <td>-0.916797</td>
      <td>-1.202214</td>
      <td>0.022668</td>
      <td>-0.118531</td>
      <td>-0.332637</td>
      <td>0.628660</td>
      <td>-1.751083</td>
      <td>-0.030693</td>
      <td>0.418133</td>
      <td>-0.206630</td>
      <td>home_win</td>
    </tr>
    <tr>
      <th>493025</th>
      <td>6.467731</td>
      <td>-2.125163</td>
      <td>3.092089</td>
      <td>-0.930974</td>
      <td>1.172527</td>
      <td>0.390653</td>
      <td>1.341612</td>
      <td>0.109198</td>
      <td>-0.418330</td>
      <td>0.921304</td>
      <td>2.673401</td>
      <td>1.688787</td>
      <td>-0.346764</td>
      <td>0.032250</td>
      <td>-0.148238</td>
      <td>0.544724</td>
      <td>0.856618</td>
      <td>-0.234054</td>
      <td>-0.185530</td>
      <td>1.496783</td>
      <td>0.349558</td>
      <td>-0.322471</td>
      <td>0.311433</td>
      <td>-0.720916</td>
      <td>0.765089</td>
      <td>-2.997770</td>
      <td>-1.279240</td>
      <td>1.492689</td>
      <td>-0.174247</td>
      <td>1.728035</td>
      <td>-1.262416</td>
      <td>0.290702</td>
      <td>1.097804</td>
      <td>0.377514</td>
      <td>1.681666</td>
      <td>-1.035827</td>
      <td>-0.034337</td>
      <td>1.133088</td>
      <td>0.899750</td>
      <td>1.451945</td>
      <td>-1.970162</td>
      <td>-1.958911</td>
      <td>1.072887</td>
      <td>0.806278</td>
      <td>2.735079</td>
      <td>-1.206227</td>
      <td>-2.567254</td>
      <td>0.401355</td>
      <td>-0.240084</td>
      <td>1.552955</td>
      <td>-0.417405</td>
      <td>-0.845298</td>
      <td>0.077456</td>
      <td>-1.026087</td>
      <td>-0.249720</td>
      <td>6.746068</td>
      <td>-1.225452</td>
      <td>5.441467</td>
      <td>-1.741143</td>
      <td>0.153029</td>
      <td>-0.720625</td>
      <td>1.362837</td>
      <td>0.950063</td>
      <td>-0.430052</td>
      <td>1.920381</td>
      <td>1.321520</td>
      <td>2.234319</td>
      <td>0.546943</td>
      <td>0.392576</td>
      <td>1.081937</td>
      <td>-1.157135</td>
      <td>1.181325</td>
      <td>0.901247</td>
      <td>0.722505</td>
      <td>0.967595</td>
      <td>-1.573349</td>
      <td>0.366277</td>
      <td>1.085719</td>
      <td>-0.362088</td>
      <td>0.794429</td>
      <td>-2.362155</td>
      <td>0.960996</td>
      <td>2.341796</td>
      <td>0.480765</td>
      <td>1.235871</td>
      <td>-2.036662</td>
      <td>1.423269</td>
      <td>1.975662</td>
      <td>0.456836</td>
      <td>0.564672</td>
      <td>-1.002934</td>
      <td>-2.335553</td>
      <td>0.470491</td>
      <td>-0.111478</td>
      <td>0.585989</td>
      <td>-1.407944</td>
      <td>-2.205414</td>
      <td>1.069296</td>
      <td>0.053596</td>
      <td>0.809813</td>
      <td>-2.259696</td>
      <td>-2.778135</td>
      <td>1.816538</td>
      <td>1.407815</td>
      <td>0.022775</td>
      <td>0.169010</td>
      <td>-2.319129</td>
      <td>0.361658</td>
      <td>0.935315</td>
      <td>-1.877390</td>
      <td>away_win</td>
    </tr>
    <tr>
      <th>493027</th>
      <td>7.587977</td>
      <td>-0.669761</td>
      <td>3.351410</td>
      <td>-1.725204</td>
      <td>0.971832</td>
      <td>-0.659142</td>
      <td>2.447278</td>
      <td>1.853202</td>
      <td>-0.962117</td>
      <td>0.277773</td>
      <td>0.782575</td>
      <td>2.059493</td>
      <td>1.311728</td>
      <td>-0.983126</td>
      <td>0.652947</td>
      <td>0.750721</td>
      <td>3.078327</td>
      <td>1.228950</td>
      <td>0.540478</td>
      <td>0.034199</td>
      <td>0.361849</td>
      <td>2.339354</td>
      <td>1.689313</td>
      <td>0.713260</td>
      <td>0.455514</td>
      <td>-3.759299</td>
      <td>1.204148</td>
      <td>3.191002</td>
      <td>-0.784951</td>
      <td>0.913294</td>
      <td>-3.018176</td>
      <td>0.545416</td>
      <td>2.535436</td>
      <td>-0.218828</td>
      <td>2.658318</td>
      <td>-2.589758</td>
      <td>0.570906</td>
      <td>1.654008</td>
      <td>0.345163</td>
      <td>0.580780</td>
      <td>-3.859427</td>
      <td>-1.870653</td>
      <td>2.659663</td>
      <td>-0.725774</td>
      <td>2.171793</td>
      <td>-0.199689</td>
      <td>-2.415793</td>
      <td>-0.317491</td>
      <td>0.067180</td>
      <td>0.943378</td>
      <td>-1.056848</td>
      <td>-1.205950</td>
      <td>1.067483</td>
      <td>1.246078</td>
      <td>-0.070047</td>
      <td>8.942737</td>
      <td>-2.146976</td>
      <td>1.394474</td>
      <td>-1.573403</td>
      <td>-0.305385</td>
      <td>0.326027</td>
      <td>1.666843</td>
      <td>0.694338</td>
      <td>0.105792</td>
      <td>1.385551</td>
      <td>1.716745</td>
      <td>1.133649</td>
      <td>-0.670094</td>
      <td>-1.374712</td>
      <td>0.468374</td>
      <td>1.822806</td>
      <td>1.126330</td>
      <td>-0.789073</td>
      <td>-1.478142</td>
      <td>0.878564</td>
      <td>5.021780</td>
      <td>0.645976</td>
      <td>-1.954515</td>
      <td>-1.642732</td>
      <td>0.230055</td>
      <td>0.882744</td>
      <td>1.703690</td>
      <td>0.053693</td>
      <td>0.563981</td>
      <td>0.978446</td>
      <td>-0.780143</td>
      <td>-0.700812</td>
      <td>0.612057</td>
      <td>1.416142</td>
      <td>2.059364</td>
      <td>-0.691788</td>
      <td>-2.016431</td>
      <td>0.356132</td>
      <td>-1.164456</td>
      <td>2.251789</td>
      <td>1.464444</td>
      <td>0.379332</td>
      <td>-0.902796</td>
      <td>-0.837762</td>
      <td>0.211743</td>
      <td>1.923084</td>
      <td>-2.893726</td>
      <td>-2.042800</td>
      <td>-1.018973</td>
      <td>-1.292926</td>
      <td>0.416905</td>
      <td>-2.210474</td>
      <td>-0.336256</td>
      <td>0.054563</td>
      <td>2.588626</td>
      <td>home_win</td>
    </tr>
    <tr>
      <th>493034</th>
      <td>9.172915</td>
      <td>-0.705596</td>
      <td>1.028500</td>
      <td>-0.044401</td>
      <td>1.246272</td>
      <td>3.957784</td>
      <td>1.650964</td>
      <td>-2.348632</td>
      <td>-0.837480</td>
      <td>0.223750</td>
      <td>-0.817702</td>
      <td>-0.589548</td>
      <td>0.195433</td>
      <td>-2.144582</td>
      <td>1.524434</td>
      <td>-0.559441</td>
      <td>1.233335</td>
      <td>0.426201</td>
      <td>0.609910</td>
      <td>1.364324</td>
      <td>-0.845848</td>
      <td>-0.053597</td>
      <td>0.746034</td>
      <td>-0.515174</td>
      <td>1.763346</td>
      <td>0.615229</td>
      <td>-0.611994</td>
      <td>-0.845425</td>
      <td>0.395221</td>
      <td>3.107710</td>
      <td>3.108730</td>
      <td>0.633708</td>
      <td>-1.772338</td>
      <td>0.807727</td>
      <td>1.684577</td>
      <td>1.086047</td>
      <td>-2.583583</td>
      <td>-0.596348</td>
      <td>-1.915882</td>
      <td>-0.987251</td>
      <td>4.244623</td>
      <td>1.088030</td>
      <td>-2.054898</td>
      <td>-0.414998</td>
      <td>0.270225</td>
      <td>-0.105750</td>
      <td>0.211866</td>
      <td>0.619067</td>
      <td>0.766076</td>
      <td>-0.979514</td>
      <td>1.472274</td>
      <td>-0.298277</td>
      <td>-1.530989</td>
      <td>0.743621</td>
      <td>-0.875547</td>
      <td>7.587977</td>
      <td>-0.669761</td>
      <td>3.351410</td>
      <td>-1.725204</td>
      <td>0.971832</td>
      <td>-0.659142</td>
      <td>2.447278</td>
      <td>1.853202</td>
      <td>-0.962117</td>
      <td>0.277773</td>
      <td>0.361849</td>
      <td>2.339354</td>
      <td>1.689313</td>
      <td>0.713260</td>
      <td>0.455514</td>
      <td>1.052194</td>
      <td>2.080899</td>
      <td>0.370792</td>
      <td>-0.782729</td>
      <td>0.394531</td>
      <td>2.077104</td>
      <td>1.903504</td>
      <td>0.684877</td>
      <td>1.471026</td>
      <td>-0.492695</td>
      <td>-3.759299</td>
      <td>1.204148</td>
      <td>3.191002</td>
      <td>-0.784951</td>
      <td>0.913294</td>
      <td>-0.631336</td>
      <td>-1.917288</td>
      <td>0.404582</td>
      <td>-1.604514</td>
      <td>-0.334234</td>
      <td>-3.859427</td>
      <td>-1.870653</td>
      <td>2.659663</td>
      <td>-0.725774</td>
      <td>2.171793</td>
      <td>-3.018176</td>
      <td>0.545416</td>
      <td>2.535436</td>
      <td>-0.218828</td>
      <td>2.658318</td>
      <td>-2.589758</td>
      <td>0.570906</td>
      <td>1.654008</td>
      <td>0.345163</td>
      <td>0.580780</td>
      <td>-1.056848</td>
      <td>-1.205950</td>
      <td>1.067483</td>
      <td>1.246078</td>
      <td>-0.070047</td>
      <td>home_win</td>
    </tr>
    <tr>
      <th>493040</th>
      <td>8.942737</td>
      <td>-2.146976</td>
      <td>1.394474</td>
      <td>-1.573403</td>
      <td>-0.305385</td>
      <td>0.326027</td>
      <td>1.666843</td>
      <td>0.694338</td>
      <td>0.105792</td>
      <td>1.385551</td>
      <td>1.831429</td>
      <td>1.436093</td>
      <td>-0.753915</td>
      <td>-1.703966</td>
      <td>1.116009</td>
      <td>1.716745</td>
      <td>1.133649</td>
      <td>-0.670094</td>
      <td>-1.374712</td>
      <td>0.468374</td>
      <td>1.822806</td>
      <td>1.126330</td>
      <td>-0.789073</td>
      <td>-1.478142</td>
      <td>0.878564</td>
      <td>0.882744</td>
      <td>1.703690</td>
      <td>0.053693</td>
      <td>0.563981</td>
      <td>0.978446</td>
      <td>-0.780143</td>
      <td>-0.700812</td>
      <td>0.612057</td>
      <td>1.416142</td>
      <td>2.059364</td>
      <td>-0.691788</td>
      <td>-2.016431</td>
      <td>0.356132</td>
      <td>-1.164456</td>
      <td>2.251789</td>
      <td>0.416905</td>
      <td>-2.210474</td>
      <td>-0.336256</td>
      <td>0.054563</td>
      <td>2.588626</td>
      <td>1.923084</td>
      <td>-2.893726</td>
      <td>-2.042800</td>
      <td>-1.018973</td>
      <td>-1.292926</td>
      <td>1.464444</td>
      <td>0.379332</td>
      <td>-0.902796</td>
      <td>-0.837762</td>
      <td>0.211743</td>
      <td>9.554376</td>
      <td>-0.788436</td>
      <td>2.277353</td>
      <td>0.709577</td>
      <td>0.731361</td>
      <td>1.970444</td>
      <td>1.864556</td>
      <td>-0.597320</td>
      <td>0.168243</td>
      <td>0.246152</td>
      <td>4.470705</td>
      <td>2.449109</td>
      <td>-1.396639</td>
      <td>-0.315235</td>
      <td>0.008914</td>
      <td>1.829497</td>
      <td>1.079852</td>
      <td>-2.363541</td>
      <td>-0.545278</td>
      <td>0.775451</td>
      <td>4.019384</td>
      <td>0.341251</td>
      <td>-2.730811</td>
      <td>-1.093104</td>
      <td>0.247451</td>
      <td>0.682845</td>
      <td>0.218515</td>
      <td>-0.581395</td>
      <td>0.951872</td>
      <td>2.794600</td>
      <td>-0.793495</td>
      <td>-0.740710</td>
      <td>0.492791</td>
      <td>0.151706</td>
      <td>2.489086</td>
      <td>1.933672</td>
      <td>0.891587</td>
      <td>-1.109525</td>
      <td>-0.792216</td>
      <td>-0.402166</td>
      <td>-0.203211</td>
      <td>-1.625096</td>
      <td>-0.009942</td>
      <td>0.220891</td>
      <td>1.091029</td>
      <td>0.344323</td>
      <td>-3.281820</td>
      <td>-1.047589</td>
      <td>-0.870665</td>
      <td>0.278254</td>
      <td>0.817798</td>
      <td>-2.420838</td>
      <td>-0.361229</td>
      <td>0.689072</td>
      <td>-0.850103</td>
      <td>draw</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1992089</th>
      <td>10.658392</td>
      <td>-1.977648</td>
      <td>0.867377</td>
      <td>0.480266</td>
      <td>1.074043</td>
      <td>2.309500</td>
      <td>0.931667</td>
      <td>-3.860209</td>
      <td>0.540601</td>
      <td>-0.235396</td>
      <td>4.829365</td>
      <td>3.813902</td>
      <td>-2.190143</td>
      <td>2.896975</td>
      <td>0.303158</td>
      <td>4.363347</td>
      <td>3.346856</td>
      <td>-2.511394</td>
      <td>1.256390</td>
      <td>-1.264653</td>
      <td>0.693648</td>
      <td>2.017825</td>
      <td>-1.753765</td>
      <td>-2.152817</td>
      <td>-0.662817</td>
      <td>2.162406</td>
      <td>-2.749882</td>
      <td>-4.593596</td>
      <td>0.434768</td>
      <td>0.841622</td>
      <td>-0.817444</td>
      <td>1.999270</td>
      <td>-0.449227</td>
      <td>-0.166485</td>
      <td>-0.336412</td>
      <td>1.345606</td>
      <td>0.069802</td>
      <td>-3.170566</td>
      <td>-0.790618</td>
      <td>1.344050</td>
      <td>-1.052756</td>
      <td>0.131148</td>
      <td>-1.406108</td>
      <td>-0.920376</td>
      <td>1.149814</td>
      <td>-0.756354</td>
      <td>-0.555587</td>
      <td>-0.498005</td>
      <td>0.934758</td>
      <td>-2.606863</td>
      <td>-2.571317</td>
      <td>-2.788252</td>
      <td>-0.783057</td>
      <td>-0.228549</td>
      <td>0.825315</td>
      <td>11.297455</td>
      <td>-1.943311</td>
      <td>-0.622608</td>
      <td>1.229927</td>
      <td>2.987196</td>
      <td>0.715893</td>
      <td>1.452546</td>
      <td>-2.111059</td>
      <td>-0.524585</td>
      <td>0.158115</td>
      <td>5.742015</td>
      <td>3.143138</td>
      <td>-3.280495</td>
      <td>0.901933</td>
      <td>-1.055971</td>
      <td>3.559549</td>
      <td>2.023672</td>
      <td>-2.954817</td>
      <td>1.214646</td>
      <td>0.395441</td>
      <td>-0.652392</td>
      <td>0.617508</td>
      <td>-2.119288</td>
      <td>0.420918</td>
      <td>-0.312378</td>
      <td>-1.179215</td>
      <td>0.990786</td>
      <td>-0.758331</td>
      <td>0.280326</td>
      <td>-1.019536</td>
      <td>-0.983237</td>
      <td>1.033960</td>
      <td>-0.714609</td>
      <td>-0.027433</td>
      <td>0.645499</td>
      <td>-0.468185</td>
      <td>-2.492838</td>
      <td>-2.135542</td>
      <td>0.672037</td>
      <td>1.175961</td>
      <td>0.089223</td>
      <td>-1.409898</td>
      <td>-2.184536</td>
      <td>0.860498</td>
      <td>2.273182</td>
      <td>-1.604118</td>
      <td>-1.071515</td>
      <td>-1.719817</td>
      <td>-0.597994</td>
      <td>0.268359</td>
      <td>-2.027482</td>
      <td>-0.929774</td>
      <td>-0.429955</td>
      <td>-0.408179</td>
      <td>-0.159691</td>
      <td>draw</td>
    </tr>
    <tr>
      <th>1992091</th>
      <td>12.241704</td>
      <td>-2.064268</td>
      <td>0.575912</td>
      <td>0.027214</td>
      <td>0.566339</td>
      <td>2.941632</td>
      <td>1.263739</td>
      <td>-2.564271</td>
      <td>-0.317355</td>
      <td>0.251567</td>
      <td>-0.338978</td>
      <td>1.001774</td>
      <td>-1.054876</td>
      <td>0.023669</td>
      <td>0.503527</td>
      <td>3.322061</td>
      <td>2.191727</td>
      <td>-1.431455</td>
      <td>-0.935647</td>
      <td>-2.106780</td>
      <td>0.805775</td>
      <td>2.313243</td>
      <td>-1.001739</td>
      <td>-0.734564</td>
      <td>-1.002397</td>
      <td>0.167600</td>
      <td>0.748910</td>
      <td>-2.441020</td>
      <td>-0.128875</td>
      <td>1.354357</td>
      <td>-0.992210</td>
      <td>1.486864</td>
      <td>-0.837892</td>
      <td>-0.283414</td>
      <td>0.885329</td>
      <td>-0.144093</td>
      <td>-1.113188</td>
      <td>-1.698684</td>
      <td>0.131062</td>
      <td>0.388273</td>
      <td>-1.074581</td>
      <td>-2.141164</td>
      <td>-1.372831</td>
      <td>0.593159</td>
      <td>-0.060973</td>
      <td>-1.374814</td>
      <td>-2.597658</td>
      <td>-1.353337</td>
      <td>-0.079045</td>
      <td>0.449154</td>
      <td>1.613120</td>
      <td>-2.543332</td>
      <td>-2.596794</td>
      <td>1.483082</td>
      <td>0.105411</td>
      <td>12.290704</td>
      <td>-2.430347</td>
      <td>-1.144252</td>
      <td>-0.197618</td>
      <td>0.806754</td>
      <td>4.561174</td>
      <td>1.116819</td>
      <td>-3.853639</td>
      <td>-1.449020</td>
      <td>-0.008049</td>
      <td>0.859240</td>
      <td>1.593152</td>
      <td>-1.523836</td>
      <td>0.182434</td>
      <td>0.923206</td>
      <td>3.366277</td>
      <td>2.096648</td>
      <td>-2.560049</td>
      <td>1.302320</td>
      <td>0.621418</td>
      <td>-0.179065</td>
      <td>1.542439</td>
      <td>-0.883813</td>
      <td>-1.534520</td>
      <td>-0.929437</td>
      <td>-1.274091</td>
      <td>1.044816</td>
      <td>-0.632610</td>
      <td>0.214867</td>
      <td>-1.155989</td>
      <td>-1.165921</td>
      <td>1.062657</td>
      <td>-0.462190</td>
      <td>-0.128853</td>
      <td>0.815702</td>
      <td>-0.551649</td>
      <td>-3.139936</td>
      <td>-1.871850</td>
      <td>0.919162</td>
      <td>1.101761</td>
      <td>-0.895229</td>
      <td>-1.357762</td>
      <td>-1.203311</td>
      <td>0.502165</td>
      <td>1.217481</td>
      <td>0.817539</td>
      <td>-1.879420</td>
      <td>-1.866788</td>
      <td>1.257348</td>
      <td>-2.040793</td>
      <td>-2.027482</td>
      <td>-0.929774</td>
      <td>-0.429955</td>
      <td>-0.408179</td>
      <td>-0.159691</td>
      <td>home_win</td>
    </tr>
    <tr>
      <th>1992092</th>
      <td>12.064639</td>
      <td>-2.316174</td>
      <td>0.433132</td>
      <td>0.848287</td>
      <td>2.231699</td>
      <td>0.041330</td>
      <td>1.067682</td>
      <td>-1.528655</td>
      <td>1.481193</td>
      <td>0.463667</td>
      <td>0.484107</td>
      <td>2.132478</td>
      <td>-2.712916</td>
      <td>0.415998</td>
      <td>-0.482643</td>
      <td>2.368090</td>
      <td>1.818320</td>
      <td>-1.835697</td>
      <td>2.213761</td>
      <td>1.021893</td>
      <td>2.542915</td>
      <td>2.152431</td>
      <td>-2.292237</td>
      <td>-0.564741</td>
      <td>0.029556</td>
      <td>-0.661431</td>
      <td>1.259634</td>
      <td>-1.538893</td>
      <td>0.327645</td>
      <td>1.036215</td>
      <td>-2.276135</td>
      <td>-1.075133</td>
      <td>-1.057017</td>
      <td>0.562307</td>
      <td>1.955697</td>
      <td>1.429669</td>
      <td>1.815169</td>
      <td>-2.713671</td>
      <td>-1.202667</td>
      <td>-0.206876</td>
      <td>-2.066585</td>
      <td>-1.486135</td>
      <td>-2.249610</td>
      <td>0.528683</td>
      <td>1.248011</td>
      <td>2.305110</td>
      <td>0.196755</td>
      <td>-3.015813</td>
      <td>0.884716</td>
      <td>1.782990</td>
      <td>0.202502</td>
      <td>0.363303</td>
      <td>-2.203897</td>
      <td>0.890204</td>
      <td>1.972601</td>
      <td>11.605200</td>
      <td>-2.180668</td>
      <td>0.633318</td>
      <td>0.505193</td>
      <td>2.121379</td>
      <td>0.936419</td>
      <td>1.221556</td>
      <td>-2.522024</td>
      <td>-0.246410</td>
      <td>0.123592</td>
      <td>0.472920</td>
      <td>1.911681</td>
      <td>-1.404507</td>
      <td>-1.552859</td>
      <td>0.102178</td>
      <td>2.614211</td>
      <td>3.470441</td>
      <td>-0.910743</td>
      <td>4.107990</td>
      <td>0.561911</td>
      <td>-0.386817</td>
      <td>2.148362</td>
      <td>-0.789333</td>
      <td>0.490890</td>
      <td>0.348217</td>
      <td>1.412310</td>
      <td>-0.776693</td>
      <td>-3.695553</td>
      <td>-2.537028</td>
      <td>0.654647</td>
      <td>0.038726</td>
      <td>0.194863</td>
      <td>-1.450742</td>
      <td>-1.480179</td>
      <td>-0.329070</td>
      <td>-1.248945</td>
      <td>0.135936</td>
      <td>-0.373578</td>
      <td>-0.008843</td>
      <td>1.145546</td>
      <td>-1.268382</td>
      <td>-2.736194</td>
      <td>-2.134065</td>
      <td>-2.336878</td>
      <td>0.312531</td>
      <td>-1.267814</td>
      <td>-0.952598</td>
      <td>-0.567959</td>
      <td>1.417181</td>
      <td>-1.031334</td>
      <td>-2.777047</td>
      <td>-2.958610</td>
      <td>-1.015094</td>
      <td>-0.426489</td>
      <td>1.111898</td>
      <td>away_win</td>
    </tr>
    <tr>
      <th>1992093</th>
      <td>12.055222</td>
      <td>-2.425069</td>
      <td>1.339212</td>
      <td>0.274265</td>
      <td>0.840987</td>
      <td>0.807082</td>
      <td>0.384775</td>
      <td>-3.089207</td>
      <td>-1.255036</td>
      <td>0.706752</td>
      <td>3.316945</td>
      <td>2.584242</td>
      <td>-1.555439</td>
      <td>-0.624218</td>
      <td>-1.494029</td>
      <td>0.752059</td>
      <td>2.634155</td>
      <td>0.029098</td>
      <td>-2.048228</td>
      <td>-2.345172</td>
      <td>-0.477442</td>
      <td>0.588966</td>
      <td>-1.810286</td>
      <td>-0.671132</td>
      <td>0.670969</td>
      <td>-4.479330</td>
      <td>0.957108</td>
      <td>1.567778</td>
      <td>1.821481</td>
      <td>1.887503</td>
      <td>-0.304292</td>
      <td>2.229022</td>
      <td>-0.374285</td>
      <td>-0.629550</td>
      <td>-0.550464</td>
      <td>-2.488337</td>
      <td>-2.492236</td>
      <td>0.004280</td>
      <td>-0.328167</td>
      <td>-0.137174</td>
      <td>-1.303862</td>
      <td>-3.461748</td>
      <td>-0.804760</td>
      <td>-0.413108</td>
      <td>-1.193188</td>
      <td>-3.587590</td>
      <td>-2.419330</td>
      <td>0.831607</td>
      <td>2.357133</td>
      <td>0.281695</td>
      <td>-2.476574</td>
      <td>-2.795499</td>
      <td>-0.080093</td>
      <td>0.062647</td>
      <td>-2.011499</td>
      <td>12.360092</td>
      <td>-1.868190</td>
      <td>1.990823</td>
      <td>0.546306</td>
      <td>0.372803</td>
      <td>-1.329084</td>
      <td>1.395726</td>
      <td>-0.722913</td>
      <td>-0.893829</td>
      <td>0.215497</td>
      <td>1.165437</td>
      <td>3.021942</td>
      <td>-0.188859</td>
      <td>2.311939</td>
      <td>0.403286</td>
      <td>-4.008986</td>
      <td>1.662399</td>
      <td>0.332054</td>
      <td>0.291516</td>
      <td>1.172819</td>
      <td>-1.397060</td>
      <td>1.400592</td>
      <td>-0.140128</td>
      <td>-0.156016</td>
      <td>-0.683065</td>
      <td>-1.079926</td>
      <td>0.426619</td>
      <td>-0.576034</td>
      <td>-2.142184</td>
      <td>-0.515036</td>
      <td>-0.909491</td>
      <td>1.934916</td>
      <td>0.160027</td>
      <td>0.916388</td>
      <td>1.269362</td>
      <td>-1.061561</td>
      <td>-2.717686</td>
      <td>-0.665092</td>
      <td>-0.671323</td>
      <td>-2.761000</td>
      <td>-1.823349</td>
      <td>-1.744156</td>
      <td>-0.979761</td>
      <td>0.332470</td>
      <td>0.702492</td>
      <td>-1.032879</td>
      <td>-1.066695</td>
      <td>-0.612696</td>
      <td>-0.450766</td>
      <td>0.290344</td>
      <td>-1.642076</td>
      <td>-3.316316</td>
      <td>0.011961</td>
      <td>0.375835</td>
      <td>-3.465042</td>
      <td>home_win</td>
    </tr>
    <tr>
      <th>1992095</th>
      <td>11.109830</td>
      <td>-2.853322</td>
      <td>3.703596</td>
      <td>-0.584121</td>
      <td>-0.402950</td>
      <td>-0.371390</td>
      <td>1.595755</td>
      <td>-1.118035</td>
      <td>-1.088917</td>
      <td>-0.072962</td>
      <td>0.782687</td>
      <td>3.074394</td>
      <td>-0.351447</td>
      <td>0.339711</td>
      <td>-0.876789</td>
      <td>0.954087</td>
      <td>3.884860</td>
      <td>0.323121</td>
      <td>-0.869362</td>
      <td>-1.514651</td>
      <td>-1.424525</td>
      <td>1.026468</td>
      <td>-0.666682</td>
      <td>-0.521699</td>
      <td>1.419345</td>
      <td>-2.778054</td>
      <td>-3.370352</td>
      <td>-0.446712</td>
      <td>-0.978029</td>
      <td>-0.758080</td>
      <td>2.408722</td>
      <td>1.658858</td>
      <td>-2.251337</td>
      <td>-0.820091</td>
      <td>-0.630519</td>
      <td>-2.159654</td>
      <td>1.814418</td>
      <td>0.256719</td>
      <td>0.567236</td>
      <td>-0.314540</td>
      <td>-2.998560</td>
      <td>-3.922779</td>
      <td>0.105072</td>
      <td>0.225998</td>
      <td>1.222075</td>
      <td>-1.617306</td>
      <td>-2.339904</td>
      <td>-0.992463</td>
      <td>-1.548240</td>
      <td>0.519653</td>
      <td>-2.910805</td>
      <td>-2.167197</td>
      <td>0.101099</td>
      <td>1.144445</td>
      <td>-0.165881</td>
      <td>12.307758</td>
      <td>-1.719545</td>
      <td>4.261962</td>
      <td>0.728169</td>
      <td>1.500838</td>
      <td>-1.795498</td>
      <td>2.597198</td>
      <td>0.399761</td>
      <td>-0.004946</td>
      <td>-0.912142</td>
      <td>1.097353</td>
      <td>2.822507</td>
      <td>-0.720652</td>
      <td>-1.295103</td>
      <td>-1.490739</td>
      <td>-1.267830</td>
      <td>3.175089</td>
      <td>1.368422</td>
      <td>-0.396539</td>
      <td>-2.110602</td>
      <td>0.679151</td>
      <td>1.514007</td>
      <td>-0.744474</td>
      <td>-2.598616</td>
      <td>-1.680172</td>
      <td>-2.472498</td>
      <td>1.701903</td>
      <td>0.506631</td>
      <td>-1.382100</td>
      <td>-0.412576</td>
      <td>-3.875220</td>
      <td>1.593144</td>
      <td>1.643325</td>
      <td>2.150146</td>
      <td>2.170000</td>
      <td>-1.395402</td>
      <td>0.464636</td>
      <td>-0.435683</td>
      <td>-0.975001</td>
      <td>-1.386601</td>
      <td>-3.586664</td>
      <td>-2.865959</td>
      <td>0.395006</td>
      <td>1.476579</td>
      <td>2.034092</td>
      <td>-4.892234</td>
      <td>-0.601987</td>
      <td>1.354383</td>
      <td>0.830095</td>
      <td>-0.426868</td>
      <td>-3.654878</td>
      <td>-2.094599</td>
      <td>1.339425</td>
      <td>0.285083</td>
      <td>-2.245146</td>
      <td>home_win</td>
    </tr>
  </tbody>
</table>
<p>21374 rows × 111 columns</p>
</div>



- Split the table into train and test set.


```python
train_bool = df_match_player_attr_pcs.reset_index().match_api_id.isin(train_match_api_id)
test_bool = df_match_player_attr_pcs.reset_index().match_api_id.isin(test_match_api_id) 
```


```python
df_pc_train = df_match_player_attr_pcs.reset_index()[train_bool].set_index("match_api_id")
df_pc_test = df_match_player_attr_pcs.reset_index()[test_bool].set_index("match_api_id")
```


```python
X_pc_train = df_pc_train.drop("match_result", axis = 1)
y_pc_train = df_pc_train.match_result 

X_pc_test = df_pc_test.drop("match_result", axis = 1)
y_pc_test = df_pc_test.match_result
```


```python
print("Number of train data: ", X_pc_train.shape[0])
print("Number of test data: ", X_pc_test.shape[0])
```

    Number of train data:  16988
    Number of test data:  2621


- Preprocess the data before modeling.


```python
# Transform the match_result class to numerical labels.

le = preprocessing.LabelEncoder()
le.fit(y_pc_train)

y_pc_train_encd = le.transform(y_pc_train)
y_pc_test_encd = le.transform(y_pc_test)
```


```python
names = ["KNN", 
         "LDA", 
         "QDA", 
         "Naive Bayes",
         "Logistic regression",
         "Decesion tree", 
         "Random Forest",  
         "AdaBoost",
         "XGBoost",
         "Polynomial kernel SVM",
         "Radial kernel SVM",
         "GBM",
         "LightGBM"
        ]

classifiers = [
    KNeighborsClassifier(3),
    LinearDiscriminantAnalysis(),
    QuadraticDiscriminantAnalysis(),
    GaussianNB(), 
    LogisticRegression(),
    DecisionTreeClassifier(random_state = 42),
    RandomForestClassifier(),
    AdaBoostClassifier(),
    xgb.XGBClassifier(),
    SVC(kernel = "poly", probability = True),
    SVC(kernel = "rbf", probability = True),
    GradientBoostingClassifier(),
    lgb.LGBMClassifier()
    ]
```


```python
result_accuracy = pd.DataFrame(names, columns = ["model_name"])
```


```python
# baseline accuracy

y_pred_baseline = le.transform(["home_win"])
baseline_accuracy = np.mean(y_pred_baseline == y_pc_test_encd)
result_accuracy["Baseline accuracy"] = baseline_accuracy
```


```python
y_pred_dict = {}

for name, clf in zip(names, classifiers):
    clf.fit(X_pc_train, y_pc_train_encd)
    
    y_pred = clf.predict(X_pc_test)
    y_pred_dict[name] = y_pred
    
    accuracy = np.mean(y_pred == y_pc_test_encd)
    
    result_accuracy.loc[result_accuracy.model_name == name, "Player PC Variables"] = round(accuracy * 100, 3)
```


```python
result_accuracy
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model_name</th>
      <th>Baseline accuracy</th>
      <th>Player PC Variables</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>KNN</td>
      <td>0.442961</td>
      <td>41.320</td>
    </tr>
    <tr>
      <th>1</th>
      <td>LDA</td>
      <td>0.442961</td>
      <td>49.943</td>
    </tr>
    <tr>
      <th>2</th>
      <td>QDA</td>
      <td>0.442961</td>
      <td>42.350</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Naive Bayes</td>
      <td>0.442961</td>
      <td>47.272</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logistic regression</td>
      <td>0.442961</td>
      <td>50.172</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Decesion tree</td>
      <td>0.442961</td>
      <td>38.077</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Random Forest</td>
      <td>0.442961</td>
      <td>49.790</td>
    </tr>
    <tr>
      <th>7</th>
      <td>AdaBoost</td>
      <td>0.442961</td>
      <td>50.362</td>
    </tr>
    <tr>
      <th>8</th>
      <td>XGBoost</td>
      <td>0.442961</td>
      <td>47.196</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Polynomial kernel SVM</td>
      <td>0.442961</td>
      <td>50.515</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Radial kernel SVM</td>
      <td>0.442961</td>
      <td>50.897</td>
    </tr>
    <tr>
      <th>11</th>
      <td>GBM</td>
      <td>0.442961</td>
      <td>50.820</td>
    </tr>
    <tr>
      <th>12</th>
      <td>LightGBM</td>
      <td>0.442961</td>
      <td>49.866</td>
    </tr>
  </tbody>
</table>
</div>



- Except for the KNN, QDA, and Decision tree models, all models have higher accuracies than the baseline accuracy when use the player PC information.

## 3.2. Variable set 2: Betting information features


```python
df_match_betting_stat = df_match_betting_stat.merge(df_match_basic[["match_api_id", "match_result"]], how = "left", on = "match_api_id")
```

- Split the data into train and test set.


```python
train_bool = df_match_betting_stat.match_api_id.isin(train_match_api_id)
test_bool = df_match_betting_stat.match_api_id.isin(test_match_api_id) 
```


```python
df_bet_train = df_match_betting_stat[train_bool].set_index("match_api_id")
df_bet_test = df_match_betting_stat[test_bool].set_index("match_api_id")
```


```python
X_bet_train = df_bet_train.drop("match_result", axis = 1)
y_bet_train = df_bet_train.match_result 

X_bet_test = df_bet_test.drop("match_result", axis = 1)
y_bet_test = df_bet_test.match_result
```


```python
print("Number of train data: ", X_bet_train.shape[0])
print("Number of test data: ", X_bet_test.shape[0])
```

    Number of train data:  16988
    Number of test data:  2621


- Preprocess variables before modeling.


```python
# Transform the match_result class to numerical labels.
y_bet_train_encd = le.transform(y_bet_train)
y_bet_test_encd = le.transform(y_bet_test)
```


```python
# Standardize features
col_names = X_bet_train.columns

scaler = StandardScaler()
scaler.fit(X_bet_train)

X_bet_train_std = pd.DataFrame(scaler.transform(X_bet_train), columns = col_names)
X_bet_test_std = pd.DataFrame(scaler.transform(X_bet_test), columns = col_names)

```


```python
for name, clf in zip(names, classifiers):
    clf.fit(X_bet_train_std, y_bet_train_encd)
    
    y_pred = clf.predict(X_bet_test_std)
    y_pred_dict[name] = y_pred
    
    accuracy = np.mean(y_pred == y_bet_test_encd)
    
    result_accuracy.loc[result_accuracy.model_name == name, "Betting Statistics Variables"] = round(accuracy * 100, 3)
```


```python
result_accuracy
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model_name</th>
      <th>Baseline accuracy</th>
      <th>Player PC Variables</th>
      <th>Betting Statistics Variables</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>KNN</td>
      <td>0.442961</td>
      <td>41.320</td>
      <td>44.220</td>
    </tr>
    <tr>
      <th>1</th>
      <td>LDA</td>
      <td>0.442961</td>
      <td>49.943</td>
      <td>51.469</td>
    </tr>
    <tr>
      <th>2</th>
      <td>QDA</td>
      <td>0.442961</td>
      <td>42.350</td>
      <td>40.557</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Naive Bayes</td>
      <td>0.442961</td>
      <td>47.272</td>
      <td>42.198</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logistic regression</td>
      <td>0.442961</td>
      <td>50.172</td>
      <td>51.545</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Decesion tree</td>
      <td>0.442961</td>
      <td>38.077</td>
      <td>43.304</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Random Forest</td>
      <td>0.442961</td>
      <td>49.790</td>
      <td>48.798</td>
    </tr>
    <tr>
      <th>7</th>
      <td>AdaBoost</td>
      <td>0.442961</td>
      <td>50.362</td>
      <td>51.698</td>
    </tr>
    <tr>
      <th>8</th>
      <td>XGBoost</td>
      <td>0.442961</td>
      <td>47.196</td>
      <td>50.439</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Polynomial kernel SVM</td>
      <td>0.442961</td>
      <td>50.515</td>
      <td>48.760</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Radial kernel SVM</td>
      <td>0.442961</td>
      <td>50.897</td>
      <td>51.393</td>
    </tr>
    <tr>
      <th>11</th>
      <td>GBM</td>
      <td>0.442961</td>
      <td>50.820</td>
      <td>52.079</td>
    </tr>
    <tr>
      <th>12</th>
      <td>LightGBM</td>
      <td>0.442961</td>
      <td>49.866</td>
      <td>52.041</td>
    </tr>
  </tbody>
</table>
</div>



- Except for the KNN, QDA, Naive Bayes, and decision tree models, all models have higher accuracies than the baseline accuracy when use the betting statistics information.
- Overall, accuracies are higher when using betting information than when using pc information.

## 3.3 Variable set 3: Team attribute features


```python
df_match_team_num_attr = df_match_team_num_attr.merge(df_match_basic[["match_api_id", "match_result"]], how = "left", on = "match_api_id")
```

- Split the data into train and test set.


```python
train_bool = df_match_team_num_attr.match_api_id.isin(train_match_api_id)
test_bool = df_match_team_num_attr.match_api_id.isin(test_match_api_id) 
```


```python
df_team_train = df_match_team_num_attr[train_bool].set_index("match_api_id")
df_team_test = df_match_team_num_attr[test_bool].set_index("match_api_id")
```


```python
X_team_train = df_team_train.drop("match_result", axis = 1)
y_team_train = df_team_train.match_result 

X_team_test = df_team_test.drop("match_result", axis = 1)
y_team_test = df_team_test.match_result
```


```python
print("Number of train data: ", X_team_train.shape[0])
print("Number of test data: ", X_team_test.shape[0])
```

    Number of train data:  16988
    Number of test data:  2621


- Preprocess the data before modeling.


```python
# Transform the match_result class to numerical labels.
y_team_train_encd = le.transform(y_team_train)
y_team_test_encd = le.transform(y_team_test)
```


```python
# fill the missing values with 0
X_team_train.fillna(0, inplace = True)
X_team_test.fillna(0, inplace = True)
```


```python
# Standardize features
col_names = X_team_train.columns

scaler = StandardScaler()
scaler.fit(X_team_train)

X_team_train_std = pd.DataFrame(scaler.transform(X_team_train), columns = col_names)
X_team_test_std = pd.DataFrame(scaler.transform(X_team_test), columns = col_names)

```


```python
for name, clf in zip(names, classifiers):
    clf.fit(X_team_train_std, y_team_train_encd)
    
    y_pred = clf.predict(X_team_test_std)
    y_pred_dict[name] = y_pred
    
    accuracy = np.mean(y_pred == y_team_test_encd)
    
    result_accuracy.loc[result_accuracy.model_name == name, "Team attribute Variables"] = round(accuracy * 100, 3)
```


```python
result_accuracy
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model_name</th>
      <th>Baseline accuracy</th>
      <th>Player PC Variables</th>
      <th>Betting Statistics Variables</th>
      <th>Team attribute Variables</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>KNN</td>
      <td>0.442961</td>
      <td>41.320</td>
      <td>44.220</td>
      <td>39.412</td>
    </tr>
    <tr>
      <th>1</th>
      <td>LDA</td>
      <td>0.442961</td>
      <td>49.943</td>
      <td>51.469</td>
      <td>45.670</td>
    </tr>
    <tr>
      <th>2</th>
      <td>QDA</td>
      <td>0.442961</td>
      <td>42.350</td>
      <td>40.557</td>
      <td>45.784</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Naive Bayes</td>
      <td>0.442961</td>
      <td>47.272</td>
      <td>42.198</td>
      <td>46.280</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logistic regression</td>
      <td>0.442961</td>
      <td>50.172</td>
      <td>51.545</td>
      <td>45.555</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Decesion tree</td>
      <td>0.442961</td>
      <td>38.077</td>
      <td>43.304</td>
      <td>38.001</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Random Forest</td>
      <td>0.442961</td>
      <td>49.790</td>
      <td>48.798</td>
      <td>45.326</td>
    </tr>
    <tr>
      <th>7</th>
      <td>AdaBoost</td>
      <td>0.442961</td>
      <td>50.362</td>
      <td>51.698</td>
      <td>46.814</td>
    </tr>
    <tr>
      <th>8</th>
      <td>XGBoost</td>
      <td>0.442961</td>
      <td>47.196</td>
      <td>50.439</td>
      <td>43.037</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Polynomial kernel SVM</td>
      <td>0.442961</td>
      <td>50.515</td>
      <td>48.760</td>
      <td>44.601</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Radial kernel SVM</td>
      <td>0.442961</td>
      <td>50.897</td>
      <td>51.393</td>
      <td>44.868</td>
    </tr>
    <tr>
      <th>11</th>
      <td>GBM</td>
      <td>0.442961</td>
      <td>50.820</td>
      <td>52.079</td>
      <td>47.310</td>
    </tr>
    <tr>
      <th>12</th>
      <td>LightGBM</td>
      <td>0.442961</td>
      <td>49.866</td>
      <td>52.041</td>
      <td>47.119</td>
    </tr>
  </tbody>
</table>
</div>



- Except for the KNN, XGBoost, and decision tree models, all models have higher accuracies than the baseline accuracy when use the each team's attribute information.
- When using team attribute information, the overall accuracies are lower than when using other variables.

## 3.4. Variable set 4: Goal and win percentage rolling features


```python
df_team_win_goal_rolling_features = df_team_win_goal_rolling_features.merge(df_match_basic[["match_api_id", "match_result"]], how = "left", on = "match_api_id")
```

- Split the data into train and test set.


```python
train_bool = df_team_win_goal_rolling_features.reset_index().match_api_id.isin(train_match_api_id)
test_bool = df_team_win_goal_rolling_features.reset_index().match_api_id.isin(test_match_api_id) 
```


```python
df_rolling_train = df_team_win_goal_rolling_features[train_bool].set_index("match_api_id")
df_rolling_test = df_team_win_goal_rolling_features[test_bool].set_index("match_api_id")
```


```python
X_rolling_train = df_rolling_train.drop("match_result", axis = 1)
y_rolling_train = df_rolling_train.match_result 

X_rolling_test = df_rolling_test.drop("match_result", axis = 1)
y_rolling_test = df_rolling_test.match_result
```


```python
print("Number of train data: ", X_rolling_train.shape[0])
print("Number of test data: ", X_rolling_test.shape[0])
```

    Number of train data:  16988
    Number of test data:  2621


- Preprocess the data befor modeling.


```python
# Transform the match_result class to numerical labels.

y_rolling_train_encd = le.transform(y_rolling_train)
y_rolling_test_encd = le.transform(y_rolling_test)
```


```python
# fill missing values with 0

X_rolling_train.fillna(0, inplace = True)
X_rolling_test.fillna(0, inplace = True)
```


```python
# Standardize features

col_names = X_rolling_train.columns

scaler = StandardScaler()
scaler.fit(X_rolling_train)

X_rolling_train_std = pd.DataFrame(scaler.transform(X_rolling_train), columns = col_names)
X_rolling_test_std = pd.DataFrame(scaler.transform(X_rolling_test), columns = col_names)

```


```python
for name, clf in zip(names, classifiers):
    clf.fit(X_rolling_train_std, y_rolling_train_encd)
    
    y_pred = clf.predict(X_rolling_test_std)
    y_pred_dict[name] = y_pred
    
    accuracy = np.mean(y_pred == y_rolling_test_encd)
    
    result_accuracy.loc[result_accuracy.model_name == name, "Team's goal and win percentage rolling Variables"] = round(accuracy * 100, 3)
```


```python
result_accuracy
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model_name</th>
      <th>Baseline accuracy</th>
      <th>Player PC Variables</th>
      <th>Betting Statistics Variables</th>
      <th>Team attribute Variables</th>
      <th>Team's goal and win percentage rolling Variables</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>KNN</td>
      <td>0.442961</td>
      <td>41.320</td>
      <td>44.220</td>
      <td>39.412</td>
      <td>43.342</td>
    </tr>
    <tr>
      <th>1</th>
      <td>LDA</td>
      <td>0.442961</td>
      <td>49.943</td>
      <td>51.469</td>
      <td>45.670</td>
      <td>49.676</td>
    </tr>
    <tr>
      <th>2</th>
      <td>QDA</td>
      <td>0.442961</td>
      <td>42.350</td>
      <td>40.557</td>
      <td>45.784</td>
      <td>45.059</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Naive Bayes</td>
      <td>0.442961</td>
      <td>47.272</td>
      <td>42.198</td>
      <td>46.280</td>
      <td>46.929</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logistic regression</td>
      <td>0.442961</td>
      <td>50.172</td>
      <td>51.545</td>
      <td>45.555</td>
      <td>49.790</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Decesion tree</td>
      <td>0.442961</td>
      <td>38.077</td>
      <td>43.304</td>
      <td>38.001</td>
      <td>39.489</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Random Forest</td>
      <td>0.442961</td>
      <td>49.790</td>
      <td>48.798</td>
      <td>45.326</td>
      <td>49.447</td>
    </tr>
    <tr>
      <th>7</th>
      <td>AdaBoost</td>
      <td>0.442961</td>
      <td>50.362</td>
      <td>51.698</td>
      <td>46.814</td>
      <td>50.019</td>
    </tr>
    <tr>
      <th>8</th>
      <td>XGBoost</td>
      <td>0.442961</td>
      <td>47.196</td>
      <td>50.439</td>
      <td>43.037</td>
      <td>48.607</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Polynomial kernel SVM</td>
      <td>0.442961</td>
      <td>50.515</td>
      <td>48.760</td>
      <td>44.601</td>
      <td>48.264</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Radial kernel SVM</td>
      <td>0.442961</td>
      <td>50.897</td>
      <td>51.393</td>
      <td>44.868</td>
      <td>49.828</td>
    </tr>
    <tr>
      <th>11</th>
      <td>GBM</td>
      <td>0.442961</td>
      <td>50.820</td>
      <td>52.079</td>
      <td>47.310</td>
      <td>49.752</td>
    </tr>
    <tr>
      <th>12</th>
      <td>LightGBM</td>
      <td>0.442961</td>
      <td>49.866</td>
      <td>52.041</td>
      <td>47.119</td>
      <td>48.989</td>
    </tr>
  </tbody>
</table>
</div>



- Except for the KNN and decision tree models, all models have higher accuracies than the baseline accuracy when use the each team's goal and win percentage rolling features.
- Overall, the performance of all models is not bad when using each team's goal and win percentage rolling features.

## 3.5. Variable set 5: each team's Elo rating


```python
df_match_elo = df_match_elo.merge(df_match_basic[["match_api_id", "match_result"]], how = "left", on = "match_api_id")
```

- Split the data into train and test set.


```python
train_bool = df_match_elo.reset_index().match_api_id.isin(train_match_api_id)
test_bool = df_match_elo.reset_index().match_api_id.isin(test_match_api_id) 
```


```python
df_elo_train = df_match_elo[train_bool].set_index("match_api_id")
df_elo_test = df_match_elo[test_bool].set_index("match_api_id")
```


```python
X_elo_train = df_elo_train.drop("match_result", axis = 1)
y_elo_train = df_elo_train.match_result 

X_elo_test = df_elo_test.drop("match_result", axis = 1)
y_elo_test = df_elo_test.match_result
```


```python
print("Number of train data: ", X_rolling_train.shape[0])
print("Number of test data: ", X_rolling_test.shape[0])
```

    Number of train data:  16988
    Number of test data:  2621


- Preprocess the data before modeling.


```python
# Transform the match_result class to numerical labels.

y_elo_train_encd = le.transform(y_elo_train)
y_elo_test_encd = le.transform(y_elo_test)
```


```python
# fill missing values with 0

X_elo_train.fillna(0, inplace = True)
X_elo_test.fillna(0, inplace = True)
```


```python
# Standardize features

col_names = X_elo_train.columns

scaler = StandardScaler()
scaler.fit(X_elo_train)

X_elo_train_std = pd.DataFrame(scaler.transform(X_elo_train), columns = col_names)
X_elo_test_std = pd.DataFrame(scaler.transform(X_elo_test), columns = col_names)

```


```python
for name, clf in zip(names, classifiers):
    clf.fit(X_elo_train_std, y_elo_train_encd)
    
    y_pred = clf.predict(X_elo_test_std)
    y_pred_dict[name] = y_pred
    
    accuracy = np.mean(y_pred == y_elo_test_encd)
    
    result_accuracy.loc[result_accuracy.model_name == name, "Team's Elo rating related Variables"] = round(accuracy * 100, 3)
```


```python
result_accuracy
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model_name</th>
      <th>Baseline accuracy</th>
      <th>Player PC Variables</th>
      <th>Betting Statistics Variables</th>
      <th>Team attribute Variables</th>
      <th>Team's goal and win percentage rolling Variables</th>
      <th>Team's Elo rating related Variables</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>KNN</td>
      <td>0.442961</td>
      <td>41.320</td>
      <td>44.220</td>
      <td>39.412</td>
      <td>43.342</td>
      <td>40.710</td>
    </tr>
    <tr>
      <th>1</th>
      <td>LDA</td>
      <td>0.442961</td>
      <td>49.943</td>
      <td>51.469</td>
      <td>45.670</td>
      <td>49.676</td>
      <td>50.630</td>
    </tr>
    <tr>
      <th>2</th>
      <td>QDA</td>
      <td>0.442961</td>
      <td>42.350</td>
      <td>40.557</td>
      <td>45.784</td>
      <td>45.059</td>
      <td>38.878</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Naive Bayes</td>
      <td>0.442961</td>
      <td>47.272</td>
      <td>42.198</td>
      <td>46.280</td>
      <td>46.929</td>
      <td>48.607</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logistic regression</td>
      <td>0.442961</td>
      <td>50.172</td>
      <td>51.545</td>
      <td>45.555</td>
      <td>49.790</td>
      <td>50.630</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Decesion tree</td>
      <td>0.442961</td>
      <td>38.077</td>
      <td>43.304</td>
      <td>38.001</td>
      <td>39.489</td>
      <td>38.573</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Random Forest</td>
      <td>0.442961</td>
      <td>49.790</td>
      <td>48.798</td>
      <td>45.326</td>
      <td>49.447</td>
      <td>49.142</td>
    </tr>
    <tr>
      <th>7</th>
      <td>AdaBoost</td>
      <td>0.442961</td>
      <td>50.362</td>
      <td>51.698</td>
      <td>46.814</td>
      <td>50.019</td>
      <td>51.011</td>
    </tr>
    <tr>
      <th>8</th>
      <td>XGBoost</td>
      <td>0.442961</td>
      <td>47.196</td>
      <td>50.439</td>
      <td>43.037</td>
      <td>48.607</td>
      <td>48.150</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Polynomial kernel SVM</td>
      <td>0.442961</td>
      <td>50.515</td>
      <td>48.760</td>
      <td>44.601</td>
      <td>48.264</td>
      <td>48.874</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Radial kernel SVM</td>
      <td>0.442961</td>
      <td>50.897</td>
      <td>51.393</td>
      <td>44.868</td>
      <td>49.828</td>
      <td>50.439</td>
    </tr>
    <tr>
      <th>11</th>
      <td>GBM</td>
      <td>0.442961</td>
      <td>50.820</td>
      <td>52.079</td>
      <td>47.310</td>
      <td>49.752</td>
      <td>50.591</td>
    </tr>
    <tr>
      <th>12</th>
      <td>LightGBM</td>
      <td>0.442961</td>
      <td>49.866</td>
      <td>52.041</td>
      <td>47.119</td>
      <td>48.989</td>
      <td>49.561</td>
    </tr>
  </tbody>
</table>
</div>



- Except for the KNN, QDA, and decision tree models, all models have higher accuracies than the baseline accuracy when use the each team's Elo rating related features.

## 3.6. Use all variables

- Merge all feature tables.


```python
df_all = df_match_player_attr_pcs.merge(df_match_betting_stat.drop("match_result", axis = 1), how = "left", on = ["match_api_id"]) \
                                 .merge(df_match_team_num_attr.drop("match_result", axis = 1), how = "left", on = ["match_api_id"]) \
                                 .merge(df_team_win_goal_rolling_features.drop("match_result", axis = 1), how = "left", on = ["match_api_id"])  \
                                 .merge(df_match_elo.drop("match_result", axis = 1), how = "left", on = ["match_api_id"])
                                 
```

- Split the data into train and test set.


```python
train_bool = df_all.match_api_id.isin(train_match_api_id)
test_bool = df_all.match_api_id.isin(test_match_api_id) 
```


```python
df_all_train = df_all[train_bool].set_index("match_api_id")
df_all_test = df_all[test_bool].set_index("match_api_id")
```


```python
X_all_train = df_all_train.drop("match_result", axis = 1)
y_all_train = df_all_train.match_result 

X_all_test = df_all_test.drop("match_result", axis = 1)
y_all_test = df_all_test.match_result
```


```python
print("Number of train data: ", X_all_train.shape[0])
print("Number of test data: ", X_all_test.shape[0])
```

    Number of train data:  16988
    Number of test data:  2621


- Preprocess the data before modeling.


```python
# Transform the match_result class to numerical labels.

y_all_train_encd = le.transform(y_all_train)
y_all_test_encd = le.transform(y_all_test)
```


```python
# fill missing values with 0

X_all_train.fillna(0, inplace = True)
X_all_test.fillna(0, inplace = True)
```


```python
# Standardize features

col_names = X_all_train.columns

scaler = StandardScaler()
scaler.fit(X_all_train)

X_all_train_std = pd.DataFrame(scaler.transform(X_all_train), columns = col_names)
X_all_test_std = pd.DataFrame(scaler.transform(X_all_test), columns = col_names)

```

- Save the tables.


```python
df_all.to_csv("../data/df_all.csv", index = False)

train_match_api_id.to_csv("../data/train_match_api_id.csv", index = False)
test_match_api_id.to_csv("../data/test_match_api_id.csv", index = False)

X_all_train.to_csv("../data/X_all_train.csv", index = False)
X_all_test.to_csv("../data/X_all_train.csv", index = False)

X_all_train_std.to_csv("../data/X_all_train_std.csv", index = False)
X_all_test_std.to_csv("../data/X_all_test_std.csv", index = False)

y_all_train.to_csv("../data/y_all_train.csv", index = False)
y_all_test.to_csv("../data/y_all_test.csv", index = False)

```


```python
for name, clf in zip(names, classifiers):
    clf.fit(X_all_train_std, y_all_train_encd)
    
    y_pred = clf.predict(X_all_test_std)
    y_pred_dict[name] = y_pred
    
    accuracy = np.mean(y_pred == y_all_test_encd)
    
    result_accuracy.loc[result_accuracy.model_name == name, "All Variables"] = round(accuracy * 100, 3)
```


```python
result_accuracy
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model_name</th>
      <th>Baseline accuracy</th>
      <th>Player PC Variables</th>
      <th>Betting Statistics Variables</th>
      <th>Team attribute Variables</th>
      <th>Team's goal and win percentage rolling Variables</th>
      <th>Team's Elo rating related Variables</th>
      <th>All Variables</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>KNN</td>
      <td>0.442961</td>
      <td>41.320</td>
      <td>44.220</td>
      <td>39.412</td>
      <td>43.342</td>
      <td>40.710</td>
      <td>43.686</td>
    </tr>
    <tr>
      <th>1</th>
      <td>LDA</td>
      <td>0.442961</td>
      <td>49.943</td>
      <td>51.469</td>
      <td>45.670</td>
      <td>49.676</td>
      <td>50.630</td>
      <td>50.706</td>
    </tr>
    <tr>
      <th>2</th>
      <td>QDA</td>
      <td>0.442961</td>
      <td>42.350</td>
      <td>40.557</td>
      <td>45.784</td>
      <td>45.059</td>
      <td>38.878</td>
      <td>46.051</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Naive Bayes</td>
      <td>0.442961</td>
      <td>47.272</td>
      <td>42.198</td>
      <td>46.280</td>
      <td>46.929</td>
      <td>48.607</td>
      <td>45.937</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logistic regression</td>
      <td>0.442961</td>
      <td>50.172</td>
      <td>51.545</td>
      <td>45.555</td>
      <td>49.790</td>
      <td>50.630</td>
      <td>51.316</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Decesion tree</td>
      <td>0.442961</td>
      <td>38.077</td>
      <td>43.304</td>
      <td>38.001</td>
      <td>39.489</td>
      <td>38.573</td>
      <td>41.892</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Random Forest</td>
      <td>0.442961</td>
      <td>49.790</td>
      <td>48.798</td>
      <td>45.326</td>
      <td>49.447</td>
      <td>49.142</td>
      <td>52.003</td>
    </tr>
    <tr>
      <th>7</th>
      <td>AdaBoost</td>
      <td>0.442961</td>
      <td>50.362</td>
      <td>51.698</td>
      <td>46.814</td>
      <td>50.019</td>
      <td>51.011</td>
      <td>51.278</td>
    </tr>
    <tr>
      <th>8</th>
      <td>XGBoost</td>
      <td>0.442961</td>
      <td>47.196</td>
      <td>50.439</td>
      <td>43.037</td>
      <td>48.607</td>
      <td>48.150</td>
      <td>49.447</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Polynomial kernel SVM</td>
      <td>0.442961</td>
      <td>50.515</td>
      <td>48.760</td>
      <td>44.601</td>
      <td>48.264</td>
      <td>48.874</td>
      <td>48.913</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Radial kernel SVM</td>
      <td>0.442961</td>
      <td>50.897</td>
      <td>51.393</td>
      <td>44.868</td>
      <td>49.828</td>
      <td>50.439</td>
      <td>51.240</td>
    </tr>
    <tr>
      <th>11</th>
      <td>GBM</td>
      <td>0.442961</td>
      <td>50.820</td>
      <td>52.079</td>
      <td>47.310</td>
      <td>49.752</td>
      <td>50.591</td>
      <td>51.736</td>
    </tr>
    <tr>
      <th>12</th>
      <td>LightGBM</td>
      <td>0.442961</td>
      <td>49.866</td>
      <td>52.041</td>
      <td>47.119</td>
      <td>48.989</td>
      <td>49.561</td>
      <td>51.164</td>
    </tr>
  </tbody>
</table>
</div>



- When all variables were used, the accuracy of random forest is the highest at 52.003 
- So, let's tune the hyperparameters of the random forest.
- Also, among the models with accuracy greater than 50, since the LightGBM is faster to tune than other models, let's tune the LightGBM	as well.

- Before tune the hyperparameters, let's check the confusion matrix of the random forest and the LightGBM.

### Default Random Forest confusion matrix


```python
rf_default = RandomForestClassifier()
rf_default.fit(X_all_train_std, y_all_train_encd)
rf_default_pred = rf_default.predict(X_all_test_std)
```


```python
le.inverse_transform(y_all_test_encd)
```




    array(['home_win', 'home_win', 'home_win', ..., 'home_win', 'draw',
           'home_win'], dtype=object)




```python
rf_default_cm = confusion_matrix(le.inverse_transform(y_all_test_encd), 
                                 le.inverse_transform(rf_default_pred))
cm_display = ConfusionMatrixDisplay(confusion_matrix = rf_default_cm, 
                                    display_labels = le.inverse_transform(rf_default.classes_))
cm_display.plot();    
```


    
![png](/assets/images/projects/european_soccer_prediction/3.1.modeling_119_0.png)
    



```python
print(classification_report(le.inverse_transform(y_all_test_encd), 
                            le.inverse_transform(rf_default_pred)))
```

                  precision    recall  f1-score   support
    
        away_win       0.49      0.49      0.49       801
            draw       0.31      0.05      0.09       659
        home_win       0.53      0.77      0.63      1161
    
        accuracy                           0.51      2621
       macro avg       0.44      0.44      0.40      2621
    weighted avg       0.46      0.51      0.45      2621
    


### Default LightGBM confusion matrix


```python
lgbm_default = lgb.LGBMClassifier()
lgbm_default.fit(X_all_train_std, y_all_train_encd)
lgbm_default_pred = lgbm_default.predict(X_all_test_std)
```


```python
lgbm_default_cm = confusion_matrix(le.inverse_transform(y_all_test_encd), 
                                   le.inverse_transform(lgbm_default_pred))
cm_display = ConfusionMatrixDisplay(confusion_matrix = lgbm_default_cm, 
                                    display_labels = le.inverse_transform(lgbm_default.classes_))
cm_display.plot();    
```


    
![png](/assets/images/projects/european_soccer_prediction/3.1.modeling_123_0.png)
    



```python
print(classification_report(le.inverse_transform(y_all_test_encd), 
                            le.inverse_transform(lgbm_default_pred)))
```

                  precision    recall  f1-score   support
    
        away_win       0.49      0.48      0.49       801
            draw       0.30      0.07      0.11       659
        home_win       0.54      0.79      0.64      1161
    
        accuracy                           0.51      2621
       macro avg       0.44      0.44      0.41      2621
    weighted avg       0.46      0.51      0.46      2621
    


# 4. Hyperparameter tuning

## 4.1. Random forest

- Candidate hyperparameters are as follow:
    - n_estimators: 100, 300, 500, 1000
    - learning_rate: 1e-8 ~ 1 
    - max_depth: 3 ~ 20 
    - max_features: auto, sqrt, log2
    - min_samples_leaf: 1 ~ 10
    - min_samples_split: 2 ~ 10


```python
def rf_objective(trial, X, y):
    param_grid = {
        "n_estimators": trial.suggest_categorical("n_estimators", [100, 300, 500, 1000]),
        "max_depth": trial.suggest_int("max_depth", 3, 20, step = 2),
        "max_features": trial.suggest_categorical("max_features", ["sqrt", "log2"]),
        "min_samples_leaf": trial.suggest_int("min_samples_leaf", 1, 10),
        "min_samples_split": trial.suggest_int("min_samples_split", 2, 10),
    }
    
    cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)
    cv_scores = np.empty(5)
    
    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        
        model = RandomForestClassifier(**param_grid, n_jobs = -1)
        model.fit(X_train, y_train)
          
        pred = model.predict(X_test)
        cv_scores[idx] = np.mean(pred == y_test)

    return np.mean(cv_scores)
```


```python
rf_study = optuna.create_study(direction = "maximize", study_name = "RandomForest Classifier")
func = lambda trial: rf_objective(trial, X_all_train_std, y_all_train_encd)
rf_study.optimize(func, n_trials = 20)
```

- Best parameters are as follow:


```python
rf_study.best_params
```




    {'n_estimators': 300,
     'max_depth': 5,
     'max_features': 'sqrt',
     'min_samples_leaf': 7,
     'min_samples_split': 4}



- Let's check the test accuracy with the best hyperparameters set.


```python
rf_best = RandomForestClassifier(**rf_study.best_params)
rf_best.fit(X_all_train_std, y_all_train_encd)
rf_best_pred = rf_best.predict(X_all_test_std)
rf_best_accuracy = np.mean(rf_best_pred == y_all_test_encd)
```


```python
print("Accuracy before tuning the hyperparameters: ", result_accuracy[result_accuracy.model_name == "Random Forest"]["All Variables"].values[0])
print("Accuracy after tuning the hyperparameters: ", rf_best_accuracy * 100)
```

    Accuracy before tuning the hyperparameters:  52.003
    Accuracy after tuning the hyperparameters:  52.04120564669973


- Let's check the confusion matrix of the tuned random forest model.


```python
fig, axes = plt.subplots(1, 2, figsize = (15, 5))

# confusion matrix for the random forest with default hyperparameters

rf_default_display = ConfusionMatrixDisplay(confusion_matrix = rf_default_cm, 
                                            display_labels = le.inverse_transform(rf_default.classes_))

# confusion matrix for the random forest with the best hyperparameters

rf_tuned_cm = confusion_matrix(le.inverse_transform(y_all_test_encd), 
                               le.inverse_transform(rf_best_pred))
rf_best_display = ConfusionMatrixDisplay(confusion_matrix = rf_tuned_cm, 
                                    display_labels = le.inverse_transform(rf_best.classes_))

rf_default_display.plot(ax = axes[0])
axes[0].set_title("Random Forest before tuning", fontsize = 15)

rf_best_display.plot(ax = axes[1])
axes[1].set_title("Random Forest after tuning", fontsize = 15)

plt.tight_layout()
```


    
![png](/assets/images/projects/european_soccer_prediction/3.1.modeling_136_0.png)
    



```python
print("< Random Forest before tuning >")
print("")
print(classification_report(le.inverse_transform(y_all_test_encd), 
                            le.inverse_transform(rf_default_pred)))

print("")
print("< Random Forest after tuning >")
print("")
print(classification_report(le.inverse_transform(y_all_test_encd), 
                            le.inverse_transform(rf_best_pred)))
```

    < Random Forest before tuning >
    
                  precision    recall  f1-score   support
    
        away_win       0.49      0.49      0.49       801
            draw       0.31      0.05      0.09       659
        home_win       0.53      0.77      0.63      1161
    
        accuracy                           0.51      2621
       macro avg       0.44      0.44      0.40      2621
    weighted avg       0.46      0.51      0.45      2621
    
    
    < Random Forest after tuning >
    
                  precision    recall  f1-score   support
    
        away_win       0.50      0.50      0.50       801
            draw       0.00      0.00      0.00       659
        home_win       0.53      0.83      0.65      1161
    
        accuracy                           0.52      2621
       macro avg       0.34      0.44      0.38      2621
    weighted avg       0.39      0.52      0.44      2621
    


- Results for away_win and home_win have improved, but the model is struggling to predict the draw.

## 4.2. LightGBM	

- Candidate hyperparameters are as follow:
    - learning_rate: 0.01 ~ 0.3 
    - num_leaves: 20 ~ 3000 with step = 20
    - max_depth: 3 ~ 12 
    - min_data_in_leaf: 200 ~ 10000 with step = 100
    - max_bing: 200 ~ 300
    - lambda_l1: 0 ~ 100 with step = 5
    - lambda_l2: 0 ~ 100 with step = 5
    - min_gain_to_split: 0 ~ 15
    - bagging_fraction: 0.2 ~ 0.95 with step = 0.1
    - feature_fraction: 0.2 ~ 0.95 with step = 0.1


```python
def lgbm_objective(trial, X, y):
    param_grid = {
        "n_estimators": trial.suggest_categorical("n_estimators", [10000]),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "num_leaves": trial.suggest_int("num_leaves", 20, 3000, step = 20),
        "max_depth": trial.suggest_int("max_depth", 3, 12),
        "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 200, 10000, step = 100),
        "max_bin": trial.suggest_int("max_bin", 200, 300),
        "lambda_l1": trial.suggest_int("lambda_l1", 0, 100, step = 5),
        "lambda_l2": trial.suggest_int("lambda_l2", 0, 100, step = 5),
        "min_gain_to_split": trial.suggest_float("min_gain_to_split", 0, 15),
        "bagging_fraction": trial.suggest_float(
            "bagging_fraction", 0.2, 0.95, step = 0.1
        ),
        "bagging_freq": trial.suggest_categorical("bagging_freq", [1]),
        "feature_fraction": trial.suggest_float(
            "feature_fraction", 0.2, 0.95, step = 0.1
        ),
        "silent": 1,
        "verbose": -1
    }
    
    cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)
    cv_scores = np.empty(5)
    
    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        model = lgb.LGBMClassifier(objective = "multiclass", num_class = 3, **param_grid, n_jobs = -1)
        model.fit(
            X_train,
            y_train,
            eval_set=[(X_test, y_test)],
            #eval_metric = accuracy_score,
            early_stopping_rounds = 100,
            # callbacks=[
            #     LightGBMPruningCallback(trial, accuracy_score)
            # ],  # Add a pruning callback
            verbose = -1
        )
        preds = model.predict(X_test)
        accuracy = np.mean(y_test == preds)
        cv_scores[idx] = accuracy

    return np.mean(cv_scores)
```


```python
lgbm_study = optuna.create_study(direction = "maximize", study_name = "LightGBM Classifier")
func = lambda trial: lgbm_objective(trial, X_all_train_std, y_all_train_encd)
lgbm_study.optimize(func, n_trials = 100)
```

- Best parameters are as follow:


```python
lgbm_study.best_params
```




    {'n_estimators': 10000,
     'learning_rate': 0.29341244351241397,
     'num_leaves': 1560,
     'max_depth': 12,
     'min_data_in_leaf': 1800,
     'max_bin': 205,
     'lambda_l1': 20,
     'lambda_l2': 0,
     'min_gain_to_split': 12.558014144849205,
     'bagging_fraction': 0.8,
     'bagging_freq': 1,
     'feature_fraction': 0.30000000000000004}




```python
lgb_best = lgb.LGBMClassifier(**lgbm_study.best_params, n_jobs = -1)
lgb_best.fit(X_all_train_std, y_all_train_encd)
lgb_best_pred = lgb_best.predict(X_all_test_std)
lgbm_best_accuracy = np.mean(lgb_best_pred == y_all_test_encd)
```


```python
print("Accuracy before tuning the hyperparameters: ", result_accuracy[result_accuracy.model_name == "LightGBM"]["All Variables"].values[0])
print("Accuracy after tuning the hyperparameters: ", lgbm_best_accuracy * 100)
```

    Accuracy before tuning the hyperparameters:  51.164
    Accuracy after tuning the hyperparameters:  52.003052270125906



```python
fig, axes = plt.subplots(1, 2, figsize = (15, 5))

# confusion matrix for the lgbm with default hyperparameters

lgbm_default_display = ConfusionMatrixDisplay(confusion_matrix = lgbm_default_cm, 
                                              display_labels = le.inverse_transform(lgbm_default.classes_))

# confusion matrix for the lgbm with the best hyperparameters

lgbm_tuned_cm = confusion_matrix(le.inverse_transform(y_all_test_encd), 
                                 le.inverse_transform(lgb_best_pred))
lgbm_best_display = ConfusionMatrixDisplay(confusion_matrix = lgbm_tuned_cm, 
                                           display_labels = le.inverse_transform(lgb_best.classes_))

lgbm_default_display.plot(ax = axes[0])
axes[0].set_title("LightGBM before tuning", fontsize = 15)

lgbm_best_display.plot(ax = axes[1])
axes[1].set_title("LightGBM after tuning", fontsize = 15)

plt.tight_layout()
```


    
![png](/assets/images/projects/european_soccer_prediction/3.1.modeling_147_0.png)
    



```python
print("< LightGBM before tuning >")
print("")
print(classification_report(le.inverse_transform(y_all_test_encd), 
                            le.inverse_transform(lgbm_default_pred)))

print("")
print("< LightGBM after tuning >")
print("")
print(classification_report(le.inverse_transform(y_all_test_encd), 
                            le.inverse_transform(lgb_best_pred)))
```

    < LightGBM before tuning >
    
                  precision    recall  f1-score   support
    
        away_win       0.49      0.48      0.49       801
            draw       0.30      0.07      0.11       659
        home_win       0.54      0.79      0.64      1161
    
        accuracy                           0.51      2621
       macro avg       0.44      0.44      0.41      2621
    weighted avg       0.46      0.51      0.46      2621
    
    
    < LightGBM after tuning >
    
                  precision    recall  f1-score   support
    
        away_win       0.49      0.51      0.50       801
            draw       0.00      0.00      0.00       659
        home_win       0.53      0.82      0.65      1161
    
        accuracy                           0.52      2621
       macro avg       0.34      0.44      0.38      2621
    weighted avg       0.39      0.52      0.44      2621
    


- Results for away_win and home_win have improved, but the LightGBM is also struggling to predict the draw.

# 5. Feature importance

- Let's check the feature importance from the tuned random forest model based on feature permutation.


```python
rf_best_params = {
    'n_estimators': 300,
    'max_depth': 5,
    'max_features': 'sqrt',
    'min_samples_leaf': 7,
    'min_samples_split': 4
}
```


```python
rf_best = RandomForestClassifier(**rf_best_params)
rf_best.fit(X_all_train_std, y_all_train_encd)
rf_best_pred = rf_best.predict(X_all_test_std)
rf_best_accuracy = np.mean(rf_best_pred == y_all_test_encd)
```


```python
result = permutation_importance(
    rf_best, X_all_test_std, y_all_test_encd, n_repeats=10, random_state=42, n_jobs=-1
)
```


```python
rf_feature_imp_permutation =  pd.DataFrame(sorted(zip(result.importances_mean, col_names)), columns=['Value','Feature'])
```


```python
plt.figure(figsize=(20, 10))
sns.barplot(x="Value", y="Feature", data = rf_feature_imp_permutation.sort_values("Value", ascending = False).head(50))
plt.title('Random Forest Features')
plt.tight_layout()
plt.show()
```


    
![png](/assets/images/projects/european_soccer_prediction/3.1.modeling_156_0.png)
    


- Above plot shows top 50 most important features for predicting the match results.

- Let's compare the distribution of the feature importance between different variable sets.
    - variable set 1: player attributes PC features
    - Variable set 2: betting information features
    - variable set 3: team attributes features
    - variable set 4: goal and win percentage rolling features
    - variable set 5: each team's Elo rating


```python
player_attr_pc_vars = df_match_player_attr_pcs.columns
bet_stat_vars = df_match_betting_stat.columns
team_attr_vars = df_match_team_num_attr.columns
team_rolling_vars = df_team_win_goal_rolling_features.columns
elo_vars = df_match_elo.columns

```


```python
rf_feature_imp_permutation.loc[rf_feature_imp_permutation.Feature.isin(player_attr_pc_vars), "feature_set"] = f"Player attribute PC variables (#: {len(player_attr_pc_vars) - 1})"
rf_feature_imp_permutation.loc[rf_feature_imp_permutation.Feature.isin(bet_stat_vars), "feature_set"] = f"Betting odds statistics variables (#: {len(bet_stat_vars) - 1})"
rf_feature_imp_permutation.loc[rf_feature_imp_permutation.Feature.isin(team_attr_vars), "feature_set"] = f"Team attribute variables (#: {len(team_attr_vars) - 1})"
rf_feature_imp_permutation.loc[rf_feature_imp_permutation.Feature.isin(team_rolling_vars), "feature_set"] = f"Team's recent average goal and win percentage variables (#: {len(team_rolling_vars) - 1})"
rf_feature_imp_permutation.loc[rf_feature_imp_permutation.Feature.isin(elo_vars), "feature_set"] = f"Team's recent Elo variables (#: {len(elo_vars) - 1})"

```


```python
plt.figure(figsize = (12, 5))
sns.boxplot(data = rf_feature_imp_permutation, x = "Value", y = "feature_set")
plt.xlabel("Feature importance", fontsize = 12)
plt.ylabel("Feature sets", fontsize = 12)
plt.title("Feature importance distribution from different feature sets", fontsize = 15)
```




    Text(0.5, 1.0, 'Feature importance distribution from different feature sets')




    
![png](/assets/images/projects/european_soccer_prediction/3.1.modeling_161_1.png)
    


- The betting odds statistics variables shows the highest importance among different feature sets.
- Team attribute variables have the lowest feature importance.
- The remaining three variable sets show similar importance.

## 5.1. Betting odds statistics variables

- Betting odds statistics can be subdivided into:
    - home win, away win, and draw
    - mean, max, min, std


```python
betting_importance = rf_feature_imp_permutation[rf_feature_imp_permutation.feature_set == "Betting odds statistics variables (#: 13)"]
betting_importance["home_away"] = betting_importance.Feature.str.split("_").str[0]
betting_importance["statistics"] = betting_importance.Feature.str.split("_").str[2]

```


```python
betting_importance.sort_values("Value", ascending = False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Value</th>
      <th>Feature</th>
      <th>feature_set</th>
      <th>home_away</th>
      <th>statistics</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>234</th>
      <td>0.004159</td>
      <td>H_odd_mean</td>
      <td>Betting odds statistics variables (#: 13)</td>
      <td>H</td>
      <td>mean</td>
    </tr>
    <tr>
      <th>233</th>
      <td>0.003892</td>
      <td>A_odd_mean</td>
      <td>Betting odds statistics variables (#: 13)</td>
      <td>A</td>
      <td>mean</td>
    </tr>
    <tr>
      <th>232</th>
      <td>0.003853</td>
      <td>A_odd_max</td>
      <td>Betting odds statistics variables (#: 13)</td>
      <td>A</td>
      <td>max</td>
    </tr>
    <tr>
      <th>231</th>
      <td>0.003014</td>
      <td>H_odd_max</td>
      <td>Betting odds statistics variables (#: 13)</td>
      <td>H</td>
      <td>max</td>
    </tr>
    <tr>
      <th>230</th>
      <td>0.002251</td>
      <td>H_odd_min</td>
      <td>Betting odds statistics variables (#: 13)</td>
      <td>H</td>
      <td>min</td>
    </tr>
    <tr>
      <th>229</th>
      <td>0.001831</td>
      <td>H_odd_std</td>
      <td>Betting odds statistics variables (#: 13)</td>
      <td>H</td>
      <td>std</td>
    </tr>
    <tr>
      <th>228</th>
      <td>0.001831</td>
      <td>A_odd_min</td>
      <td>Betting odds statistics variables (#: 13)</td>
      <td>A</td>
      <td>min</td>
    </tr>
    <tr>
      <th>227</th>
      <td>0.001145</td>
      <td>A_odd_std</td>
      <td>Betting odds statistics variables (#: 13)</td>
      <td>A</td>
      <td>std</td>
    </tr>
    <tr>
      <th>226</th>
      <td>0.001106</td>
      <td>D_odd_std</td>
      <td>Betting odds statistics variables (#: 13)</td>
      <td>D</td>
      <td>std</td>
    </tr>
    <tr>
      <th>224</th>
      <td>0.000954</td>
      <td>D_odd_max</td>
      <td>Betting odds statistics variables (#: 13)</td>
      <td>D</td>
      <td>max</td>
    </tr>
    <tr>
      <th>221</th>
      <td>0.000801</td>
      <td>D_odd_min</td>
      <td>Betting odds statistics variables (#: 13)</td>
      <td>D</td>
      <td>min</td>
    </tr>
    <tr>
      <th>204</th>
      <td>0.000496</td>
      <td>D_odd_mean</td>
      <td>Betting odds statistics variables (#: 13)</td>
      <td>D</td>
      <td>mean</td>
    </tr>
  </tbody>
</table>
</div>



- The importance of variables for home and away were high, and the importance of variables for draw were low.
- The importance of variables related to mean and max were high.

## 5.2. Team's recent Elo variables

- Elo rating related variables can be subdivided into:
    - home team, away team
    - average, std
    - recent 1, 3, 5, 10, 20, 30, 60, 90 matches


```python
elo_importance = rf_feature_imp_permutation[rf_feature_imp_permutation.feature_set == "Team's recent Elo variables (#: 34)"]

```


```python
elo_importance["home_away"] = elo_importance.Feature.str.split("_").str[0]
elo_importance["statistics"] = elo_importance.Feature.str.split("_").str[2]
elo_importance["matches"] = elo_importance.Feature.str.split("_").str[5]
```


```python
elo_importance.groupby("home_away").Value.mean().reset_index()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>home_away</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>away</td>
      <td>0.000057</td>
    </tr>
    <tr>
      <th>1</th>
      <td>elo</td>
      <td>0.000458</td>
    </tr>
    <tr>
      <th>2</th>
      <td>home</td>
      <td>0.000210</td>
    </tr>
  </tbody>
</table>
</div>


